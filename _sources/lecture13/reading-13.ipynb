{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression via Least Squares\n",
    "\n",
    "## Supervised learning\n",
    "\n",
    "Suppose that you observe $n$, $d$-dimensional, *inputs*:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{1:n} = \\{\\mathbf{x}_1,\\dots,\\mathbf{x}_n\\},\n",
    "$$\n",
    "\n",
    "and *outputs*:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_{1:n} = \\{y_1,\\dots,y_n\\}.\n",
    "$$\n",
    "\n",
    "The supervised learning problem consists of using the data $\\mathbf{x}_{1:n}$ and $\\mathbf{y}_{1:n}$ to find\n",
    "the map that connects the inputs to the outputs.\n",
    "\n",
    "We call the inputs $\\mathbf{x}$ *features*.\n",
    "\n",
    "The outputs $\\mathbf{y}$ are also called *targets*.\n",
    "\n",
    "### The regression problem\n",
    "\n",
    "When the outputs are continuous variables, e.g., dollars, weight, and mass, we say we have a *regression* problem.\n",
    "You may have heard of *linear regression*.\n",
    "That's when the map that connects the inputs to the outputs is a linear function.\n",
    "There is also the *generalized linear regression* problem, where the map is a nonlinear function but linear in the parameters.\n",
    "For example, the map could be a polynomial function of the inputs.\n",
    "You can also do *nonlinear regression*, where the map is a nonlinear function, and the map is nonlinear in the parameters.\n",
    "Neural networks are an example of nonlinear regression.\n",
    "\n",
    "### The classification problem\n",
    "\n",
    "When the outputs are discrete labels, e.g., 0 or 1, \"cat\" or \"dog,\" we say we have a *classification* problem.\n",
    "*Logistic regression* is an example of a classification problem.\n",
    "You can also solve the classification problem with neural networks.\n",
    "\n",
    "### How do we train a supervised learning model?\n",
    "\n",
    "There are two basic ways to train a supervised learning model:\n",
    "\n",
    "1. Optimization. We find the parameters by maximizing the likelihood of the data. Equivalently, we minimize the negative log-likelihood of the data (the *loss*).\n",
    "\n",
    "2. Probabilistic inference. We apply Bayes' rule to find the posterior distribution of the parameters given the data. We then use the posterior distribution to make predictions. We can characterize the posterior distribution analytically, sample from it, or approximate it with a simpler distribution.\n",
    "\n",
    ":::{note}\n",
    "This lecture moves fast.\n",
    "If you have never seen least squares before, please go over the following material from my undergraduate course:\n",
    "+ [Lecture 14 - Covariance, Correlation, and Linear Regression with One Variable](https://purduemechanicalengineering.github.io/me-297-intro-to-data-science/lecture14/intro.html).\n",
    "+ [Lecture 15 - Linear Regression / Regression with One Variable Revisted](https://purduemechanicalengineering.github.io/me-297-intro-to-data-science/lecture15/regression-with-one-variable-revisited.html#).\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
