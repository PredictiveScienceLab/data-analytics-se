{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling Methods\n",
    "\n",
    "## References\n",
    "\n",
    "+ These notes.\n",
    "+ Chapter 11 of Bishop 2006.\n",
    "\n",
    "## Problem Definition\n",
    "\n",
    "We have seen that the Bayesian formulation of inverse problems results in intractable posterior distributions.\n",
    "In particular, these posteriors are known only up to a normalization constant.\n",
    "In the next series of lectures, we are going to develop methodologies that allows us to sample from these distributions.\n",
    "The most celebrated of these methodologies is Markov Chain Monte Carlo (MCMC) which has at its core the Metropolis algorithms.\n",
    "It is a long way to go, but we can state right away the problem definition and what we are set out to do.\n",
    "\n",
    "Without loss of generality, let $X\\in\\mathcal{X}\\subset\\mathbb{R}^d$ be random variable with an arbitrary probability density, say $\\pi(x)$ known up to a normalization constant.\n",
    "That is, we have that:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\frac{h(x)}{Z},\n",
    "$$\n",
    "\n",
    "where $h(x)$ is a known function that we can evaluate at will, but $Z$ is not known.\n",
    "Our goal is to generate samples from $\\pi(x)$, by only evaluating $h(x)$.\n",
    "The revolutionary idea of Metropolis was to construct a stochastic process using only $h(x)$ samples from which resemble (in some way we will specify below) samples from $\\pi(x)$.\n",
    "To understand the details, we will have to introduce some key concepts.\n",
    "\n",
    "## Markov Chains\n",
    "\n",
    "### Definition\n",
    "Let $X_n, n=1,2,\\dots$ be a stochastic process taking values in $\\mathcal{X}\\subset\\mathbb{R}^d$ whcih could either be discrete or continuous.\n",
    "We will refer to $n$ as the *time step*.\n",
    "We say that this stochastic process is a *Markov chain* if the evolution of $X_{n+1}$ depends only on the value of $X_n$ and not on all the history of the process.\n",
    "Let us define this a little bit more rigorously.\n",
    "\n",
    "Let $x_1,\\dots,x_n\\in\\mathcal{X}$ be the observed values of the process up to $n$-th time step.\n",
    "The Markov property can now be expressed as:\n",
    "\n",
    "$$\n",
    "p(X_{n+1}=x_{n+1}|X_1=x_1,\\dots,X_n=x_n) = p(X_{n+1}=x_{n+1}|X_n=x_n).\n",
    "$$\n",
    "\n",
    "If there is no ambiguity, we will be simplifying the notation by dropping the capital letters.\n",
    "That is, we will be writting:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_1,\\dots,x_n) = p(x_{n+1}|x_n).\n",
    "$$\n",
    "\n",
    "To simplify things even further, we will also use the collective notation:\n",
    "\n",
    "$$\n",
    "x_{1:n} = (x_1,\\dots,x_n)\\in\\mathcal{X}^n.\n",
    "$$\n",
    "\n",
    "With this notation, we can re-write the Markov property in even simpler terms:\n",
    "\n",
    "$$\n",
    "p(x_{n+1}|x_{1:n}) = p(x_{n+1}|x_n).\n",
    "$$\n",
    "\n",
    "### The joint distribution of a Markov chain\n",
    "The *joint distribution* is defined as:\n",
    "\n",
    "$$\n",
    "p(x_{1:n}) := P(X_1=x_1,\\dots,X_n=x_n).\n",
    "$$\n",
    "\n",
    "If $X_n$ is a Markov chain, then we simply have:\n",
    "\n",
    "$$\n",
    "p(x_{1:n}) = p(x_1)p(x_2|x_1)\\dots p(x_n|x_{n-1}),\n",
    "$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\n",
    "p(x_{1:n}) = p(x_1)\\prod_{t=2}^np(x_t|x_{t-1}).\n",
    "$$\n",
    "\n",
    "So, we see that we know the joint distribution of a Markov chain if we know the probability of hoping from one state to the next.\n",
    "This propability is known as the transition kernel of the Markov chain.\n",
    "\n",
    "### Transition Kernel\n",
    "To describe a Markov chain, we only need to know the *transition kernel*.\n",
    "The transition kernel gives the probability of moving from one state to any other at a given step.\n",
    "Mathematically, the transition kernel of the $n$-th step is the function:\n",
    "\n",
    "$$\n",
    "T_n:\\mathcal{X}\\times \\mathcal{X}\\rightarrow \\mathbb{R}_+,\n",
    "$$\n",
    "\n",
    "defined by:\n",
    "\n",
    "$$\n",
    "T_n(x_n, x_{n+1}) = P(X_{n+1}=x_{n+1}|X_n=x_n).\n",
    "$$\n",
    "\n",
    "In words, $T_n(x_n, x_{n+1})$ is the probability that at step $n$ we jump from state $x_n$ to to state $x_{n+1}$.\n",
    "\n",
    "Please, note that the transition kernel, in general, depends on the time step $n$.\n",
    "We say that the Markov chain is *stationary*, if the transition kernel does not depend on $n$, i.e., if\n",
    "\n",
    "$$\n",
    "T_n(x_n, x_{n+1}) = T(x_n,x_{n+1}).\n",
    "$$\n",
    "\n",
    "**From now on, we will only consider stationary Markov chains.**\n",
    "For stationary Markov chains, and when there is now ambiguity, we will be writing:\n",
    "\n",
    "$$\n",
    "T(x_n,x_{n+1}) = p(x_{n+1}|x_n).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invariant Distributions\n",
    "\n",
    "Let $X_1,X_2,\\dots$ be a Markov chain with transition kernel $T(x,x')$ and $\\pi(x)$ be a probability density.\n",
    "We say that the Markov chain leaves $\\pi(x)$ *invariant* if:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\int \\pi(x')T(x',x)dx'.\n",
    "$$\n",
    "\n",
    "In words, $\\pi(x)$ is invariant if when you start from a sample from it and you follow the Markov chain you get a sample from it.\n",
    "\n",
    "Invariance is one of the key requirements of a working MCMC algorithm.\n",
    "Whatever you do, the chain you construct must be invariant with respect to the distribution from which you want to sample.\n",
    "If you manage to do that, then once you get one sample from your distribution, you can get as many as you want by simply following the transition kernel.\n",
    "\n",
    "\n",
    "\n",
    "## The Detailed Balance Condition\n",
    "Checking invariance is not trivial for a generic Markov chain.\n",
    "However, there is a sufficient condition that guarantees invariance.\n",
    "This condition is known as the *detailed balance condition* and it is:\n",
    "$$\n",
    "\\pi(x)T(x,x') = \\pi(x')T(x',x).\n",
    "$$\n",
    "In words, a Markov chain that satisfies the detailed balance condition is *reversible* in the following sense.\n",
    "The probability of sampling an $x$ and transitioning to $x'$ is the same as the probability of doing the reverse.\n",
    "\n",
    "If the detailed balance condition is satisfied, then $\\pi(x)$ is an invariant distribution:\n",
    "$$\n",
    "\\int \\pi(x')T(x',x)dx' = \\int \\pi(x)T(x,x')dx' = \\pi(x)\\int T(x,x')dx' = \\pi(x),\n",
    "$$\n",
    "since\n",
    "$$\n",
    "\\int T(x,x') dx' = \\int p(x'|x)dx' = 1.\n",
    "$$\n",
    "The reverse does not necessarily hold.\n",
    "The key idea of Metropolis was to construct a Markov chain that satisfies the detailed balance condition for the distribution you are interested in.\n",
    "\n",
    "## Ergodicity\n",
    "\n",
    "A Markov chain may have no invariant distribution (e.g., the random walk does not have an invariant distribution), one, or more than one.\n",
    "To guarantee uniqueness of the invariant distribution, need *ergodicity*.\n",
    "To define ergodicity precicely for contiuous Markov chains, we need a little bit of notation.\n",
    "In words, ergodicity requires that the random variable $X_n$ converges in distribution to $\\pi(x)$ irrespective of the starting point.\n",
    "Obviously, this is not easy to show for a generic Markov chain.\n",
    "Fortunately, we know that a Markov chain is ergodic if:\n",
    "\n",
    "+ it is *aperiodic* (i.e., it does not return to the same state at fixed intervals)\n",
    "+ it is *positive recurrent* (i.e., the expected number of steps for returning to the same state is finite).\n",
    "\n",
    "## Equilibrium Distribution\n",
    "If a Markov chain is ergodic and it has an invariant distribution, then that invariant distribution is unique and it is called the *equilibrium distribution*.\n",
    "The Metropolis algorithms constructs a Markov chain that has a desired equilibrium distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metropolis Algorithm\n",
    "\n",
    "Now, let's get back to the initial problem of sampling from:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\frac{h(x)}{Z},\n",
    "$$\n",
    "\n",
    "without knowing $Z$.\n",
    "In 1953, Metropolis et al. demonstrated how we can construct a Markov chain with $\\pi(x)$ as the equilibrium density.\n",
    "The algorithm is based on biasing an underlying symmetric, stationary Markov chain.\n",
    "Let $T(x,x')$ be the transition kernel of this underlying Markov chain (also called the *proposal distribution*.\n",
    "The transition kernel must be symmetric, i.e.,\n",
    "\n",
    "$$\n",
    "T(x,x') = T(x',x).\n",
    "$$\n",
    "\n",
    "A very common choice of the proposal distribution is the random walk transition kernel:\n",
    "\n",
    "$$\n",
    "T(x,x') = \\mathcal{N}(x'|x, \\Sigma).\n",
    "$$\n",
    "\n",
    "However, this is just one possibility.\n",
    "Once we have pick a proposal, we construct the desired Markov chain as follows:\n",
    "\n",
    "+ **Initialization:** Pick an arbitrary starting point $x_0$.\n",
    "+ For each time step $n$:\n",
    "    - **Generation:** Sample a candidate $x$ from $T(x_n, x)$.\n",
    "    - **Calculation:** Calculate the *acceptance ratio*:\n",
    "\n",
    "    $$\n",
    "    \\alpha(x_n, x) = \\min\\left\\{1, \\frac{h(x)}{h(x_n)}\\right\\}.\n",
    "    $$\n",
    "    \n",
    "    This is the only place where you may need to evaluate the underlying model.\n",
    "    - **Accept/Reject:**\n",
    "        - Generate a uniform number $u\\sim \\mathcal{U}([0,1])$.\n",
    "        - If $u\\le \\alpha$, *accept* and set $x_{n+1}=x$.\n",
    "        - If $u > \\alpha$, *reject* ad set $x_{n+1} = x_n$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Does Metropolis Work?\n",
    "\n",
    "Well, it works because it gives us a Markov chain with the desired equilibrium distribution.\n",
    "That is, a chain that has an invariant distribution of our choice that it is also ergodic.\n",
    "\n",
    "To show that $\\pi(x)$ is the invariant distribution of the Metropolis Markov chain, we will show that the latter satisfies the detailed balance condition.\n",
    "To this end, we need the transition kernel of the chain.\n",
    "The transition kernel $K(x,x')$ gives the probability that the Metropolis chain moves from $x$ to $x'$.\n",
    "It is:\n",
    "\n",
    "$$\n",
    "K(x,x') = T(x,x')\\alpha(x,x') + (1 - r(x))\\delta(x' - x),\n",
    "$$\n",
    "\n",
    "where $T(x,x')$ is the transition kernel of the proposal distribution,\n",
    "\n",
    "$$\n",
    "\\alpha(x,x') = \\min\\left\\{1, \\frac{h(x')}{h(x)}\\right\\}\n",
    "$$\n",
    "\n",
    "is the acceptance ratio,\n",
    "\n",
    "$$\n",
    "r(x) = \\int T(x, y)\\alpha(x, y)dy,\n",
    "$$\n",
    "\n",
    "is the probability of accepting any move, i.e., $1 - r(x)$ is the probability of not accepting the move, and $\\delta(x-x')$ is the Dirac delta centered at $x'$.\n",
    "\n",
    "Let's prove that the detailed balance holds for this transition kernel.\n",
    "For $x = x'$ the equation holds trivially (even though we would have to interpret it slightly differently to be 100% rigorous).\n",
    "For $x\\not= x'$, we have:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\pi(x) K(x, x') &=& \\frac{h(x)}{Z} T(x,x')\\alpha(x,x') \\\\\n",
    "&=& \\frac{h(x)}{Z}T(x,x')\\min\\left\\{1, \\frac{h(x')}{h(x)}\\right\\}\\\\\n",
    "&=& \\frac{h(x')}{h(x')}\\frac{h(x)}{Z}T(x,x')\\min\\left\\{1, \\frac{h(x')}{h(x)}\\right\\}\\\\\n",
    "&=& \\frac{h(x')}{Z}T(x,x')\\min\\left\\{\\frac{h(x)}{h(x')},\\frac{h(x)}{h(x')}\\cdot\\frac{h(x')}{h(x)}\\right\\}\\\\\n",
    "&=& \\pi(x')T(x,x')\\min\\left\\{\\frac{h(x)}{h(x')},1\\right\\}\\\\\n",
    "&=& \\pi(x')T(x,x')\\alpha(x',x)\\\\\n",
    "&=& \\pi(x')T(x', x)\\alpha(x',x)\\\\\n",
    "&=& \\pi(x')K(x',x),\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we also made use of the symmetry of the proposl $T(x,x') = T(x',x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metropolis-Hastings Algorithm\n",
    "\n",
    "The Metroplis algorithm requires that the proposal kernel $T(x,x')$ is symmetric.\n",
    "Hastings (1970) created an algorithm that does not require symmetric proposal kernels.\n",
    "The only thing that changes is the acceptance ratio.\n",
    "In every other regard, the algorithm is exactly the same:\n",
    "\n",
    "+ **Initialization:** Pick an arbitrary starting point $x_0$.\n",
    "+ For each time step $n$:\n",
    "    - **Generation:** Sample a candidate $x$ from $T(x_n, x)$.\n",
    "    - **Calculation:** Calculate the *acceptance ratio*:\n",
    "    \n",
    "    $$\n",
    "    \\alpha(x_n, x) = \\min\\left\\{1, \\frac{h(x)}{h(x_n)}\\frac{T(x,x_n)}{T(x_n,x)}\\right\\}.\n",
    "    $$\n",
    "    \n",
    "    This is the only place where you may need to evaluate the underlying model.\n",
    "    - **Accept/Reject:**\n",
    "        - Generate a uniform number $u\\sim \\mathcal{U}([0,1])$.\n",
    "        - If $u\\le \\alpha$, *accept* and set $x_{n+1}=x$.\n",
    "        - If $u > \\alpha$, *reject* ad set $x_{n+1} = x_n$.\n",
    "        \n",
    "## Why Does Metropolis-Hastings Work?\n",
    "\n",
    "Well, it works because it gives us a Markov chain with the desired equilibrium distribution.\n",
    "That is, a chain that has an invariant distribution of our choice that it is also ergodic.\n",
    "\n",
    "To show that $\\pi(x)$ is the invariant distribution of the Metropolis-Hastings Markov chain, we will show that the latter satisfies the detailed balance condition.\n",
    "To this end, we need the transition kernel of the chain.\n",
    "The transition kernel $K(x,x')$ gives the probability that the Metropolis chain moves from $x$ to $x'$.\n",
    "It is:\n",
    "$$\n",
    "K(x,x') = T(x,x')\\alpha(x,x') + (1 - r(x))\\delta(x' - x),\n",
    "$$\n",
    "where $T(x,x')$ is the transition kernel of the proposal distribution,\n",
    "$$\n",
    "\\alpha(x,x') = \\min\\left\\{1, \\frac{h(x')}{h(x)}\\frac{T(x',x)}{T(x,x')}\\right\\}\n",
    "$$\n",
    "is the acceptance ratio,\n",
    "$$\n",
    "r(x) = \\int T(x, y)\\alpha(x, y)dy,\n",
    "$$\n",
    "is the probability of accepting any move, i.e., $1 - r(x)$ is the probability of not accepting the move, and $\\delta(x-x')$ is the Dirac delta centered at $x'$.\n",
    "\n",
    "Let's prove that the detailed balance holds for this transition kernel.\n",
    "For $x = x'$ the equation holds trivially (even though we would have to interpret it slightly differntly to be 100% rigorous).\n",
    "For $x\\not= x'$, we have:\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\pi(x) K(x, x') &=& \\frac{h(x)}{Z} T(x,x')\\alpha(x,x') \\\\\n",
    "&=& \\frac{h(x)}{Z}T(x,x')\\min\\left\\{1, \\frac{h(x')}{h(x)}\\frac{T(x',x)}{T(x,x')}\\right\\}\\\\\n",
    "&=& \\frac{h(x')}{h(x')}\\frac{T(x',x)}{T(x',x)}\\frac{h(x)}{Z}T(x,x')\\min\\left\\{1, \\frac{h(x')}{h(x)}\\frac{T(x',x)}{T(x,x')}\\right\\}\\\\\n",
    "&=& \\frac{h(x')}{Z}T(x',x)\\min\\left\\{\\frac{h(x)}{h(x')}\\frac{T(x,x')}{T(x',x)},\\frac{h(x)}{h(x')}\\frac{T(x,x')}{T(x',x)}\\cdot\\frac{h(x')}{h(x)}\\frac{T(x',x)}{T(x,x')}\\right\\}\\\\\n",
    "&=& \\pi(x')T(x',x)\\min\\left\\{\\frac{h(x)}{h(x')\\frac{T(x,x')}{T(x',x)}},1\\right\\}\\\\\n",
    "&=& \\pi(x')T(x',x)\\alpha(x',x)\\\\\n",
    "&=& \\pi(x')K(x',x).\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "## The Metropolis-Hastings Algorithm is Not One Algorithm!\n",
    "\n",
    "For every choice of proposal $T(x',x)$, you get a different MH algorithm.\n",
    "This is extremely empowering, since you can construct an MH that best exploits your problem.\n",
    "Over the years, several cases have been proposed that are extremely useful.\n",
    "We enumerate a few:\n",
    "\n",
    "+ [Metrpolis Adjusted Langevin Dynamics](https://en.wikipedia.org/wiki/Metropolis-adjusted_Langevin_algorithm): This algorithm uses gradient information to push your chain towards highly probable states.\n",
    "\n",
    "+ [Hybrid Monte Carlo](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo): This method associates the random variables you have with the generalized coordinates of a hypothetical physical system, and the negative log of the probability density you want to sample from with a fictitious energy. The proposal follows the hypothetical Hamiltonian dynamics with randomly sampled (fake) velocities. This moves you to low energy states which are associated with high probabilities.\n",
    "\n",
    "+ [Riemannian Manifold Hamiltonian Monte Carlo](https://arxiv.org/abs/0907.1100): One of the most advanced algorithms. Like HMC, but it exploits the Riemannian structure of the parameter space to automatically adapts to local features.\n",
    "\n",
    "## Metropolis Adjusted Langevin Dynamics (MALA)\n",
    "\n",
    "Understanding Langevin dynamics fully requires familiarity with [Itô calculus](https://en.wikipedia.org/wiki/Itô_calculus).\n",
    "The math is very advanced, but we will do our best to explain what is going on intuitively.\n",
    "Remember that we want to sample from:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\frac{h(x)}{Z},\n",
    "$$\n",
    "\n",
    "where $Z$ is unknown.\n",
    "\n",
    "Consider the stochastic differential equation (Itô diffusion):\n",
    "\n",
    "$$\n",
    "\\dot{X}_t = -\\nabla V(X_t) + \\sqrt{2}\\dot{W}_t,\n",
    "$$\n",
    "\n",
    "where the time is continuous, and $W_t$ is a Brownian motion.\n",
    "This is called the Langevin equation.\n",
    "Intuitively, think of $X_t$ as the position of a particle that wants to move towards regions of low potential energy $V(x)$ but it is bombarded continuously by random forces.\n",
    "What we want to do, is pick a $V(x)$ that will force this fictitious particle to move towards regions of high $h(x)$.\n",
    "This can be done in many ways, but let us take:\n",
    "\n",
    "$$\n",
    "V(x) = -\\log h(x),\n",
    "$$\n",
    "\n",
    "because we already know the answer!\n",
    "Using the theory of stochastic differential equations one can show that the distribution of $X_t$, say $\\rho_t$, converges to a stationary distribution $\\rho_\\infty$.\n",
    "Well, it turns out that:\n",
    "\n",
    "$$\n",
    "\\rho_\\infty(x) \\propto h(x),\n",
    "$$\n",
    "\n",
    "i.e.,\n",
    "\n",
    "$$\n",
    "\\rho_\\infty = \\pi.\n",
    "$$\n",
    "\n",
    "So, here is the idea:\n",
    "+ simulate the Langevin equation for a large enough time\n",
    "+ and you should get a sample from $\\pi$.\n",
    "\n",
    "The only issue is that you cannot get exact sample paths from the Langevin equation.\n",
    "You have to use a time discretization scheme.\n",
    "The simplest such scheme is the Euler-Maruyama method (generalization of the Euler method for ODEs to SODEs).\n",
    "You fix a time step $\\Delta t > 0$ and you take:\n",
    "\n",
    "$$\n",
    "X_{n+1} = X_n + \\Delta t \\nabla \\log h(X_n) + \\sqrt{2\\Delta t}Z_n,\n",
    "$$\n",
    "\n",
    "where $Z_n\\sim \\mathcal{N}(0,I_d)$ independent.\n",
    "This is basically a discrete time Markov chain with a non-symmetric transition kernel:\n",
    "\n",
    "$$\n",
    "T(x,x') = \\mathcal{N}\\left(x'|x + \\Delta t\\nabla \\log h(x), 2\\Delta t\\right) \\propto \\exp\\left\\{-\\frac{\\parallel x+\\Delta t\\log h(x)-x'\\parallel_2^2}{4\\Delta t}\\right\\}.\n",
    "$$\n",
    "\n",
    "In the limit of $\\Delta t\\rightarrow 0$, you will get exact sample paths and, hence, samples from $\\pi(x)$.\n",
    "Unfortunately, for finite $\\Delta t$, you will converge to a perturbed version of $\\pi(x)$.\n",
    "Fortunately, you can use $T(x,x')$ as the proposal kernel of a Metropolis-Hastings algorithm.\n",
    "In other words, you can Metropolize the discretized version of the Langevin equation.\n",
    "Then, your resulting Markov chain will satisfy the detailed balance for the right probability density and you are all set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Transition Kernels\n",
    "\n",
    "Now, let's get back to the original problem of sampling from:\n",
    "\n",
    "$$\n",
    "\\pi(x) = \\frac{h(x)}{Z}.\n",
    "$$\n",
    "\n",
    "Assume that you have $m$ different Markov kernels, $K_1(x,x'), \\dots, K_m(x,x')$ (which could be Metropolis-Hastings kernels) that leave $\\pi(x)$ invariant.\n",
    "Then, the Markov kernel that consists of applying these kernels in order also leaves $\\pi(x)$ invariant.\n",
    "This kernel is:\n",
    "\n",
    "$$\n",
    "K(x,x') = \\int K_1(x, x_1)K_2(x_1, x_2)\\dots K_m(x_{m-1}, x')dx_1dx_2\\dots dx_{m-1}.\n",
    "$$\n",
    "\n",
    "The proof is trivial:\n",
    "\n",
    "$$\n",
    "\\begin{array}{ccc}\n",
    "\\int \\pi(x) K(x, x') dx &=& \\int \\pi(x) \\int K_1(x, x_1)K_2(x_1, x_2)\\dots K_m(x_{m-1}, x')dx_1dx_2\\dots dx_{m-1} dx\\\\\n",
    "&=& \\int \\left(\\int \\pi(x) K(x, x_1) dx \\right) K_2(x_1, x_2)\\dots K_m(x_{m-1}, x')dx_1dx_2\\dots dx_{m-1} \\\\\n",
    "&=& \\int \\pi(x_1) K_3(x_2, x_4)\\dots K_m(x_{m-1}, x')dx_2dx_3\\dots dx_{m-1}\\\\\n",
    "&=& \\int \\left(\\pi(x_1) K_2(x_1, x_2)dx_1\\right)K_3(x_2, x_4)\\dots K_m(x_{m-1}, x')dx_2dx_3\\dots dx_{m-1}\\\\\n",
    "&=& \\int \\pi(x_2)K_3(x_2, x_4)\\dots K_m(x_{m-1}, x')dx_2dx_3\\dots dx_{m-1}\\\\\n",
    "&=& \\dots\\\\\n",
    "&=& \\pi(x').\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "So, if you have many Metropolis-Hastings kernels, or any other kernels really, you can combine them all together in arbitrary ways.\n",
    "You will still be getting samples from the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gibbs Sampler\n",
    "\n",
    "The Gibbs sampler is based on the idea of combining kernels that operate on groups of components of your random variables and exploit the availability of the conditional distributions.\n",
    "For example, assume that $x$ consists of $m$ groups:\n",
    "\n",
    "$$\n",
    "x = (x_{g1}, \\dots, x_{gm}).\n",
    "$$\n",
    "\n",
    "Note that each group may consist of more than one variables.\n",
    "That is, the number of groups $m\\le d$.\n",
    "\n",
    "Let $x_{gi}$ denote the $i$-th group of variables, $i=1,\\dots,m$, and\n",
    "\n",
    "$$\n",
    "x_{g,-i} = \\left(x_{g1},\\dots,x_{g,i-1},x_{g,i+1},\\dots,x_{gm}\\right),\n",
    "$$\n",
    "\n",
    "all groups except $i$.\n",
    "Of course, we have that:\n",
    "\n",
    "$$\n",
    "x = (x_{gi}, x_{g,-i}).\n",
    "$$\n",
    "\n",
    "To implement a Gibbs sampler, we need the ability to sample from the conditional probability densities:\n",
    "\n",
    "$$\n",
    "\\pi(x_{gi} | x_{g,-i}) = \\frac{\\pi(x_{gi},x_{g,-i})}{\\pi(x_{g-i})}.\n",
    "$$\n",
    "\n",
    "If it is possible to sample easily from this distribution, then we are all set.\n",
    "Then we say that we are using an *exact Gibbs sampler*.\n",
    "If analytical samples are not possible, then we can simply construct a Metropolis-Hastings kernel that samples from the conditional (you just think of $x_{g,-i}$ as given when you are using this kernel.\n",
    "Then we say that we are using an *approximate Gibbs sampler*.\n",
    "\n",
    "There are many possible versions of the Gibbs depending on how we select from which conditional to sample next.\n",
    "The simplest version is this following in which we sample from all the conditionals in order:\n",
    "\n",
    "+ Initialize the sampler:\n",
    "\n",
    "$$\n",
    "x_0 = (x_{0g1},\\dots,x_{0gm}).\n",
    "$$\n",
    "\n",
    "+ For steps $t=1,2\\dots$ do:\n",
    "    - Set:\n",
    "\n",
    "        $$\n",
    "            x_{t} \\leftarrow x_{t-1}.\n",
    "        $$\n",
    "\n",
    "    - Sample from the conditional of group $i$:\n",
    "        \n",
    "        $$\n",
    "        x_{tgi} \\sim \\pi(\\cdot|x_{tg,-i}).\n",
    "        $$\n",
    "        \n",
    "There various flavors of the (approximate) Gibbs sampler; each one with its own pros and cons.\n",
    "For example, another very common approach is to select $i$ at random for each step $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
