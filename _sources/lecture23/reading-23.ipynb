{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Global Optimization\n",
    "\n",
    "## References\n",
    "\n",
    "+ [A Tutorial on Bayesian Global Optimization, by Peter Frazier](https://arxiv.org/abs/1807.02811#:~:text=Bayesian%20optimization%20is%20an%20approach,stochastic%20noise%20in%20function%20evaluations.)\n",
    "\n",
    "## Baysian Global Optimization of a Scalar Function without Noise\n",
    "\n",
    "We are going to addresss the problem:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^* = \\arg\\max_{\\mathbf{x}}f(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "under the assumption that:\n",
    "+ we can evaluate $f(\\mathbf{x})$ at any $\\mathbf{x}$;\n",
    "+ evaluating $f(\\mathbf{x})$ takes a lot of time/money;\n",
    "+ we cannot evaluate the gradient $\\nabla f(\\mathbf{x})$;\n",
    "+ the dimensionality of $\\mathbf{x}$ is not very high.\n",
    "\n",
    "Why is this an important problem?\n",
    "Well, $\\mathbf{f}(\\mathbf{x})$ may be a very expensive simulation that you would like to optimize under a limitted budget.\n",
    "It could also be an experimentally measured quantity of interest, but you would have to make sure that the noise is not very big.\n",
    "We will see what you can do when you have noise later.\n",
    "\n",
    "### Sequential Information Acquisition for Optimization\n",
    "\n",
    "What do we mean by *sequential information acquisiton* for optimization?\n",
    "It is just an algorithm that tells us where to evaluate a function next if we would like to find its maximum/minimum.\n",
    "The problem can be formulated in a rigorous mathematical way using the theory and tools of dynamic programming.\n",
    "However, the full fledged mathematical problem of designing optimal evaluation policies tends to be harder than the original problem of just maximizing a function.\n",
    "So, we usually resort to heurestics.\n",
    "These heuristics are not optimal in a mathematical sense, but they are convenient in a practical sense.\n",
    "In a way, the heuristics strike a balance the theoretically optimal (but computationally intractable) thing to do, and just randomly selecting evaluations.\n",
    "\n",
    "A generic heuristic, which we are going to call \"one-step-look ahead information acquisition policy\" works as follows:\n",
    "+ We start with an initial data set consiting of $n_0$ input-output observations:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{n_0} = \\left(\\mathbf{x}_{1:n_0}, \\mathbf{y}_{1:n_0}\\right).\n",
    "$$\n",
    "\n",
    "For example, one may obtain these through Latin Hypercube Sampling of their input space.\n",
    "+ For $n=n_0,n_0+1,\\dots$, we start do the following:\n",
    "    - We use the current dataset to quantify our state of knowledge about $f(\\mathbf{x})$.\n",
    "    For example, we can use Gaussian process regression (GPR) or any other Bayesian regression method to obtain the predictive distribution:\n",
    "\n",
    "        $$\n",
    "        f(\\cdot) | \\mathcal{D}_{n_0} \\sim p(f(\\cdot)|\\mathcal{D}_{n_0}).\n",
    "        $$\n",
    "\n",
    "    - Then we pick the *most important* point to evaluate next by looking at maximizing an *acquisition function*\n",
    "    $a_{n_0}(\\mathbf{x})$ which depends on our current state of knowledge (we will see specific examples below).\n",
    "    This acquisition function quantifies, for example, how much value or how much information there is in\n",
    "    in doing an evaluation at $\\mathbf{x}$.\n",
    "    We can assume that it is a nonnegative function.\n",
    "    So, we to pick the next point we solve a problem of the form:\n",
    "\n",
    "    $$\n",
    "    \\mathbf{x}_{n+1} = \\arg\\max a_{n}(\\mathbf{x}).\n",
    "    $$\n",
    "\n",
    "    That is, we pick the point with the maximum value or the maximum information.\n",
    "    - If maximum value of the acquisition function is smaller than a threshold, e.g., if $a_n(\\mathbf{x}) \\le \\epsilon$ for some $\\epsilon > 0$, then we STOP.\n",
    "    - Otheweise, we evaluate the function at the selected $\\mathbf{x}_{n+1}$ to obtain:\n",
    "\n",
    "    $$\n",
    "    y_n = f(\\mathbf{x}_n).\n",
    "    $$\n",
    "\n",
    "    This would entain either running a simulation or doing an experiment.\n",
    "    - We augment our original data set with the new observation:\n",
    "\n",
    "    $$\n",
    "    \\mathcal{D}_{n+1} = \\left((\\mathbf{x}_{1:n},\\mathbf{x}_n), (\\mathbf{y}_{1:n},y_n)\\right).\n",
    "    $$\n",
    "\n",
    "    - We use Bayes' rule to update our state of knowledge:\n",
    "\n",
    "    $$\n",
    "    f(\\cdot)|\\mathcal{D}_{n+1} \\sim p(f(\\cdot)|\\mathcal{D}_{n+1}) \\propto p(y_{n+1}|x_{n+1}, f(\\cdot))p(f(\\cdot)|\\mathcal{D}_{n}).\n",
    "    $$\n",
    "\n",
    "    - If we have run out of evaluation budget, we STOP. Otherwise, we continue the loop.\n",
    "+ At this point, we report our current state of knowledge about the maximum of the function.\n",
    "For example, we can report what is the *observed maximum*.\n",
    "That is, we can find the index of the evaluation corresponding to the observed maximum:\n",
    "\n",
    "$$\n",
    "i^* = \\arg\\max_{1\\le i\\le n} y_i,\n",
    "$$\n",
    "\n",
    "and say that the maximum was $y_{i^*}$ and the location of the maximum is $\\mathbf{x}_{i^*}$.\n",
    "Alternatively, and in particular if our optimizatioin budget is low, we can also quantify our epistemic uncertainty about the location of the maximum. We discuss this idea in detail towards the end of this notebook.\n",
    "\n",
    "We will provide specific examples of information acquisition functions in the hands-on activities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
