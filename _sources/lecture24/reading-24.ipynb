{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "## References\n",
    "\n",
    "+ Chapters 6, 7, and 8 of https://www.deeplearningbook.org/\n",
    "+ These notes.\n",
    "\n",
    "These notes are not exhaustive. They merely provide a summary. Please consult the book chapters for the complete details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep neural networks as function approximators\n",
    "\n",
    "Deep neural networks (DNN) are function approximators that express information in a hierarchical, layered fashion. \n",
    "They can be used to approximate a function of $d$ inputs to $q$ outputs using some parameters $\\theta$.\n",
    "We, typicaly write $\\mathbf{y} = f(\\mathbf{x};\\theta)$.\n",
    "Here $f(\\mathbf{x};\\theta)$ is the DNN and $\\theta$ are its parameters.\n",
    "Both these concepts will be clarified below.\n",
    "\n",
    "Mathematically, deep neural networks can be expressed as compositions of simpler one-layer neural neteworks:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x};\\theta) = (f_L \\circ f_{L-1} \\circ \\cdots \\circ f_1)( \\mathbf{x}).\n",
    "$$\n",
    "\n",
    "In the simplest setting, the layers $f_l$s are a composition of an elementwise nonlinearity with a linear transformation:\n",
    "\n",
    "$$\n",
    "f_i ( \\mathbf{z} ) = h^{(i)} ( \\mathbf{W}^{(i)} \\mathbf{z} + \\mathbf{b}^{(i)}  ),\n",
    "$$\n",
    "\n",
    "where, $\\mathbf{W}^{(i)}$ is a matrix of parameters, $\\mathbf{b}^{(i)}$ is a vector of parameters, and $h^{(i)}$ is a nonlinear function applied in an elementwise fashion (i.e., applied separately to each one of the inputs that are provided to it). A DNN with this structure is called a *fully-connected* DNN. \n",
    "\n",
    "In deep learning parlance the matrix $\\mathbf{W}^{(i)}$ is referred to as a *weight* matrix, the vector $\\mathbf{b}^{(i)}$ is referred to as a *bias*.\n",
    "The function $h^{(i)}(\\cdot)$ is called the *activation* function. It is typical for all but the last layer of a DNN to have the same activation function. \n",
    "\n",
    "At the final layer, the dimensionality of the output and the choice of the activation function are dictated by constraints on the final output of the function $f$. For example:\n",
    "1. If the output from $f$ is a real number with no constraints, the output diemnsions is $d_L=1$ and $h^{(L)}(\\mathbf{z}) = 1$.\n",
    "2. If the output from $f$ is a positive real, $n^{(L)} = q = 1$ and $\\sigma_L(x) = \\exp(x)$. \n",
    "3. If the output from $f$ is a probability mass function on $K$ categories, $n^{(L)} = q = K$ and $h^{(L)}(\\mathbf{z}) = \\frac{\\exp(z_i)}{\\sum_{j=1}^{K} \\exp(z_j)}, i=1, 2, \\dots, K$.\n",
    "Don't try to memorize this. We will revisit it in the next lecture.\n",
    "\n",
    "Different ways of constructing the compositional structure of $f$ lead to different *architectures* such as fully connected networks (shown above), *recurrent neural networks*, *convolutional neural networks*, *autoencoders*, *residual networks* etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Activation functions \n",
    "\n",
    "The most common activation functions include the rectified Linear Units or ReLU (and variants), sigmoid functions, hyperolic tangents, sinusoids, step functions etc. We will visualize them in the hands-on activity.\n",
    "\n",
    "### Universal theorem for neural networks\n",
    "\n",
    "The [universal approximation theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) guarantees that DNNs are really good function approximators.\n",
    "In plain English, the (original) theorem states that if you take any decent activation function and build with it a dense neural network you can approximate any continuous function (defined on a compact input domain) arbitrarily well if you keep increasing the number of neurons you use.\n",
    "Recently, researchers have proven similar theorems for deep neural networks.\n",
    "In general, you can rest assured that if you grow your network by adding neurons and layers it can approximate pretty much anything you may need.\n",
    "That's one of the reasons why deep neural networks have been (re)gaining momentum recently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training regression networks - Loss function view\n",
    "\n",
    "Assume that you want to solve a regression problem.\n",
    "You have input data:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{1:n} = (\\mathbf{x}_1,\\dots,\\mathbf{x}_n),\n",
    "$$\n",
    "\n",
    "and output data:\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_{1:n} = (y_1,\\dots,y_n).\n",
    "$$\n",
    "\n",
    "You want to use them to find the map between $\\mathbf{x}$ and $y$ using DNNs.\n",
    "\n",
    "Well, you start by using a DNN $y=f(\\mathbf{x};\\theta)$ to represent the map from $\\mathbf{x}$ to $y$.\n",
    "Here $\\theta$ are the parameters of the network (the weights and biases of all the layrs).\n",
    "Your problem is to fit $\\theta$ to the available data.\n",
    "\n",
    "The simplest way forward is to follow a least-squares approach.\n",
    "First, define a, so-called, loss function:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-f(\\mathbf{x}_i;\\theta)\\right)^2.\n",
    "$$\n",
    "\n",
    "This loss function is the sum of the squares of the prediction error of the DNN for a given $\\theta$.\n",
    "Once you have the loss function, you can fit $\\theta$ by minimizing it:\n",
    "\n",
    "$$\n",
    "\\theta^* = \\arg\\min L(\\theta).\n",
    "$$\n",
    "\n",
    "However, this minimization problem does not have an analytical solution.\n",
    "Neither does it have a unique solution.\n",
    "It is a non-linear, non-convex optimization problem.\n",
    "It requries special treatement.\n",
    "We will talk about it in a while.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training regression networks - Probabilistic view\n",
    "\n",
    "Sometimes is it not clear how to come up with loss functions.\n",
    "In such situations we can employ a probabilistic view.\n",
    "We need to come up with a likelihood function that helps us connect the model to the observed data.\n",
    "So, in general, we need to come up with:\n",
    "$\n",
    "p(y_{1:n}|\\mathbf{x}_{1:n},\\theta).\n",
    "$\n",
    "Then we can fit the parameters by maximizing the log-likelihood, which is the same as minimizing the \"loss\" function:\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\log p(y_{1:n}|\\mathbf{x}_{1:n},\\theta).\n",
    "$$\n",
    "\n",
    "This approach is going to give you the same thing as a the classic approach under the following assumptions:\n",
    "- the observations are independent (conditional on the model)\n",
    "- the measurement noise is Gaussian with mean given by the DNN and a constant variance.\n",
    "\n",
    "Let's show this.\n",
    "Take:\n",
    "\n",
    "\\begin{split}\n",
    "p(y_i|\\mathbf{x}_i,\\theta) &= N(y_i | f(\\mathbf{x}_i;\\theta), \\sigma^2)\\\\\n",
    "&= \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left\\{-\\frac{\\left(y_i-f(\\mathbf{x}_i;\\theta)\\right)^2}{2\\sigma^2}\\right\\},\n",
    "\\end{split}\n",
    "\n",
    "where $\\sigma^2$ is the measurement noise variance.\n",
    "Then, from independence, we have:\n",
    "\n",
    "$$\n",
    "p(y_{1:n}|\\mathbf{x}_{1:n},\\theta) = \\prod_{i=1}^np(y_i|\\mathbf{x}_i,\\theta).\n",
    "$$\n",
    "\n",
    "So, we should be minimizing:\n",
    "\n",
    "\\begin{split}\n",
    "L'(\\theta) &= -\\log p(y_{1:n}|\\mathbf{x}_{1:n},\\theta)\\\\\n",
    "&= -\\sum_{i=1}^n\\log p(y_i|\\mathbf{x}_i,\\theta)\\\\\n",
    "&= \\frac{1}{2\\sigma^2}\\sum_{i=1}^n\\left(y_i-f(\\mathbf{x}_i;\\theta)\\right)^2 + \\text{const}.\n",
    "\\end{split}\n",
    "\n",
    "Well, that's the same (up to an additive constant) as the $L(\\theta)$ we had before.\n",
    "The benefit of the probabilistic approach is that it allows you to be more flexible with the way you model the measurement process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The minimization problem as a stochastic optimization problem\n",
    "\n",
    "As I mentioned earlier, $L(\\theta)$ is non-linear and non-convext.\n",
    "Classic, gradient-based, optimization techniques do not work well on it.\n",
    "They tend to get trapped in bad local minima.\n",
    "Adding a little bit of stochasticity in the optimization algorithm helps it avoid these bad local minima.\n",
    "Such *stochastic optimization algorithms* are still finding local minima, but they are better ones!\n",
    "\n",
    "Another potential problem is that $L(\\theta)$ may involve a summation over millions of observations (in the case of big data).\n",
    "In this regime, gradient-based optimization algorithms are also computationally inefficient.\n",
    "Stochastic optimization algorithms subsample the available data allowing you to break them down into computationally digestible *batches*.\n",
    "\n",
    "Let's first say what a stochastic optimization problem is.\n",
    "Then we are going to show how we can recast a typical $\\min L(\\theta)$ problema as a stochastic optimization problem.\n",
    "A stochastic optimization problem, is a problem of the form:\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathbb{E}_Z[\\ell(\\theta;Z)],\n",
    "$$\n",
    "\n",
    "where $\\ell(\\theta;Z)$ is some scalar function of $\\theta$ and the random vector $Z$.\n",
    "The expectation is over $Z$.\n",
    "Basically, you just want to minimize the expectation over $Z$ of $\\ell(\\theta;Z)$.\n",
    "That's it.\n",
    "\n",
    "Okay. Back to our original problem.\n",
    "Take:\n",
    "\n",
    "$$\n",
    "L(\\theta) = \\frac{1}{n}\\sum_{i=1}^n \\left(y_i-f(\\mathbf{x}_i;\\theta)\\right)^2.\n",
    "$$\n",
    "\n",
    "We need to write this as an expectation of something.\n",
    "But an expectation of what?\n",
    "Well, it is going to be an expectation over randomly selected batches of the observed data.\n",
    "This is by no means the only choice. But it is a very useful choice.\n",
    "Let's see how we can do this.\n",
    "\n",
    "First, let's visit the observations one by one.\n",
    "Take $I$ to be a Categorical random variable that picks with equal probability the index of one of the $n$ observations, i.e.,\n",
    "\n",
    "$$\n",
    "I \\sim \\operatorname{Categorical}\\left(\\frac{1}{n},\\dots,\\frac{1}{n}\\right).\n",
    "$$\n",
    "\n",
    "Take:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta;I) = \\left(y_I-f(\\mathbf{x}_I;\\theta)\\right)^2.\n",
    "$$\n",
    "\n",
    "So, here $Z = I$.\n",
    "Let's take the expectation over $I$ and see what it is going to give us:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbb{E}_I[\\ell(\\theta;I)] &= \\sum_{i=1}^np(I=i)\\ell(\\theta;i)\\\\\n",
    "&= \\sum_{i=1}^n\\frac{1}{n}\\left(y_i-f(\\mathbf{x}_i;\\theta)\\right)^2\\\\\n",
    "&= \\frac{1}{n}\\sum_{i=1}^n\\left(y_i-f(\\mathbf{x}_i;\\theta)\\right)^2\\\\\n",
    "&= L(\\theta).\n",
    "\\end{split}\n",
    "\n",
    "Great! Minimizing $L(\\theta)$ is the same as minimizing $\\mathbb{E}_I[\\ell(\\theta;I)]$.\n",
    "\n",
    "Let's now do it again, but using an $m$-sized randomly selected batch from the observed data.\n",
    "Take $I_1,I_2,\\dots,I_m$ to be independent and identically distributed Categoricals that pick with equal probability an index from 1 to $n$.\n",
    "Then define:\n",
    "\n",
    "$$\n",
    "\\ell_m(\\theta;I_{1:m}) = \\frac{1}{m}\\sum_{j=1}^m\\left(y_{I_j}-f(\\mathbf{x}_{I_j};\\theta)\\right)^2.\n",
    "$$\n",
    "\n",
    "So, here $Z = (I_1,\\dots,I_m)$.\n",
    "Now take the expectation of this over the $I$'s:\n",
    "\n",
    "\\begin{split}\n",
    "\\mathbb{E}[\\ell_m(\\theta;I_{1:m})] &=\n",
    "\\mathbb{E}\\left[\\frac{1}{m}\\sum_{j=1}^m\\left(y_{I_j}-f(\\mathbf{x}_{I_j};\\theta)\\right)^2\\right]\\\\\n",
    "&= \\frac{1}{m}\\sum_{j=1}^m\\mathbb{E}\\left[\\left(y_{I_j}-f(\\mathbf{x}_{I_j};\\theta)\\right)^2\\right]\\\\\n",
    "&= \\frac{1}{m}\\sum_{j=1}^m L(\\theta)\\\\\n",
    "&= \\frac{m}{m}L(\\theta)\\\\\n",
    "&= L(\\theta),\n",
    "\\end{split}\n",
    "\n",
    "where we have used that $\\mathbb{E}\\left[\\left(y_{I_j}-f(\\mathbf{x}_{I_j};\\theta)\\right)^2\\right] = L(\\theta)$ since it follows from our previous analysis.\n",
    "Therefore, minimizing $L(\\theta)$ is the same as minimizing the expectation of $\\ell_m(\\theta;I_{1:m})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Robbins-Monro algorithm\n",
    "\n",
    "We reached the point where we can discuss the simplest variant of a stochastic optimization algorithm.\n",
    "It is known as the *stochastic gradient descent* or the [Robbins-Monro algorithm](https://projecteuclid.org/euclid.aoms/1177729586).\n",
    "It goes as follows.\n",
    "Take the stochastic optimization problem:\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\mathbb{E}_Z[\\ell(\\theta;Z)].\n",
    "$$\n",
    "\n",
    "And consider the RM algorithm:\n",
    "+ initialize $\\theta$ to $\\theta_0$\n",
    "+ Iterate:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha_t \\nabla_{\\theta}\\ell(\\theta_t,z_t),\n",
    "$$\n",
    "\n",
    "where $z_t$ are independent samples of $Z$.\n",
    "\n",
    "In this algorithm, $\\theta_t$ is gradually evolved following a noisy gradient signal.\n",
    "The sequence $\\alpha_t$ is known as the *learning rate* and it is our choice.\n",
    "The Robbins-Monro theorem gaurantees that the RM algorithm converges to a local minimum of the expectation $\\mathbb{E}[\\ell(\\theta,Z)]$ if the learning rate satisfies the following properties:\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^\\infty \\alpha_t = +\\infty,\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\sum_{t=1}^\\infty \\alpha_t^2 < +\\infty.\n",
    "$$\n",
    "\n",
    "Intuitively, these properties say that the learning rate should converge to zero (this is an implication of the convergence of the second series) but not too fast (an implication of the divergence of the first series).\n",
    "There are many sequencies of learning rates that satisfy these constraints.\n",
    "Here is a very commonly used one:\n",
    "\n",
    "$$\n",
    "\\alpha_t = \\frac{A}{(Bt + C)^\\rho},\n",
    "$$\n",
    "\n",
    "with $\\rho$ a number between $0.5$ and $1$ (exclusive)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application of the Robbins-Monro algorithm to training regression networks\n",
    "\n",
    "The algorithm for training regression networks becomes:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\alpha_t\\nabla_{\\theta} \\frac{1}{m}\\sum_{j=1}^m\\left(y_{i_{tj}}-f(\\mathbf{x}_{i_{tj}};\\theta_t)\\right)^2,\n",
    "$$\n",
    "\n",
    "where $i_{t1},\\dots,i_{tm}$ are randomly selected indices of the observation data.\n",
    "Using properties of the gradient, you can also write this as:\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - 2\\alpha_t \\frac{1}{m}\\sum_{j=1}^m\\left(y_{i_{tj}}-f(\\mathbf{x}_{i_{tj}};\\theta_t)\\right)\\nabla_{\\theta}f(\\mathbf{x}_{i_{tj}};\\theta_t).\n",
    "$$\n",
    "\n",
    "That's pretty much it...\n",
    "\n",
    "Notice that to carry out the algorithm, we need to $\\nabla_{\\theta}f(\\mathbf{x}_{i_{tj}};\\theta_t)$, i.e., the gradient of the neural network output with respect to the parameters (weights and biases).\n",
    "This is done using the chain rule.\n",
    "The algorithm is known as the [back-propagation algorithm](https://en.wikipedia.org/wiki/Backpropagation).\n",
    "We are not going to cover it.\n",
    "Nowadays, you don't have to worry about derivatives.\n",
    "Software like [PyTorch](https://pytorch.org/), [TensorFlow](https://www.tensorflow.org/) and [JAX](https://jax.readthedocs.io/en/latest/index.html#) can find the derivatives for you.\n",
    "In the hands-on activity, I will introduce you to PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced variations of stochastic gradient descent\n",
    "\n",
    "The RM algorithm is the simplest stochastic optimization algorithm that I could explain in a lecture.\n",
    "It works, but it is not the most commonly used.\n",
    "There are more powerful algorithms like *stochastic gradient descent with momentum*, *AdaGrad*, or *Adam* (adaptive moment estimation).\n",
    "I will show you in the hands-on activities how you can use these algorithms as implemented in PyTorch, but I am not going to explain their details.\n",
    "If you want to know the details, please read Chapter 8 of the deep learning book referenced at the very beginning of this document."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
