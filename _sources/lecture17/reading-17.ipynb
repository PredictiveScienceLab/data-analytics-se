{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "\n",
    "You are given a some observations $\\mathbf{x}_{1:n} = (\\mathbf{x}_1,\\dots,\\mathbf{x}_n)$, e.g., a bunch of pictures, and you want to find some structure in the data.\n",
    "This is [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning).\n",
    "The key difference from supervised learning is that you do nopt have any labels/targets/outputs.\n",
    "The unsupervised learning problem may sound very open ended (and it is), but some specific examples we are going to study:\n",
    "\n",
    "+ [Clustering](https://en.wikipedia.org/wiki/Cluster_analysis): Split the observations into, say $K$, distinct clusters.\n",
    "+ [Dimensionality reduction](https://en.wikipedia.org/wiki/Dimensionality_reduction): Reduce the dimensionality of the data.\n",
    "+ [Density estimation](https://en.wikipedia.org/wiki/Density_estimation): Learn the probability density that gave rise to the data, i.e., learn how to generate new samples with similar features as the observations.\n",
    "+ and [more](https://en.wikipedia.org/wiki/Unsupervised_learning).\n",
    "\n",
    "Each one of these unsupervised learning problem requires several lectures to develop fully. Since we do not have this kind of time, we are going to study the most basic algorithms for the three most popular unsupervised learning problems (clustering, density estimation, and dimensionality reduction) in this and the next lecture.\n",
    "\n",
    "## Clustering using the K-means algorithm\n",
    "\n",
    "K-means is the simplest algorithm for spliting the dataset $\\mathbf{x}_{1:n}$ in $K$ clusters.\n",
    "The mathematical details of the algorithm are described in the video lectures that follow.\n",
    "If you want study the algorithm independently, I suggest reading [Chapter 20.1, D. MacKay (2003)](http://www.inference.org.uk/mackay/itprnn/ps/284.292.pdf).\n",
    "\n",
    "## Density estimation using mixtures of Gaussians\n",
    "\n",
    "As we will argue in the video, the clustering problem is related to the density estimation problem.\n",
    "In the density estimation problem we are trying to learn a model $p(\\mathbf{x})$ that allows us to generate examples similar to our observations.\n",
    "We are going to work with the Gaussian mixture model which has this form:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x}) = \\sum_{k=1}^K\\pi_kN(\\mathbf{x}|\\boldsymbol{\\mu}_k,\\boldsymbol{\\Sigma}_k),\n",
    "$$\n",
    "\n",
    "where $\\pi_k, \\boldsymbol{\\mu}_k$, and $\\boldsymbol{\\Sigma}_k$ are parameters to be estimated from the data.\n",
    "The Gaussian mixture model basically assumes that there are $K$ possible sources that may generate the data and that each one of them is a multivariate Gaussian with parameters to be estimated.\n",
    "Solving the density estimation problem using mixtures of Gaussian essentially solves the clustering problem as well because you can think of the $K$ different Gaussians as defining the clusters.\n",
    "The complete details will be developed in the video lectures.\n",
    "If you want to study Gaussian mixtures independently, I suggest reading [Chapter 9, Bishop (2006)](https://www.springer.com/gp/book/9780387310732) (You should have free online access to the book through Purdue).\n",
    "\n",
    "## Avoiding overfitting using the Bayesian information criterion\n",
    "\n",
    "If you pick a model with too many parameters you may overfit. If you pick a model with too few parameters, you may underfit.\n",
    "In the Gaussian mixtures example above, you get to choose $K$.\n",
    "You can choose $K$ to be one, and then you are essentially saying that there is a single cluster that can be descibed with a multivariate Gaussian.\n",
    "This may be inadequate and you are going to underfit the data.\n",
    "On the other extreme, you may pick $K$ to be $n$, i.e., the number of observations that you have.\n",
    "In this case, you would be essentially fitting a different Gaussian on each observed point.\n",
    "This is of course ridiculous. You are definitely going to overfit.\n",
    "\n",
    "So, how do you pick the right number of mixture components $K$?\n",
    "The formal answer is that you should be Bayesian all the way.\n",
    "Put a prior probability on $K$, the characterize the posterior over $K$.\n",
    "This is doable, it is known as Bayesian model selection (see [Chapter 1.3, Bishop (2006)](https://www.springer.com/gp/book/9780387310732), but we are not yet equipped to carry it out.\n",
    "However, there is an approximation to Bayesian model selection that is very straightforward to carrty out: the [Bayesian information criterion](https://en.wikipedia.org/wiki/Bayesian_information_criterion) or BIC.\n",
    "The details are presented in the video lecture, but if you want to study this independently the wikipedia article on [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is well written and you may also want to take a look at [Chapter 4.4.1, Bishop (2006)](https://www.springer.com/gp/book/9780387310732)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
