{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 7 - Part B\n",
    "\n",
    "*Note that there are two different notebooks for HW assignment 7. This is part A. There will be two different assignments in gradescope for each part. The deadlines are the same for both parts.*\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 27-28 (inclusive).\n",
    "\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')\n",
    "import seaborn as sns\n",
    "sns.set_context(\"paper\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download(\n",
    "    url : str,\n",
    "    local_filename : str = None\n",
    "):\n",
    "    \"\"\"Download a file from a url.\n",
    "    \n",
    "    Arguments\n",
    "    url            -- The url we want to download.\n",
    "    local_filename -- The filemame to write on. If not\n",
    "                      specified \n",
    "    \"\"\"\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    urllib.request.urlretrieve(url, local_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Google colab\n",
    "!pip install pyro-ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro\n",
    "import pyro.distributions as dist\n",
    "from pyro.infer import MCMC, NUTS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1  - Bayesian Linear regression on steroids \n",
    "\n",
    "The purpose of this problem is to demonstrate that we have learned enough to do very complicated things.\n",
    "In the first part, we will do Bayesian linear regression with radial basis functions (RBFs) in which we characterize the posterior of all parameters, including the length-scales of the RBFs.\n",
    "In the second part, we are going to build a model that has an input-varying noise. Such models are called heteroscedastic models. \n",
    "\n",
    "We need to write some `pytorch` code to compute the design matrix. This is absolutely necessary so that `pyro` can differentiate through all expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadialBasisFunctions(torch.nn.Module):\n",
    "    \"\"\"Radial basis functions basis.\n",
    "    \n",
    "    Arguments:\n",
    "    X   -  The centers of the radial basis functions.\n",
    "    ell -  The assumed length scale.\n",
    "    \"\"\"\n",
    "    def __init__(self, X, ell):\n",
    "        super().__init__()\n",
    "        self.X = X\n",
    "        self.ell = ell\n",
    "        self.num_basis = X.shape[0]\n",
    "    def forward(self, x):\n",
    "        distances = torch.cdist(x, self.X)\n",
    "        return torch.exp(-.5 * distances ** 2 / self.ell ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can use them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the basis\n",
    "x_centers = torch.linspace(-1, 1, 10).unsqueeze(-1)\n",
    "ell = 0.2\n",
    "basis = RadialBasisFunctions(x_centers, ell)\n",
    "\n",
    "# Some points (need to be N x 1)\n",
    "x = torch.linspace(-1, 1, 100).unsqueeze(-1)\n",
    "\n",
    "# Evaluate the basis\n",
    "Phi = basis(x)\n",
    "\n",
    "# Here is the shape of Phi\n",
    "print(Phi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how they look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(Phi.shape[1]):\n",
    "    ax.plot(x, Phi[:, i], label=f\"$\\phi_{i}$\")\n",
    "ax.set(xlabel=\"$x$\", ylabel=\"$\\phi(x)$\")\n",
    "ax.legend(loc=\"best\", frameon=False)\n",
    "sns.despine(trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A - Hierarchical Bayesian linear regression with input-independent noise\n",
    "\n",
    "We will analyze the motorcycle dataset. The data is loaded below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/lecturebook/data/motor.dat\"\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the scaled data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = np.loadtxt('motor.dat')\n",
    "scaler = StandardScaler()\n",
    "data = scaler.fit_transform(data)\n",
    "X = torch.tensor(data[:, 0], dtype=torch.float32).unsqueeze(-1)\n",
    "Y = torch.tensor(data[:, 1], dtype=torch.float32)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, Y, 'x')\n",
    "ax.set(xlabel=\"$x$\", ylabel=\"$y$\")\n",
    "sns.despine(trim=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.I\n",
    "\n",
    "Your goal is to implement the model described below.\n",
    "We use the radial basis functions (`RadialBasisFunction`) with centers, $x_i$ at $m=50$ equidistant points between the minimum and maximum of the observed inputs:\n",
    "\n",
    "$$\n",
    "\\phi_i(x;\\ell) = \\exp \\left( - \\frac{(x - x_i)^2}{2 \\ell^2} \\right),\n",
    "$$\n",
    "\n",
    "for $i=1,\\dots,m$.\n",
    "We denote the vector of RBFs evaluated at $x$ as $\\boldsymbol{\\phi}(x;\\ell)$.\n",
    "\n",
    "We are not going to pick the length-scales $\\ell$ by hand. Instead, we will put a prior on it:\n",
    "\n",
    "$$\n",
    "\\ell \\sim \\text{Exponential}(1).\n",
    "$$\n",
    "\n",
    "The corresponding weights have priors:\n",
    "\n",
    "$$\n",
    "w_j | \\alpha_i \\sim N(0, \\alpha_j^2),\n",
    "$$\n",
    "\n",
    "and its $\\alpha_j$ has a prior:\n",
    "\n",
    "$$\n",
    "\\alpha_j \\sim \\text{Exponential}(1),\n",
    "$$\n",
    "\n",
    "for $j=1,\\dots,m$.\n",
    "\n",
    "Denote our data as:\n",
    "\n",
    "$$\n",
    "x_{1:n} = (x_1, \\dots, x_n)^T,\\;\\text{(inputs)},\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "y_{1:n} = (y_1, \\dots, y_n)^T,\\;\\text{(outputs)}.\n",
    "$$\n",
    "\n",
    "The likelihood of the data is:\n",
    "\n",
    "$$\n",
    "y_i | \\mathbf{w}, \\sigma \\sim N(\\mathbf{w}^T \\boldsymbol{\\phi}(x_i;\\ell), \\sigma^2),\n",
    "$$\n",
    "\n",
    "for $i=1,\\dots,n$.\n",
    "\n",
    "$$\n",
    "y_n | \\ell, \\mathbf{w}, \\sigma \\sim N(\\mathbf{w}^T \\boldsymbol{\\phi}(x_n;\\ell), \\sigma^2).\n",
    "$$\n",
    "\n",
    "Complete the `pyro` implementation of that model:\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y, num_centers=50):\n",
    "    with pyro.plate(\"centers\", num_centers):\n",
    "        alpha = pyro.sample(\"alpha\", dist.Exponential(1.0))\n",
    "        # Notice below that dist.Normal needs the standard deviation - not the variance\n",
    "        # We follow a different convention in the lecture notes\n",
    "        w = pyro.sample(\"w\", dist.Normal(0.0, alpha))\n",
    "    ell = # Complete the code assign to ell the correct prior distribution (an Exponential(1)).\n",
    "    # Hint: Look at alpha.\n",
    "    sigma = # Complete the code assign to sigma the correct prior distribution (an Exponential(1))\n",
    "    x_centers = torch.linspace(X.min(), X.max(), num_centers).unsqueeze(-1)\n",
    "    Phi = RadialBasisFunctions(x_centers, ell)(X)\n",
    "    with pyro.plate(\"data\", X.shape[0]):\n",
    "        pyro.sample(\"y\", dist.Normal(Phi @ w, sigma), obs=y)\n",
    "    # Notice that I'm making the model return all the variables that I have made.\n",
    "    # This is not essential for characterizing the posterior, but it does reduce redundant code\n",
    "    # when we are trying to get the posterior predictive.\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph will help to understand the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.render_model(model, (X, Y), render_distributions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `pyro.infer.autoguide.AutoDiagonalNormal` to make the guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guide = pyro.infer.autoguide.AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use variational inference. Here is the training code from the hans-on activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, guide, data, num_iter=5_000):\n",
    "    \"\"\"Train a model with a guide.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    model    -- The model to train.\n",
    "    guide    -- The guide to train.\n",
    "    data     -- The data to train the model with.\n",
    "    num_iter -- The number of iterations to train.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    elbos -- The ELBOs for each iteration.\n",
    "    param_store -- The parameters of the model.\n",
    "    \"\"\"\n",
    "\n",
    "    pyro.clear_param_store()\n",
    "\n",
    "    optimizer = pyro.optim.Adam({\"lr\": 0.001})\n",
    "\n",
    "    svi = pyro.infer.SVI(\n",
    "        model,\n",
    "        guide,\n",
    "        optimizer,\n",
    "        loss=pyro.infer.JitTrace_ELBO()\n",
    "    )\n",
    "\n",
    "    elbos = []\n",
    "    for i in range(num_iter):\n",
    "        loss = svi.step(*data)\n",
    "        elbos.append(-loss)\n",
    "        if i % 1_000 == 0:\n",
    "            print(f\"Iteration: {i} Loss: {loss}\")\n",
    "\n",
    "    return elbos, pyro.get_param_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.II\n",
    "\n",
    "Train the model for 20,000 iterations. Call the `train()` function we defined above to do it.\n",
    "Make sure you store the returned elbo values because you will need them later.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.III\n",
    "\n",
    "Plot the evolution of the ELBO.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.IV\n",
    "\n",
    "Take 1,000 posterior samples.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "I'm giving you this one because it is a bit tricky. You need to use the `pyro.infer.Predictive` class to do it. Here is how you can use it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples = pyro.infer.Predictive(model, guide=guide, num_samples=10)(X, Y)\n",
    "# Just modify the call to get the right number of samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.V\n",
    "\n",
    "Plot the histograms of the posteriors of $\\ell$, $\\sigma$, $\\alpha_{10}$ and $w_{10}$.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, here is how to extract the samples.\n",
    "ell = post_samples[\"ell\"]\n",
    "# You can do `post_samples.keys()` to see all the keys.\n",
    "# But they should correspond to the names of the latent variables in the model.\n",
    "sigma = # Your code here\n",
    "alphas = # Your code here\n",
    "ws = # Your code here\n",
    "\n",
    "# Here is the code to make the histogram for the length scale.\n",
    "fig, ax = plt.subplots()\n",
    "# **VERY IMPORTANT** - You need to detach the tensor from the computational graph.\n",
    "# Otherwise, you will get very very strange behavior.\n",
    "ax.hist(ell.detach().numpy(), bins=20, alpha=.5)\n",
    "ax.set(xlabel=\"$\\ell$\", ylabel=\"Frequency\")\n",
    "sns.despine(trim=True);\n",
    "\n",
    "# Your code for the other histograms here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.VI\n",
    "\n",
    "Let's extend them model to make predictions.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, I'm giving you most of the code here.\n",
    "\n",
    "def predictive_model(X, y, num_centers=50):\n",
    "    # First we run the original model get all the variables\n",
    "    params = model(X, y, num_centers)\n",
    "    # Here is how you can access the variables\n",
    "    w = params[\"w\"]\n",
    "    ell = # Access the length scale\n",
    "    sigma = # Access the standard deviation of the measurement noise\n",
    "    x_centers = # Access the centers of the radial basis functions\n",
    "    # Here are the points where we want to make predictions\n",
    "    xs = torch.linspace(X.min(), X.max(), 100).unsqueeze(-1)\n",
    "    # Evaluate the basis on the prediction points\n",
    "    Phi = RadialBasisFunctions(x_centers, ell)(xs)\n",
    "    # Make the predictions - we use a deterministic node here because we want to\n",
    "    # save the results of the predictions.\n",
    "    predictions = pyro.deterministic(\"predictions\", Phi @ w)\n",
    "    # Finally, we add the measurement noise\n",
    "    predictions_with_noise = pyro.sample(\"predictions_with_noise\", dist.Normal(predictions, sigma))\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.VII\n",
    "\n",
    "Extract the posterior predictive distribution using 10,000 samples. Separate aleatory and epistemic uncertainty.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how to make the predictions. Just change the number of samples to the right number.\n",
    "post_pred = pyro.infer.Predictive(predictive_model, guide=guide, num_samples=10)(X, Y)\n",
    "# We will predict here:\n",
    "xs = torch.linspace(X.min(), X.max(), 100).unsqueeze(-1)\n",
    "# You can extract the predictions from post_pred like this:\n",
    "predictions = post_pred[\"predictions\"]\n",
    "# Note that we extracted the deterministic node called \"predictions\" from the model.\n",
    "# Get the epistemic uncertainty in the usual way:\n",
    "p_500, p_025, p_975 = np.percentile(predictions, [50, 2.5, 97.5], axis=0)\n",
    "# Extract predictions with noise\n",
    "predictions_with_noise = # Your code here\n",
    "# Get the aleatory uncertainty\n",
    "ap_025, ap_975 = # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A.VIII\n",
    "\n",
    "Plot the data, the median, the 95% credible interval of epistemic uncertainty and the 95% credible interval of aleatory uncertainty, along with five samples from the posterior.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. You have everything you need to make the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Heteroscedastic regression\n",
    "\n",
    "We are going to build a model that has an input-varying noise. Such models are called heteroscedastic models.\n",
    "Here I will let you do more of the work.\n",
    "\n",
    "Everything is as before for $\\ell$, the $\\alpha_j$'s, and the $w_j$'s.\n",
    "We now introduce a model for the noise that is input dependent.\n",
    "It will use the same RBFs as the mean function.\n",
    "But let's use a different length-scale, $\\ell_\\sigma$.\n",
    "So, we add:\n",
    "\n",
    "$$\n",
    "\\ell_\\sigma \\sim \\text{Exponential}(1),\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha_{\\sigma,j} \\sim \\text{Exponential}(1),\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "w_{\\sigma,j} | \\alpha_{\\sigma,j} \\sim N(0, \\alpha_{\\sigma,j}^2),\n",
    "$$\n",
    "\n",
    "for $j=1,\\dots,m$.\n",
    "\n",
    "Our model for the input-dependent noise variance is:\n",
    "\n",
    "$$\n",
    "\\sigma(x;\\mathbf{w}_\\sigma,\\ell) = \\exp\\left(\\mathbf{w}_\\sigma^T \\boldsymbol{\\phi}(x;\\ell_\\sigma)\\right).\n",
    "$$\n",
    "\n",
    "So, the likelihood of the data is:\n",
    "\n",
    "$$\n",
    "y_i | \\mathbf{w}, \\mathbf{w}_\\sigma \\sim N\\left(\\mathbf{w}^T \\boldsymbol{\\phi}(x_i;\\ell), \\sigma^2(x_i;\\mathbf{w}_\\sigma,\\ell)\\right),\n",
    "$$\n",
    "\n",
    "You will implement this model.\n",
    "\n",
    "### Part B.I\n",
    "\n",
    "Complete the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, y, num_centers=50):\n",
    "    with pyro.plate(\"centers\", num_centers):\n",
    "        alpha = pyro.sample(\"alpha\", dist.Exponential(1.0))\n",
    "        w = pyro.sample(\"w\", dist.Normal(0.0, alpha))\n",
    "        # Let's add the generalized linear model for the log noise.\n",
    "        alpha_noise = # Your code here\n",
    "        w_noise = # Your code here\n",
    "    ell = pyro.sample(\"ell\", dist.Exponential(1.))\n",
    "    ell_noise = # Your code here\n",
    "    x_centers = torch.linspace(X.min(), X.max(), num_centers).unsqueeze(-1)\n",
    "    Phi = RadialBasisFunctions(x_centers, ell)(X)\n",
    "    Phi_noise = # Your code here\n",
    "    # This is the new part 2/2\n",
    "    model_mean = Phi @ w\n",
    "    sigma = # Your code here (torch.exp(<something>))\n",
    "    with pyro.plate(\"data\", X.shape[0]):\n",
    "        pyro.sample(\"y\", dist.Normal(model_mean, sigma), obs=y)\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a `pyro.infer.autoguide.AutoDiagonalNormal` guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the graph of the model using `pyro` functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.II\n",
    "\n",
    "Train the model using 20,000 iterations. Then plot the evolution of the ELBO.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.III\n",
    "\n",
    "Extend the model to make predictions.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictive_model(X, y, num_centers=50):\n",
    "    params = model(X, y, num_centers)\n",
    "    w = params[\"w\"]\n",
    "    w_noise = # Your code here\n",
    "    ell = # Your code here\n",
    "    ell_noise = # Your code here\n",
    "    sigma = # Your code here\n",
    "    x_centers = params[\"x_centers\"]\n",
    "    xs = torch.linspace(X.min(), X.max(), 100).unsqueeze(-1)\n",
    "    Phi = # Your code here\n",
    "    Phi_noise = # Your code here\n",
    "    predictions = pyro.deterministic(\"predictions\", Phi @ w)\n",
    "    sigma = # Your code here (pyro.deterministic(\"sigma\", <something>))\n",
    "    predictions_with_noise = # Your code here\n",
    "    return locals()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.IV\n",
    "\n",
    "Now, make predictions and calculate the epistemic and aleatory uncertainties as in part A.VII.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.V\n",
    "\n",
    "Make the same plot as in part A.VIII.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.VI\n",
    "\n",
    "Plot the estimated noise standard deviation as a function of of the input along with a 95% credible interval.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.VII\n",
    "\n",
    "Which model do you prefer? Why?\n",
    "\n",
    "**Answer:**\n",
    "<br><br><br><br><br><br><br><br><br><br><br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B.IX\n",
    "\n",
    "Can you think of any way to improve the model?\n",
    "Go crazy! This is the last homework assignment!\n",
    "There is no right or wrong answer here.\n",
    "But if you have a good idea, we will give you extra credit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code and answers here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
