
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Selecting Prior Information &#8212; Introduction to Scientific Machine Learning (Lecture Book)</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet">
  <link href="../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Information Entropy" href="hands-on-11.1.html" />
    <link rel="prev" title="Lecture 11 - Selecting Prior Information" href="intro.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Introduction to Scientific Machine Learning (Lecture Book)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../index.html">
   Preface
  </a>
 </li>
</ul>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../introduction.html">
   Introduction
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture01/intro.html">
     Lecture 1 - Introduction to Predictive Modeling
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
    <label for="toctree-checkbox-2">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture01/reading-01.html">
       Predictive Modeling and Scientific Machine Learning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture01/hands-on-01.1.html">
       The Uncertainty Propagation Problem
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture01/hands-on-01.2.html">
       The Model Calibration Problem
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../review_probability.html">
   Review of Probability
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
  <label for="toctree-checkbox-3">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture02/intro.html">
     Lecture 2 - Basics of Probability Theory
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
    <label for="toctree-checkbox-4">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture02/reading-02.html">
       Basics of Probability Theory
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture02/hands-on-02.html">
       Experiment with “Ranomness”
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture03/intro.html">
     Lecture 3 - Discrete Random Variables
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture03/reading-03.html">
       Discrete Random Variables
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture03/hands-on-03.html">
       Discrete Random Variables in Python
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture04/intro.html">
     Lecture 4 - Continuous Random Variables
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture04/reading-04.html">
       Continuous Random Variables
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture04/hands-on-04.1.html">
       The Uniform Distribution
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture04/hands-on-04.2.html">
       The Gaussian Distribution
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture05/intro.html">
     Lecture 5 - Collections of Random Variables
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
    <label for="toctree-checkbox-7">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture05/reading-05.html">
       Collections of Random Variables: Theory
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture05/hands-on-05.html">
       Practicing with joint probability mass functions
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture06/intro.html">
     Lecture 6 - Random Vectors
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture06/reading-06.html">
       Random Vectors
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture06/hands-on-06.1.html">
       The Multivariate Normal - Diagonal Covariance Case
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture06/hands-on-06.2.html">
       The Multivariate Normal - Full Covariance Case
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture06/hands-on-06.3.html">
       The Multivariate Normal - Marginalization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture06/hands-on-06.4.html">
       The Multivariate Normal - Conditioning
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../uncertainty_propagation.html">
   Uncertainty Propagation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture07/intro.html">
     Lecture 7 - Basic Sampling
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
    <label for="toctree-checkbox-10">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture07/hands-on-07.1.html">
       Pseudo-random number generators
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture07/hands-on-07.2.html">
       Sampling the uniform
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture07/hands-on-07.3.html">
       Sampling the categorical
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture07/hands-on-07.4.html">
       Sampling from continuous distributions - Inverse sampling
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture08/intro.html">
     Lecture 8 - The Monte Carlo Method for Estimating Expectations
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
    <label for="toctree-checkbox-11">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture08/hands-on-08.3.html">
       Sampling Estimates of Expectations
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture08/hands-on-08.4.html">
       Sampling Estimates of Variance
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture09/intro.html">
     Lecture 9 - Monte Carlo Estimates of Various Statistics
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
    <label for="toctree-checkbox-12">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture09/hands-on-09.1.html">
       Sampling Estimates of the Cumulative Distribution Function
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture09/hands-on-09.2.html">
       Sampling Estimates of the Probability Density via Histograms
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture09/hands-on-09.3.html">
       Hands-on Activity 9.3: Sampling Estimates of Predictive Quantiles
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture09/hands-on-09.4.html">
       Propagating Uncertainties through an Ordinrary Differential Equation
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture10/intro.html">
     Lecture 10 - Quantify Uncertainty in Monte Carlo Estimates
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
    <label for="toctree-checkbox-13">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture10/hands-on-10.1.html">
       Visualizing Monte Carlo Uncertainty
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture10/hands-on-10.2.html">
       The Central Limit Theorem
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture10/hands-on-10.3.html">
       Quanifying Epistemic Uncertainty in Monte Carlo estimates
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture10/hands-on-10.4.html">
       Uncertainty Propagation Through a Boundary Value Problem
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../principles_of_bi.html">
   Principles of Bayesian Inference
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active has-children">
    <a class="reference internal" href="intro.html">
     Lecture 11 - Selecting Prior Information
    </a>
    <input checked="" class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
    <label for="toctree-checkbox-15">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul class="current">
     <li class="toctree-l3 current active">
      <a class="current reference internal" href="#">
       Selecting Prior Information
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="hands-on-11.1.html">
       Information Entropy
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="hands-on-11.2.html">
       The Principle of Maximum Entropy for Discrete Random Variables
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="hands-on-11.3.html">
       The Principle of Maximum Entropy for Continuous Random Variables
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture12/intro.html">
     Lecture 12 - Analytical Examples of Bayesian Inference
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
    <label for="toctree-checkbox-16">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture12/reading-12.html">
       Analytical Examples of Bayesian Inference
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture12/hands-on-12.1.html">
       Bayesian Parameter Estimation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture12/hands-on-12.2.html">
       Credible Intervals
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture12/hands-on-12.3.html">
       Decision-Making
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture12/hands-on-12.4.html">
       Posterior Predictive Checking
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../supervised_learning.html">
   Supervised Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
  <label for="toctree-checkbox-17">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture13/intro.html">
     Lecture 13 - Linear Regression via Least Squares
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
    <label for="toctree-checkbox-18">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture13/reading-13.html">
       Linear Regression via Least Squares
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture13/hands-on-13.1.html">
       Linear regression with a single variable
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture13/hands-on-13.2.html">
       Polynomial Regression
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture13/hands-on-13.3.html">
       The Generalized Linear Model
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture13/hands-on-13.4.html">
       Measures of Predictive Accuracy
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture14/intro.html">
     Lecture 14 - Bayesian Linear Regression
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
    <label for="toctree-checkbox-19">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture14/reading-14.html">
       Bayesian Linear Regression
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture14/hands-on-14.1.html">
       Probabilistic Interpretation of Least Squares - Estimating the Measurement Noise
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture14/hands-on-14.2.html">
       Maximum a Posteriori Estimate - Avoiding Overfitting
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture14/hands-on-14.3.html">
       Bayesian Linear Regression
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture14/hands-on-14.4.html">
       The point-predictive Distribution - Separating Epistmic and Aleatory Uncertainty
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture15/intro.html">
     Lecture 15 - Advanced Topics in Bayesian Linear Regression
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
    <label for="toctree-checkbox-20">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture15/reading-15.html">
       Advanced Topics in Bayesian Linear Regression
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture15/hands-on-15.1.html">
       Evidence approximation
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture15/hands-on-15.2.html">
       Automatic Relevance Determination
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture15/hands-on-15.3.html">
       Diagnostics for Posterior Predictive
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture16/intro.html">
     Lecture 16 - Classification
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/>
    <label for="toctree-checkbox-21">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture16/reading-16.html">
       Theoretical Background on Classification
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture16/hands-on-16.1.html">
       Logistic regression with one variable (High melting explosives)
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture16/hands-on-16.2.html">
       Logistic Regression with Many Features
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture16/hands-on-16.3.html">
       Decision making
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture16/hands-on-16.4.html">
       Diagnostics for Classifications
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture16/hands-on-16.5.html">
       Multi-class Logistic Regression
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../unsupervised_learning.html">
   Unsupervised Learning
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/>
  <label for="toctree-checkbox-22">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture17/intro.html">
     Lecture 17 - Clustering and Density Estimation
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/>
    <label for="toctree-checkbox-23">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture17/reading-17.html">
       Unsupervised Learning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture17/hands-on-17.1.html">
       Clustering using k-means
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture17/hands-on-17.2.html">
       Density Estimation via Gaussian mixtures
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture18/intro.html">
     Lecture 18 - Dimensionality Reduction
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/>
    <label for="toctree-checkbox-24">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture18/reading-18.html">
       Dimensionality Reduction
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture18/hands-on-18.1.html">
       Dimensionality Reduction Examples
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture18/hands-on-18.2.html">
       Clustering High-dimensional Data
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture18/hands-on-18.3.html">
       Density Estimation with High-dimensional Data
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../state_space_models.html">
   State Space Models
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/>
  <label for="toctree-checkbox-25">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture19/intro.html">
     Lecture 19 - State Space Models - Filtering Basics
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/>
    <label for="toctree-checkbox-26">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture19/reading-19.html">
       State Space Models - Filtering Basics
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture19/hands-on-19.1.html">
       Object Tracking Example
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture20/intro.html">
     Lecture 20 - State Space Models - Kalman Filters
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/>
    <label for="toctree-checkbox-27">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture20/reading-20.html">
       State Space Models - Kalman Filters
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture20/hands-on-20.1.html">
       Kalman Filter for Object Tracking Example
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../gaussian_process_regression.html">
   Gaussian Process Regression
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/>
  <label for="toctree-checkbox-28">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture21/intro.html">
     Lecture 21 - Gaussian Process Regression: Priors on Function Spaces
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/>
    <label for="toctree-checkbox-29">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture21/reading-21.html">
       Gaussian Process Theory
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture21/hands-on-21.html">
       Example: Priors on function spaces
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture22/intro.html">
     Lecture 22 - Gaussian Process Regression: Conditioning on Data
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/>
    <label for="toctree-checkbox-30">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture22/reading-22.html">
       Gaussian Process Regression - Theory
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture22/hands-on-22.1.html">
       Gaussian Process Regression Without Noise
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture22/hands-on-22.2.html">
       Gaussian Process Regression with Noise
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture22/hands-on-22.3.html">
       Tuning the Hyperparameters
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture22/hands-on-22.4.html">
       Multivariate Gaussian Process Regression
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture23/intro.html">
     Lecture 23 - Bayesian Global Optimization
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/>
    <label for="toctree-checkbox-31">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture23/reading-23.html">
       Bayesian Global Optimization
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture23/hands-on-23.1.html">
       Maximum Mean - A Bad Information Acquisition Function
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture23/hands-on-23.2.html">
       Maximum Upper Interval
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture23/hands-on-23.3.html">
       Probability of Improvement
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture23/hands-on-23.4.html">
       Expected Improvement
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../neural_networks.html">
   Neural Networks
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/>
  <label for="toctree-checkbox-32">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture24/intro.html">
     Lecture 24 - Deep Neural Networks
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/>
    <label for="toctree-checkbox-33">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture24/reading-24.html">
       Deep Neural Networks
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture24/hands-on-24.html">
       Regression with Deep Neural Networks
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture25/intro.html">
     Lecture 25 - Deep Neural Networks Continued
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/>
    <label for="toctree-checkbox-34">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture25/reading-25.html">
       Deep Neural Networks Continued
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture25/hands-on-25.html">
       Classification with Deep Neural Networks
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture26/intro.html">
     Lecture 26 - Physics-informed Deep Neural Networks
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/>
    <label for="toctree-checkbox-35">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture26/reading-26.html">
       Physics-informed Deep Neural Networks
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture26/hands-on-26.1.html">
       Physics-informed regularization: Solving ODEs
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture26/hands-on-26.2.html">
       Physics-informed regularization: Solving PDEs
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../advanced_methods.html">
   Advanced Methods for Characterizing Posteriors
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/>
  <label for="toctree-checkbox-36">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture27/intro.html">
     Lecture 27 - Sampling Methods
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/>
    <label for="toctree-checkbox-37">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture27/reading-27.html">
       Sampling Methods
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture27/hands-on-27.1.html">
       Probabilistic programming with
       <code class="docutils literal notranslate">
        <span class="pre">
         PyMC3
        </span>
       </code>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture27/hands-on-27.2.html">
       Sampling From the Distributions With Random Walk Metropolis
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture27/hands-on-27.3.html">
       The Metropolis-Hastings Algorithm
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture27/hands-on-27.4.html">
       Gibbs Sampling
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture27/hands-on-27.5.html">
       Sequential Monte Carlo
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../lecture28/intro.html">
     Lecture 28 - Variational Inference
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/>
    <label for="toctree-checkbox-38">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture28/reading-28.html">
       Variational Inference
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../lecture28/hands-on-28.html">
       Variational Inference Examples
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../bibliography.html">
   Bibliography
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/lecture11/reading-11.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/PredictiveScienceLab/data-analytics-se/master?urlpath=lab/tree/lecturebook/lecture11/reading-11.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/lecturebook/lecture11/reading-11.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-we-come-up-with-the-right-probability-assignments">
   How do we come up with the right probability assignments?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principle-of-insufficient-reason">
   Principle of Insufficient Reason
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-throwing-a-six-sided-die">
     Example: Throwing a six-sided die
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-principle-of-maximum-entropy">
   The Principle of Maximum Entropy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-entropy">
     Information entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-description-of-testable-information">
     Mathematical description of testable information
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#case-1">
       Case 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#case-2">
       Case 2
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-statement-of-the-principle-of-maximum-entropy">
     Mathematical statement of the principle of maximum entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-of-discrete-maximum-entropy-distributions">
     Examples of discrete maximum entropy distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-distributions">
     Continuous distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-note-on-q-x">
     A note on
     <span class="math notranslate nohighlight">
      \(q(x)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-of-continuous-maximum-entropy-distributions">
     Examples of continuous maximum entropy distributions
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Selecting Prior Information</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#how-do-we-come-up-with-the-right-probability-assignments">
   How do we come up with the right probability assignments?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#principle-of-insufficient-reason">
   Principle of Insufficient Reason
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-throwing-a-six-sided-die">
     Example: Throwing a six-sided die
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-principle-of-maximum-entropy">
   The Principle of Maximum Entropy
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-entropy">
     Information entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-description-of-testable-information">
     Mathematical description of testable information
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#case-1">
       Case 1
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#case-2">
       Case 2
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#mathematical-statement-of-the-principle-of-maximum-entropy">
     Mathematical statement of the principle of maximum entropy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-of-discrete-maximum-entropy-distributions">
     Examples of discrete maximum entropy distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#continuous-distributions">
     Continuous distributions
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-note-on-q-x">
     A note on
     <span class="math notranslate nohighlight">
      \(q(x)\)
     </span>
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#examples-of-continuous-maximum-entropy-distributions">
     Examples of continuous maximum entropy distributions
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="selecting-prior-information">
<h1>Selecting Prior Information<a class="headerlink" href="#selecting-prior-information" title="Permalink to this headline">¶</a></h1>
<div class="section" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://en.wikipedia.org/wiki/Principle_of_maximum_entropy">Principle of maximum entropy</a> wikipedia entrty.</p></li>
</ul>
</div>
<div class="section" id="how-do-we-come-up-with-the-right-probability-assignments">
<h2>How do we come up with the right probability assignments?<a class="headerlink" href="#how-do-we-come-up-with-the-right-probability-assignments" title="Permalink to this headline">¶</a></h2>
<p>In applications we often found ourselves in a situation where we have to pick prior probabilities of a given variable. That is, pick probabilities before we see any specific data from that variable.
An important question is how we come up with these prior probabilities.
Is there a systematic theoretical framework we could follow?
There are basically three widely accepted ways:</p>
<ul class="simple">
<li><p>The principle of insufficient reason.</p></li>
<li><p>The principle of maximum entropy.</p></li>
<li><p>The principle of transformation groups.</p></li>
</ul>
<p>In this lecture, we will explain the first two. The third one, transformation groups, is rather advanced and we will not discuss it.
At the beginning, what we talk about will just work with discrete random variables.
Continuous random variables are a little bit trickier and we are going to discuss them at the end.</p>
</div>
<div class="section" id="principle-of-insufficient-reason">
<h2>Principle of Insufficient Reason<a class="headerlink" href="#principle-of-insufficient-reason" title="Permalink to this headline">¶</a></h2>
<p>The principle of insufficient reason has its origins to Laplace. The original statement was:</p>
<blockquote>
<div><p>The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favorable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible.
<em>Pierre-Simon Laplace</em></p>
</div></blockquote>
<p>Let’s restate this in simpler terms.
Assume that the random variable <span class="math notranslate nohighlight">\(X\)</span> can take <span class="math notranslate nohighlight">\(N\)</span> possible values, <span class="math notranslate nohighlight">\(1, 2,\dots,N\)</span>.
If this is all we know about this random variable then <em>the principle of insufficient reason</em> tells us to set:</p>
<div class="math notranslate nohighlight">
\[
p(x) = \frac{1}{N},
\]</div>
<p>for <span class="math notranslate nohighlight">\(x\)</span> in <span class="math notranslate nohighlight">\(\{1,2,\dots,N\}\)</span>.
That is, the principle of insufficient reason tells us to assign the same probability to each possibility.
Intuitively, any other choice we could make would introduce a bias towards one value or another.</p>
<div class="section" id="example-throwing-a-six-sided-die">
<h3>Example: Throwing a six-sided die<a class="headerlink" href="#example-throwing-a-six-sided-die" title="Permalink to this headline">¶</a></h3>
<p>Consider a six-sided die with sides numbered <span class="math notranslate nohighlight">\(1\)</span> to <span class="math notranslate nohighlight">\(6\)</span>.
Call <span class="math notranslate nohighlight">\(X\)</span> the random variable corresponding to an experiment of throwing the die.
What is the probability of the die taking a specific value.
Using the principle of insufficient reason, we set:</p>
<div class="math notranslate nohighlight">
\[
p(X=x) = \frac{1}{6}.
\]</div>
</div>
</div>
<div class="section" id="the-principle-of-maximum-entropy">
<h2>The Principle of Maximum Entropy<a class="headerlink" href="#the-principle-of-maximum-entropy" title="Permalink to this headline">¶</a></h2>
<p>The principle of maximum entropy extends the principe of insufficient reason in a very useful way.
It tells you what probability distribution to assign to a random variable <span class="math notranslate nohighlight">\(X\)</span> when you have some prior information about it.
This information could include, for example, the expected value of <span class="math notranslate nohighlight">\(X\)</span>, or maybe its variance (see the section on <em>testable prior information</em> for a more precise description of what is allowed).
The simplest non-mathematical definition of the principle of maximum entropy I could find is due to E. T. Jaynes:</p>
<blockquote>
<div><p>The knowledge of average values does give a reason for preferring some possibilities to others, but we would like […] to assign a probability distribution which is as uniform as it can be while agreeing with the available information.”</p>
</div></blockquote>
<p>Why does he say “as uniform as it can be?”
He does this because he wants the principle to be consistent with the principle of insufficient reason when there is not available information.
Of course, the uniform distribution is the most “uncertain” distribution, so we could also say that we are looking for a maximumally uncertain distribution which agrees with the available information.
The “uncertainty” of a probability distribution is measured by its “information entropy”, a concept that we explain in the subsequent section.</p>
<div class="section" id="information-entropy">
<h3>Information entropy<a class="headerlink" href="#information-entropy" title="Permalink to this headline">¶</a></h3>
<p>We would like to know, how much uncertainty there is in a probability mass function <span class="math notranslate nohighlight">\(p(x)\)</span>.
In 1948, <a class="reference external" href="https://en.wikipedia.org/wiki/Claude_Shannon">Claude Shannon</a> posed and answered this problem in his seminal paper titled “A Mathematical Theory of Communication.”
The details of his derivation are beyond the scope of this course, but they can be summarized as follows:</p>
<ul class="simple">
<li><p>He looked for a functional <span class="math notranslate nohighlight">\(\mathbb{H}[p(X)]\)</span> that measured the uncertainty of the probability mass function <span class="math notranslate nohighlight">\(p(x)\)</span> using real values.</p></li>
<li><p>He wrote down some axioms that this functional should satisfy. For example, that is should be continuous, and that it should have its maximum when <span class="math notranslate nohighlight">\(p(x)\)</span> is the uniform (because the uniform distribution has the maximum uncertainty).</p></li>
<li><p>He did a little bit of math, and provied that (up to an arbitrary multiplicative constant) the function he was looking for must have this form:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\mathbb{H}[p(X)] = -\sum_x \log p(x) p(x).
\]</div>
<ul class="simple">
<li><p>As he was looking for a name for this function, he showed his discovery to <a class="reference external" href="https://en.wikipedia.org/wiki/John_von_Neumann">von Neumann</a> who recognized the similarity to the entropy of statistical mechanics first introduced by <a class="reference external" href="https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs">J. W. Gibbs</a>.</p></li>
</ul>
<p>Notice that the function is maximized at <span class="math notranslate nohighlight">\(p_0 = 0.5\)</span> because this corresponds to maximum uncertainty.
The function is minimized (as a matter of fact it is exactly zero) at <span class="math notranslate nohighlight">\(p_0 = 0\)</span> and <span class="math notranslate nohighlight">\(p_0 = 1\)</span> because both these cases correspond to minimum uncertainty (you are certain what is going to happen).</p>
</div>
<div class="section" id="mathematical-description-of-testable-information">
<h3>Mathematical description of testable information<a class="headerlink" href="#mathematical-description-of-testable-information" title="Permalink to this headline">¶</a></h3>
<p>For our purposes, it suffices to assume that our information about <span class="math notranslate nohighlight">\(X\)</span> comes in the form of expectations of functions of <span class="math notranslate nohighlight">\(X\)</span>, i.e., it is:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[f_k(X)] = F_k,
\]</div>
<p>for some <em>known functions</em> <span class="math notranslate nohighlight">\(f_k(x)\)</span> and some <em>known values</em> <span class="math notranslate nohighlight">\(F_k\)</span> for their expectations, <span class="math notranslate nohighlight">\(k=1,\dots,K\)</span>.
Let’s demonstrate that this definition includes some important cases:</p>
<div class="section" id="case-1">
<h4>Case 1<a class="headerlink" href="#case-1" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(I = \)</span> “The expected value of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span>.” This is obviously included as it is just the statement</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X] = \mu.
\]</div>
<p>So, we are covered by setting <span class="math notranslate nohighlight">\(K=1\)</span>, <span class="math notranslate nohighlight">\(f_1(x) = x\)</span>, and <span class="math notranslate nohighlight">\(F_1 = \mu\)</span>.</p>
</div>
<div class="section" id="case-2">
<h4>Case 2<a class="headerlink" href="#case-2" title="Permalink to this headline">¶</a></h4>
<p><span class="math notranslate nohighlight">\(I = \)</span> “The expected value of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\mu\)</span> and the variance of <span class="math notranslate nohighlight">\(X\)</span> is <span class="math notranslate nohighlight">\(\sigma^2\)</span>.” Here we obviously have <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \mu\)</span>, just like before. The second condition is about the variance, <span class="math notranslate nohighlight">\(\mathbb{V}[X] = \sigma^2\)</span>. We can easily turn this into an expectation by using the formula <span class="math notranslate nohighlight">\(\mathbb{V}[X] = \mathbb{E}[X^2] - (\mathbb{E}[X])^2)\)</span>. It becomes:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[X^2] = \sigma^2 + \mu^2.
\]</div>
<p>So, we are covered again with <span class="math notranslate nohighlight">\(K=2\)</span>, <span class="math notranslate nohighlight">\(f_1(x) = x\)</span>, <span class="math notranslate nohighlight">\(f_2(x) = x^2\)</span>, <span class="math notranslate nohighlight">\(F_1 = \mu\)</span>, <span class="math notranslate nohighlight">\(F_2 = \sigma^2 + \mu^2\)</span>.</p>
</div>
</div>
<div class="section" id="mathematical-statement-of-the-principle-of-maximum-entropy">
<h3>Mathematical statement of the principle of maximum entropy<a class="headerlink" href="#mathematical-statement-of-the-principle-of-maximum-entropy" title="Permalink to this headline">¶</a></h3>
<p>Having defined the measure of uncertainty and how the available information is modeled, we can now state the principle of maximum entropy mathematically.
Take a random variable with <span class="math notranslate nohighlight">\(N\)</span> different possibilities with probabilities <span class="math notranslate nohighlight">\(p_1=p(X=x_1),\dots,p_N = p(X=x_N)\)</span> to be identified.
We need to maximize:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{H}[p(X)] = -\sum_{i=1}^N p_i\log p_i,
\]</div>
<p>subject to the normalization constraint:</p>
<div class="math notranslate nohighlight">
\[
\sum_i p_i = 1,
\]</div>
<p>and the testable information constraints:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[f_k(X)] = F_k.
\]</div>
<p>The general solution of this problem can be found using the <a class="reference external" href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">Karush-Kuhn-Tucker conditions</a>.
If you go through the derivation, you will find that:</p>
<div class="math notranslate nohighlight">
\[
p(X=x_i) = \frac{1}{Z}\exp\left\{\sum_{k=1}^K\lambda_kf_k(x_i)\right\},
\]</div>
<p>where the <span class="math notranslate nohighlight">\(\lambda_k\)</span>’s are constants and <span class="math notranslate nohighlight">\(Z\)</span> is the normalization constant:</p>
<div class="math notranslate nohighlight">
\[
Z = \sum_i \exp\left\{\sum_{k=1}^K\lambda_kf_k(x_i)\right\}.
\]</div>
<p>The <span class="math notranslate nohighlight">\(\lambda_k\)</span>’s can be identified by solving the system of non-linear equations:</p>
<div class="math notranslate nohighlight">
\[
F_k = \frac{\partial\log Z}{\partial \lambda_k},
\]</div>
<p>for <span class="math notranslate nohighlight">\(k=1,\dots,K\)</span>.</p>
</div>
<div class="section" id="examples-of-discrete-maximum-entropy-distributions">
<h3>Examples of discrete maximum entropy distributions<a class="headerlink" href="#examples-of-discrete-maximum-entropy-distributions" title="Permalink to this headline">¶</a></h3>
<p>In what follows, we provide some examples of maximum entropy distributions that naturally arise.</p>
<ul class="simple">
<li><p>The categorical with equal probabilities <span class="math notranslate nohighlight">\(\operatorname{Categorical}(\frac{1}{N},\dots,\frac{1}{N})\)</span> is the maximum entropy distribution for a random variable <span class="math notranslate nohighlight">\(X\)</span> taking <span class="math notranslate nohighlight">\(N\)</span> different values (no other constraints).</p></li>
<li><p>The Bernoulli distribution <span class="math notranslate nohighlight">\(\operatorname{Bernoulli}(\theta)\)</span> is the maximum entropy distribution for a random variable <span class="math notranslate nohighlight">\(X\)</span> taking two values <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span> with known expectation <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \theta\)</span>.</p></li>
<li><p>The Binomial distribution <span class="math notranslate nohighlight">\(B(\theta,n)\)</span> is the maximum entropy distribution for a random variable <span class="math notranslate nohighlight">\(X\)</span> taking values <span class="math notranslate nohighlight">\(0, 1,\dots,n\)</span> with known expectation <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \theta n\)</span> (within the class of <span class="math notranslate nohighlight">\(n\)</span>-generalized binomial distributions, i.e., the distribution representing the number of successful trials in <span class="math notranslate nohighlight">\(n\)</span>, potentially correlated, experiments).</p></li>
<li><p>The Poisson distribution <span class="math notranslate nohighlight">\(\operatorname{Poisson}(\lambda)\)</span> is the maximum entropy distribution for a random variable <span class="math notranslate nohighlight">\(X\)</span> taking values <span class="math notranslate nohighlight">\(0, 1, 2,\dots\)</span> with known expectation <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \lambda\)</span> (within the class of <span class="math notranslate nohighlight">\(\infty\)</span>-generalized binomial distributions).</p></li>
<li><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Canonical_ensemble">canonical ensemble</a> is the maximum entropy distribution over the states of a quantum mechanical system with known expected energy.</p></li>
<li><p>The <a class="reference external" href="https://en.wikipedia.org/wiki/Grand_canonical_ensemble">grand canonical ensemble</a> is the maximum entropy distribution over the states of a quanum mechanical system consiting of many different numbers of particles with known expected number of particles per type and known expected energy.</p></li>
</ul>
</div>
<div class="section" id="continuous-distributions">
<h3>Continuous distributions<a class="headerlink" href="#continuous-distributions" title="Permalink to this headline">¶</a></h3>
<p>Shannon’s entropy only works for discrete distributions.
Why?
Consider the naïve generalization:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{H}_{\text{naïve}}[p(X)] = -\int p(x)\log p(x)dx.
\]</div>
<p>Now, imagine that you could equally well work with a transformed version of <span class="math notranslate nohighlight">\(X\)</span>.
Mathematically, assume that <span class="math notranslate nohighlight">\(Y = T(X)\)</span> where <span class="math notranslate nohighlight">\(T(x)\)</span> is invertible.
Since <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> are connected in this way you should be getting the same information entropy independently of whether you calculate it with <span class="math notranslate nohighlight">\(p(X)\)</span> or <span class="math notranslate nohighlight">\(p(Y)\)</span>.
But, there are many counter examples where you get:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{H}_{\text{naïve}}[p(X)] \not= \mathbb{H}_{\text{naïve}}[p(Y)].
\]</div>
<p>This shows that <span class="math notranslate nohighlight">\(\mathbb{H}_{\text{naïve&quot;}}[p(X)]\)</span> is a bad definition of uncertainty for continuous distributions.</p>
<p>For continuous distributions, the correct thing to use is the relative entropy:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{H}[p(X)] = -\int p(x)\log\frac{p(x)}{q(x)}dx,
\]</div>
<p>where <span class="math notranslate nohighlight">\(q(x)\)</span> is a prior density function (not necessarily normalized) encoding maximum uncertainty.
You can find more about <span class="math notranslate nohighlight">\(q(x)\)</span> in the note below.
With this definition the maximum entropy principle for continuous random variables is as follows.
Maximize:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{H}[p(X)] = -\int p(x)\log\frac{p(x)}{q(x)}dx,
\]</div>
<p>subject to the normalization constraint:</p>
<div class="math notranslate nohighlight">
\[
\int p(x) dx = 1,
\]</div>
<p>and the testable information constraints:</p>
<div class="math notranslate nohighlight">
\[
\mathbb{E}[f_k(X)] = F_k.
\]</div>
<p>Applying the <a class="reference external" href="https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions">Karush-Kuhn-Tucker conditions</a>, we find that:</p>
<div class="math notranslate nohighlight">
\[
p(x) = \frac{q(x)}{Z}\exp\left\{\sum_{k=1}^K\lambda_kf_k(x)\right\},
\]</div>
<p>where the <span class="math notranslate nohighlight">\(\lambda_k\)</span>’s are constants and <span class="math notranslate nohighlight">\(Z\)</span> is the normalization constant:</p>
<div class="math notranslate nohighlight">
\[
Z = \int q(x)\exp\left\{\sum_{k=1}^K\lambda_kf_k(x)\right\}dx.
\]</div>
<p>The <span class="math notranslate nohighlight">\(\lambda_k\)</span>’s can be identified by solving the system of non-linear equations:</p>
<div class="math notranslate nohighlight">
\[
F_k = \frac{\partial \log Z}{\partial \lambda_k},
\]</div>
<p>for <span class="math notranslate nohighlight">\(k=1,\dots,K\)</span>.</p>
</div>
<div class="section" id="a-note-on-q-x">
<h3>A note on <span class="math notranslate nohighlight">\(q(x)\)</span><a class="headerlink" href="#a-note-on-q-x" title="Permalink to this headline">¶</a></h3>
<p>There are, of course, cases in which <span class="math notranslate nohighlight">\(q(x)\)</span> is just the uniform density.
In these cases the mathematical form of the information entropy becomes identical to the discrete case.
For example, if <span class="math notranslate nohighlight">\(x\)</span> is a location parameter, e.g., the 3D location of a particle free to move in a box, then <span class="math notranslate nohighlight">\(q(x)\)</span> is indeed uniform.
As another example, imagine a particle constrained to move on a cyclic guide.
Then <span class="math notranslate nohighlight">\(q(x)\)</span> is constant on the cyclic guide and zero everywhere else.
The takehome message dual. First, <span class="math notranslate nohighlight">\(q(x)\)</span> depends on what the underlying random variable actually is.
Second, the identification of <span class="math notranslate nohighlight">\(q(x)\)</span> is beyond the scope of the maximum entropy principle.
In other words, you need to have <span class="math notranslate nohighlight">\(q(x)\)</span> before applying the maximum entropy principle.
There are some systematic methods for identifying maximum uncertainty densities such as the <a class="reference external" href="https://en.wikipedia.org/wiki/Principle_of_transformation_groups">principle of transformation groups</a> and the theory of <a class="reference external" href="https://en.wikipedia.org/wiki/Haar_measure">Haar measures</a> but both these concepts require advanced mathematics.
In many practical examples common sense is sufficient for coming up with <span class="math notranslate nohighlight">\(q(x)\)</span>.</p>
</div>
<div class="section" id="examples-of-continuous-maximum-entropy-distributions">
<h3>Examples of continuous maximum entropy distributions<a class="headerlink" href="#examples-of-continuous-maximum-entropy-distributions" title="Permalink to this headline">¶</a></h3>
<p>In what follows, we provide some examples of maximum entropy distributions that naturally arise.</p>
<ul class="simple">
<li><p>The Uniform distribution <span class="math notranslate nohighlight">\(U([a,b])\)</span> is the maximum entropy distribution for a random variable <span class="math notranslate nohighlight">\(X\)</span> taking values in <span class="math notranslate nohighlight">\([a,b]\)</span> with <span class="math notranslate nohighlight">\(q(x) = 1\)</span> and no other constraints.</p></li>
<li><p>The normal distribution <span class="math notranslate nohighlight">\(N(\mu,\sigma^2)\)</span> is the maximum entropy distribution for a random variable <span class="math notranslate nohighlight">\(X\)</span> taking values in <span class="math notranslate nohighlight">\(\mathbb{R}\)</span> with <span class="math notranslate nohighlight">\(q(x) = 1\)</span>, known expectation <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \mu\)</span> and variance <span class="math notranslate nohighlight">\(\mathbb{V}[X] = \sigma^2\)</span>.</p></li>
<li><p>The multivariate normal distribution <span class="math notranslate nohighlight">\(N(\boldsymbol{\mu},\boldsymbol{\Sigma})\)</span> is the maximum entropy distribution for a random vector <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> taking values in <span class="math notranslate nohighlight">\(\mathbb{R}^d\)</span> with <span class="math notranslate nohighlight">\(q(\mathbf{x}) = 1\)</span> and known expectation <span class="math notranslate nohighlight">\(\mathbb{E}[\mathbf{X}] = \boldsymbol{\mu}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\mathbb{C}[X,X] = \boldsymbol{\Sigma}\)</span>.</p></li>
<li><p>The Exponential distribution <span class="math notranslate nohighlight">\(\operatorname{Exp}(\lambda)\)</span> is the maximum entropy distribution for a random variable <span class="math notranslate nohighlight">\(X\)</span> taking values in <span class="math notranslate nohighlight">\([0,\infty)\)</span> with <span class="math notranslate nohighlight">\(q(x) = 1\)</span> and known expectation <span class="math notranslate nohighlight">\(\mathbb{E}[X] = \frac{1}{\lambda}\)</span>.</p></li>
</ul>
<p>For an almost list of a commonly used maximum entropy distributions, see the <a class="reference external" href="https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution">Maximum entropy probability distribution entry of wikipedia</a>.</p>
</div>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./lecture11"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="intro.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Lecture 11 - Selecting Prior Information</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="hands-on-11.1.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Information Entropy</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Ilias Bilionis (ibilion[at]purdue.edu)<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>