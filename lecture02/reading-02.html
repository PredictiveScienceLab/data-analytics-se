
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Basics of Probability Theory &#8212; Introduction to Scientific Machine Learning (Lecture Book)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'lecture02/reading-02';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Experiment with “Randomness”" href="hands-on-02.html" />
    <link rel="prev" title="Lecture 2 - Basics of Probability Theory" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
  
    <p class="title logo__title">Introduction to Scientific Machine Learning (Lecture Book)</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../introduction.html">Introduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture01/intro.html">Lecture 1 - Introduction to Predictive Modeling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture01/reading-01.html">The Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture01/hands-on-01.1.html">The Uncertainty Propagation Problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture01/hands-on-01.2.html">The Model Calibration Problem</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="../review_probability.html">Review of Probability</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2 current active has-children"><a class="reference internal" href="intro.html">Lecture 2 - Basics of Probability Theory</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l3 current active"><a class="current reference internal" href="#">Basics of Probability Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="hands-on-02.html">Experiment with “Randomness”</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture03/intro.html">Lecture 3 - Discrete Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture03/reading-03.html">Discrete Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture03/hands-on-03.html">Discrete Random Variables in Python</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture04/intro.html">Lecture 4 - Continuous Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture04/reading-04.html">Continuous Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture04/hands-on-04.1.html">The Uniform Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture04/hands-on-04.2.html">The Gaussian Distribution</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture05/intro.html">Lecture 5 - Collections of Random Variables</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture05/reading-05.html">Collections of Random Variables: Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture05/hands-on-05.html">Practicing with Joint Probability Mass Functions</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture06/intro.html">Lecture 6 - Random Vectors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/reading-06.html">Random Vectors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.1.html">The Multivariate Normal - Diagonal Covariance Case</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.2.html">The Multivariate Normal - Full Covariance Case</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.3.html">The Multivariate Normal - Marginalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.4.html">The Multivariate Normal - Conditioning</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../uncertainty_propagation.html">Uncertainty Propagation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture07/intro.html">Lecture 7 - Basic Sampling</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.1.html">Pseudo-random number generators</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.2.html">Sampling the uniform distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.3.html">Sampling the categorical</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.4.html">Sampling from continuous distributions - Inverse sampling</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture08/intro.html">Lecture 8 - The Monte Carlo Method for Estimating Expectations</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture08/reading-08.html">The Uncertainty Propagation Problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture08/hands-on-08.3.html">The Monte Carlo Method for Estimating Expectations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture08/hands-on-08.4.html">Sampling Estimates of Variance</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture09/intro.html">Lecture 9 - Monte Carlo Estimates of Various Statistics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.1.html">Sampling Estimates of the Cumulative Distribution Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.2.html">Sampling Estimates of the Probability Density via Histograms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.3.html">Estimating Predictive Quantiles</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.4.html">Uncertainty propagation through an ordinary differential equation</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture10/intro.html">Lecture 10 - Quantify Uncertainty in Monte Carlo Estimates</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.1.html">Visualizing Monte Carlo Uncertainty</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.2.html">The Central Limit Theorem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.3.html">Quantifying Epistemic Uncertainty in Monte Carlo Estimates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.4.html">Uncertainty Propagation Through a Boundary Value Problem</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../principles_of_bi.html">Principles of Bayesian Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture11/intro.html">Lecture 11 - Selecting Prior Information</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/reading-11.html">Selecting Prior Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/hands-on-11.1.html">Information Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/hands-on-11.2.html">The Principle of Maximum Entropy for Discrete Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/hands-on-11.3.html">The Principle of Maximum Entropy for Continuous Random Variables</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture12/intro.html">Lecture 12 - Analytical Examples of Bayesian Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/reading-12.html">Bayesian inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.1.html">Example: Inferring the probability of a coin toss from data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.2.html">Credible Intervals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.3.html">Decision Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.4.html">Posterior Predictive Checking</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../supervised_learning.html">Supervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture13/intro.html">Lecture 13 - Linear Regression via Least Squares</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/reading-13.html">Linear Regression via Least Squares</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.1.html">Linear regression with a single variable</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.2.html">Polynomial Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.3.html">The Generalized Linear Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.4.html">Measures of Predictive Accuracy</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture14/intro.html">Lecture 14 - Bayesian Linear Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/reading-14.html">Bayesian Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.1.html">Probabilistic Interpretation of Least Squares - Estimating the Measurement Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.2.html">Maximum a Posteriori Estimate - Avoiding Overfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.3.html">Bayesian Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.4.html">The point-predictive Distribution - Separating Epistemic and Aleatory Uncertainty</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture15/intro.html">Lecture 15 - Advanced Topics in Bayesian Linear Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/reading-15.html">Advanced Topics in Bayesian Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/hands-on-15.1.html">Evidence approximation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/hands-on-15.2.html">Automatic Relevance Determination</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/hands-on-15.3.html">Diagnostics for Posterior Predictive</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture16/intro.html">Lecture 16 - Classification</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/reading-16.html">Theoretical Background on Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.1.html">Logistic regression with one variable (High melting explosives)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.2.html">Logistic Regression with Many Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.3.html">Decision making</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.4.html">Diagnostics for Classifications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.5.html">Multi-class Logistic Regression</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unsupervised_learning.html">Unsupervised Learning</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture17/intro.html">Lecture 17 - Clustering and Density Estimation</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture17/reading-17.html">Unsupervised Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture17/hands-on-17.1.html">Clustering using k-means</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture17/hands-on-17.2.html">Density Estimation via Gaussian mixtures</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture18/intro.html">Lecture 18 - Dimensionality Reduction</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/reading-18.html">Dimensionality Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/hands-on-18.1.html">Dimensionality Reduction Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/hands-on-18.2.html">Clustering High-dimensional Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/hands-on-18.3.html">Density Estimation with High-dimensional Data</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../state_space_models.html">State Space Models</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture19/intro.html">Lecture 19 - State Space Models - Filtering Basics</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture19/reading-19.html">State Space Models - Filtering Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture19/hands-on-19.1.html">Object Tracking Example</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture20/intro.html">Lecture 20 - State Space Models - Kalman Filters</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture20/reading-20.html">State Space Models - Kalman Filters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture20/hands-on-20.1.html">Kalman Filter for the Object Tracking Example</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../gaussian_process_regression.html">Gaussian Process Regression</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture21/intro.html">Lecture 21 - Gaussian Process Regression: Priors on Function Spaces</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture21/reading-21.html">Gaussian Process Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture21/hands-on-21.html">Example: Priors on function spaces</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture22/intro.html">Lecture 22 - Gaussian Process Regression: Conditioning on Data</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/reading-22.html">Gaussian Process Regression - Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.1.html">Gaussian Process Regression Without Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.2.html">Gaussian Process Regression with Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.3.html">Tuning the Hyperparameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.4.html">Multivariate Gaussian Process Regression</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture23/intro.html">Lecture 23 - Bayesian Global Optimization</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/reading-23.html">Bayesian Global Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.1.html">Maximum Mean - A Bad Information Acquisition Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.2.html">Maximum Upper Interval</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.3.html">Probability of Improvement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.4.html">Expected Improvement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.5.html">Expected Improvement - With Observation Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.6.html">Quantifying Epistemic Uncertainty about the Solution of the Optimization problem</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neural_networks.html">Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture24/intro.html">Lecture 24 - Deep Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture24/reading-24.html">Deep Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture24/hands-on-24.html">Regression with Deep Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture25/intro.html">Lecture 25 - Deep Neural Networks Continued</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture25/reading-25.html">Deep Neural Networks Continued</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture25/hands-on-25.html">Classification with Deep Neural Networks</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture26/intro.html">Lecture 26 - Physics-informed Deep Neural Networks</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture26/reading-26.html">Physics-informed Deep Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture26/hands-on-26.1.html">Physics-informed regularization: Solving ODEs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture26/hands-on-26.2.html">Physics-informed regularization: Solving PDEs</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../advanced_methods.html">Advanced Methods for Characterizing Posteriors</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture27/intro.html">Lecture 27 - Sampling Methods</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/reading-27.html">Sampling Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.1.html">Probabilistic numerics using <code class="docutils literal notranslate"><span class="pre">pyro</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.2.html">Sampling From the Distributions With Random Walk Metropolis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.3.html">The Metropolis-Hastings Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.4.html">Hierarchical Bayesian Models</a></li>
</ul>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture28/intro.html">Lecture 28 - Variational Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture28/reading-28.html">Variational Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture28/hands-on-28.html">Variational Inference Examples</a></li>
</ul>
</details></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../homework/intro.html">Homework</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../homework/homework-01.html">Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/homework-02.html">Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/homework-03.html">Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/homework-04.html">Homework 4</a></li>



<li class="toctree-l2"><a class="reference internal" href="../homework/homework-05.html">Homework 5</a></li>



<li class="toctree-l2"><a class="reference internal" href="../homework/homework-06.html">Homework 6</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/homework-07.html">Homework 7</a></li>
<li class="toctree-l2"><a class="reference internal" href="../homework/homework-08.html">Homework 8</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/lecturebook/lecture02/reading-02.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch on Colab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img alt="Colab logo" src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/lecture02/reading-02.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Basics of Probability Theory</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-sentences">Logical sentences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-as-a-representation-of-our-state-of-knowledge">Probability as a representation of our state of knowledge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-what-about-frequencies">But what about frequencies?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-common-sense-assumptions-that-give-rise-to-the-basic-probability-rules">The common sense assumptions that give rise to the basic probability rules.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#talking-about-probabilities">Talking about probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-probabilities">Interpretation of probabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rules-of-probability">The rules of probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-drawing-balls-from-a-box-without-replacement-1-3">Example: Drawing balls from a box without replacement (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-rules-of-probability-theory">Other rules of probability theory</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-of-the-obvious-rule">Extension of the obvious rule</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Extension of the obvious rule</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sum-rule">The sum rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-drawing-balls-from-a-box-without-replacement-2-3">Example: Drawing balls from a box without replacement (2/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-drawing-balls-from-a-box-without-replacement-3-3">Example: Drawing balls from a box without replacement (3/3)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <div class="cell tag_hide-input tag_hide-output docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">MAKE_BOOK_FIGURES</span><span class="o">=</span><span class="n">Trueimport</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">npimport</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span> <span class="k">as</span> <span class="n">stimport</span> <span class="n">matplotlib</span> <span class="k">as</span> <span class="n">mplimport</span> <span class="n">matplotlib</span><span class="o">.</span><span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span><span class="o">%</span><span class="k">matplotlib</span> inlineimport matplotlib_inlinematplotlib_inline.backend_inline.set_matplotlib_formats(&#39;svg&#39;)import seaborn as snssns.set_context(&quot;paper&quot;)sns.set_style(&quot;ticks&quot;)def set_book_style():    plt.style.use(&#39;seaborn-v0_8-white&#39;)     sns.set_style(&quot;ticks&quot;)    sns.set_palette(&quot;deep&quot;)    mpl.rcParams.update({        # Font settings        &#39;font.family&#39;: &#39;serif&#39;,  # For academic publishing        &#39;font.size&#39;: 8,  # As requested, 10pt font        &#39;axes.labelsize&#39;: 8,        &#39;axes.titlesize&#39;: 8,        &#39;xtick.labelsize&#39;: 7,  # Slightly smaller for better readability        &#39;ytick.labelsize&#39;: 7,        &#39;legend.fontsize&#39;: 7,                # Line and marker settings for consistency        &#39;axes.linewidth&#39;: 0.5,        &#39;grid.linewidth&#39;: 0.5,        &#39;lines.linewidth&#39;: 1.0,        &#39;lines.markersize&#39;: 4,                # Layout to prevent clipped labels        &#39;figure.constrained_layout.use&#39;: True,                # Default DPI (will override when saving)        &#39;figure.dpi&#39;: 600,        &#39;savefig.dpi&#39;: 600,                # Despine - remove top and right spines        &#39;axes.spines.top&#39;: False,        &#39;axes.spines.right&#39;: False,                # Remove legend frame        &#39;legend.frameon&#39;: False,                # Additional trim settings        &#39;figure.autolayout&#39;: True,  # Alternative to constrained_layout        &#39;savefig.bbox&#39;: &#39;tight&#39;,    # Trim when saving        &#39;savefig.pad_inches&#39;: 0.1   # Small padding to ensure nothing gets cut off    })def set_notebook_style():    plt.style.use(&#39;seaborn-v0_8-white&#39;)    sns.set_style(&quot;ticks&quot;)    sns.set_palette(&quot;deep&quot;)    mpl.rcParams.update({        # Font settings - using default sizes        &#39;font.family&#39;: &#39;serif&#39;,        &#39;axes.labelsize&#39;: 10,        &#39;axes.titlesize&#39;: 10,        &#39;xtick.labelsize&#39;: 9,        &#39;ytick.labelsize&#39;: 9,        &#39;legend.fontsize&#39;: 9,                # Line and marker settings        &#39;axes.linewidth&#39;: 0.5,        &#39;grid.linewidth&#39;: 0.5,        &#39;lines.linewidth&#39;: 1.0,        &#39;lines.markersize&#39;: 4,                # Layout settings        &#39;figure.constrained_layout.use&#39;: True,                # Remove only top and right spines        &#39;axes.spines.top&#39;: False,        &#39;axes.spines.right&#39;: False,                # Remove legend frame        &#39;legend.frameon&#39;: False,                # Additional settings        &#39;figure.autolayout&#39;: True,        &#39;savefig.bbox&#39;: &#39;tight&#39;,        &#39;savefig.pad_inches&#39;: 0.1    })def save_for_book(fig, filename, is_vector=True, **kwargs):    &quot;&quot;&quot;    Save a figure with book-optimized settings.        Parameters:    -----------    fig : matplotlib figure        The figure to save    filename : str        Filename without extension    is_vector : bool        If True, saves as vector at 1000 dpi. If False, saves as raster at 600 dpi.    **kwargs : dict        Additional kwargs to pass to savefig    &quot;&quot;&quot;        # Set appropriate DPI and format based on figure type    if is_vector:        dpi = 1000        ext = &#39;.pdf&#39;    else:        dpi = 600        ext = &#39;.tif&#39;        # Save the figure with book settings    fig.savefig(f&quot;{filename}{ext}&quot;, dpi=dpi, **kwargs)def make_full_width_fig():    return plt.subplots(figsize=(4.7, 2.9), constrained_layout=True)def make_half_width_fig():    return plt.subplots(figsize=(2.35, 1.45), constrained_layout=True)if MAKE_BOOK_FIGURES:    set_book_style()else:    set_notebook_style()make_full_width_fig = make_full_width_fig if MAKE_BOOK_FIGURES else lambda: plt.subplots()make_half_width_fig = make_half_width_fig if MAKE_BOOK_FIGURES else lambda: plt.subplots()
</pre></div>
</div>
</div>
</details>
</div>
<section class="tex2jax_ignore mathjax_ignore" id="basics-of-probability-theory">
<span id="id1"></span><h1>Basics of Probability Theory<a class="headerlink" href="#basics-of-probability-theory" title="Link to this heading">#</a></h1>
<section id="logical-sentences">
<span id="id2"></span><h2>Logical sentences<a class="headerlink" href="#logical-sentences" title="Link to this heading">#</a></h2>
<p>A logical sentence is a statement about the world that can be true or false.
Science makes such statements all the time.
For example, “the circumference of the Earth is 40,075 km (plus or minus 1 km)” is a logical sentence.
We will not go any deeper into the formal definition of logical sentences.
But we will require that the set of logical sentences we are working with is <em>consistent</em>.
That is, it does not contain contradictions.</p>
<p>We will denote logical sentences with capital letters: <span class="math notranslate nohighlight">\(A\)</span>, <span class="math notranslate nohighlight">\(B\)</span>, <span class="math notranslate nohighlight">\(C\)</span>, etc.
We form new logical sentences from old ones using the following logical connectives:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\text{not}\;A \equiv \neg A\)</span>: the logical sentence that is true when <span class="math notranslate nohighlight">\(A\)</span> is false and false when <span class="math notranslate nohighlight">\(A\)</span> is true.</p></li>
<li><p><span class="math notranslate nohighlight">\(A\;\text{and}\;B \equiv A, B \equiv AB\)</span>: the logical sentence that is true when <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> are true and false otherwise.</p></li>
<li><p><span class="math notranslate nohighlight">\(A\;\text{or}\;B \equiv A+B\)</span>: the logical sentence that is true when <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> are true and false otherwise.</p></li>
</ul>
</section>
<section id="probability-as-a-representation-of-our-state-of-knowledge">
<span id="id3"></span><h2>Probability as a representation of our state of knowledge<a class="headerlink" href="#probability-as-a-representation-of-our-state-of-knowledge" title="Link to this heading">#</a></h2>
<p>Let’s call <span class="math notranslate nohighlight">\(I\)</span> the logical sentence containing all <em>information</em> you have now.
And I am talking about absolutely everything:</p>
<ul class="simple">
<li><p>What your parents taught you,</p></li>
<li><p>What you learned in school, what you learned in college,</p></li>
<li><p>What your eyes see right now on some scientific instruments.</p></li>
</ul>
<p>Now consider a logical sentence <span class="math notranslate nohighlight">\(A\)</span> that says something about the world.
For example, <span class="math notranslate nohighlight">\(A\)</span> could be “The result of the next coin toss John performs will be heads.”
Or anything.
We are uncertain about <span class="math notranslate nohighlight">\(A\)</span>.
We do not know if it is true or false.
We could use the information <span class="math notranslate nohighlight">\( I \)</span> to say something more about <span class="math notranslate nohighlight">\( A \)</span>.
Maybe we believe that it is more likely that <span class="math notranslate nohighlight">\(A\)</span> is true than false.
Maybe the opposite.</p>
<p>Probability theory is about this problem.
It gives us a number <span class="math notranslate nohighlight">\(p(A|I)\)</span>.
We read <span class="math notranslate nohighlight">\(p(A|I)\)</span> as “the probability that <span class="math notranslate nohighlight">\(A\)</span> is true given that we know <span class="math notranslate nohighlight">\(I\)</span>.”
It quantifies our degree of belief that <span class="math notranslate nohighlight">\(A\)</span> is true given that we know <span class="math notranslate nohighlight">\(I\)</span>.
We say that <span class="math notranslate nohighlight">\(p(A|I)\)</span> represents our knowledge about <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(I\)</span>.</p>
</section>
<section id="but-what-about-frequencies">
<h2>But what about frequencies?<a class="headerlink" href="#but-what-about-frequencies" title="Link to this heading">#</a></h2>
<p>In introductory courses to probability or statistics, we usually learn that the probability of an event is the frequency with which it occurs in nature.
This interpretation is valid if the event is something that indeed occurs repeatedly.
However, it is pretty restrictive.</p>
<p>In particular, what can we say about an event that can happen only once?
For example, what is the probability that life on Earth will end in a billion years?
It is not possible to repeat this experiment.
It is not even possible to observe its outcome.
The frequency interpretation of probability is not applicable.
However, we would like to use probability to quantify our degree of belief that life on Earth will end in a billion years.
It makes intuitive sense that we can do this.
We want to do this.
Scientists and engineers think like this all the time.</p>
<p>But is our approach compatible with the frequency interpretation?
It can be shown, see <span id="id4">[<a class="reference internal" href="../bibliography.html#id7" title="E. T. Jaynes. Probability theory: The logic of science. Cambridge University Press, Cambridge, 2003.">Jaynes, 2003</a>]</span> for the proof, that our approach is indeed compatible with the frequency interpretation.
That is, when events occur repeatedly, then the probabilities do become frequencies.</p>
</section>
<section id="the-common-sense-assumptions-that-give-rise-to-the-basic-probability-rules">
<h2>The common sense assumptions that give rise to the basic probability rules.<a class="headerlink" href="#the-common-sense-assumptions-that-give-rise-to-the-basic-probability-rules" title="Link to this heading">#</a></h2>
<blockquote>
<div><p>Probability theory is nothing but common sense reduced to calculation. Pierre-Simon Laplace, Théorie analytique des probabilités (1814)</p>
</div></blockquote>
<p>Consider the following three ingredients:</p>
<ul class="simple">
<li><p>A: a logical sentence</p></li>
<li><p>B: another logical sentence</p></li>
<li><p>I: all the information we know</p></li>
</ul>
<p>Now, let’s try to make a robot that can argue under uncertainty.
It should be able to take logical sentences (such as <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> above) and argue about them using all its information.
What sort of system should govern this robot?
The following desiderata, see <span id="id5">[<a class="reference internal" href="../bibliography.html#id7" title="E. T. Jaynes. Probability theory: The logic of science. Cambridge University Press, Cambridge, 2003.">Jaynes, 2003</a>]</span>, seem reasonable:</p>
<ul class="simple">
<li><p>Real numbers represent degrees of plausibility.</p></li>
<li><p>The system should have a qualitative correspondence to common sense.</p></li>
<li><p>The system should be consistent in the sense that:</p>
<ul>
<li><p>If there are two ways to calculate a degree of plausibility, each way must lead to the same result.</p></li>
<li><p>The robot should take into account all evidence relevant to a question.</p></li>
<li><p>Equivalent plausibility assignments must represent equivalent states of knowledge.</p></li>
</ul>
</li>
</ul>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Cox%27s_theorem">Cox’s theorem</a> shows that:</p>
<blockquote>
<div><p>The desiderata are enough to derive the rules of probability theory.</p>
</div></blockquote>
<section id="talking-about-probabilities">
<h3>Talking about probabilities<a class="headerlink" href="#talking-about-probabilities" title="Link to this heading">#</a></h3>
<p>We read <span class="math notranslate nohighlight">\(p(A|BI)\)</span> as:</p>
<ul class="simple">
<li><p>the probability of <span class="math notranslate nohighlight">\(A\)</span> being true given that we know that <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(I\)</span> are true; or</p></li>
<li><p>the probability of <span class="math notranslate nohighlight">\(A\)</span> being true given that we know that <span class="math notranslate nohighlight">\(B\)</span> is true; or</p></li>
<li><p>the probability of <span class="math notranslate nohighlight">\(A\)</span> given <span class="math notranslate nohighlight">\(B\)</span>.</p></li>
</ul>
</section>
<section id="interpretation-of-probabilities">
<h3>Interpretation of probabilities<a class="headerlink" href="#interpretation-of-probabilities" title="Link to this heading">#</a></h3>
<p>The probability <span class="math notranslate nohighlight">\(p(A|BI)\)</span> is a number between 0 and 1, quantifying the degree of plausibility that <span class="math notranslate nohighlight">\(A\)</span> is true given <span class="math notranslate nohighlight">\(B\)</span> and <span class="math notranslate nohighlight">\(I\)</span>.
Specifically:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(A|B,I) = 1\)</span> when we are certain that <span class="math notranslate nohighlight">\(A\)</span> is true if <span class="math notranslate nohighlight">\(B\)</span> is true (and I).</p></li>
<li><p><span class="math notranslate nohighlight">\(p(A|B,I) = 0\)</span> when we are certain that <span class="math notranslate nohighlight">\(A\)</span> is false if <span class="math notranslate nohighlight">\(B\)</span> is true (and <span class="math notranslate nohighlight">\(I\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(0&lt; p(A|B,I) &lt; 1\)</span> when we are uncertain about <span class="math notranslate nohighlight">\(A\)</span> if <span class="math notranslate nohighlight">\(B\)</span> is true (and <span class="math notranslate nohighlight">\(I\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(p(A|B,I) = \frac{1}{2}\)</span> when we are completely ignorant about <span class="math notranslate nohighlight">\(A\)</span> if <span class="math notranslate nohighlight">\(B\)</span> is true (and <span class="math notranslate nohighlight">\(I\)</span>).</p></li>
</ul>
</section>
</section>
<section id="the-rules-of-probability">
<h2>The rules of probability<a class="headerlink" href="#the-rules-of-probability" title="Link to this heading">#</a></h2>
<p>Everything in probability theory derives from two rules.
These are direct consequences of the desiderata and Cox’s theorem.
They are:</p>
<ul class="simple">
<li><p>The <strong>obvious rule</strong>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(A|I) + p(\neg A|I) = 1.
\]</div>
<p>The rule states that <span class="math notranslate nohighlight">\(A\)</span> or its negation must be true.
We see why it is vitally important that you refrain from applying probability in a system that includes contradictions.</p>
<ul class="simple">
<li><p>The <strong>product rule</strong> (or Bayes’ rule or Bayes’ theorem):</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(A,B|I) = p(A|B,I)p(B|I).
\]</div>
<p>The rule states that the probability of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> is the probability of <span class="math notranslate nohighlight">\(A\)</span> given that <span class="math notranslate nohighlight">\(B\)</span> is true times the probability that <span class="math notranslate nohighlight">\(B\)</span> is true.
Even though the correspondence is not one-to-one, visualizing events using the Venn diagrams helps in understanding the product rule:</p>
<figure class="align-default" id="venn">
<img alt="../_images/venn.png" src="../_images/venn.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Venn diagram.</span><a class="headerlink" href="#venn" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>In this diagram:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(p(A,B|I)\)</span> corresponds to the brown area (normalized by the area of I).</p></li>
<li><p><span class="math notranslate nohighlight">\(p(B|I)\)</span> is the area of <span class="math notranslate nohighlight">\(B\)</span> (normalized by the area of <span class="math notranslate nohighlight">\(I\)</span>).</p></li>
<li><p><span class="math notranslate nohighlight">\(p(A|BI)\)</span> is the brown area (normalized by the area of <span class="math notranslate nohighlight">\(B\)</span>).</p></li>
</ul>
<p>Understanding the product rule takes some time.
But it is worth it, as it tells us how to update our knowledge as we gather new information.
The product rule is the essence of scientific machine learning.</p>
<section id="example-drawing-balls-from-a-box-without-replacement-1-3">
<h3>Example: Drawing balls from a box without replacement (1/3)<a class="headerlink" href="#example-drawing-balls-from-a-box-without-replacement-1-3" title="Link to this heading">#</a></h3>
<p>Consider the following information <span class="math notranslate nohighlight">\(I\)</span>:</p>
<blockquote>
<div><p>We are given a box with ten balls, 6 of which are red and 4 of which are blue. The box is sufficiently mixed so that we don’t know which one we pick when we get a ball from it. We do not put a ball back when we take it out of the box.</p>
</div></blockquote>
<figure class="align-default" id="urn">
<img alt="../_images/urn.png" src="../_images/urn.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">A box with balls.</span><a class="headerlink" href="#urn" title="Link to this image">#</a></p>
</figcaption>
</figure>
<p>Now, let’s draw the first ball.
Here is the graphical causal model up to this point:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">graphviz</span> <span class="kn">import</span> <span class="n">Digraph</span>
<span class="n">gu1</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="s1">&#39;Urn1&#39;</span><span class="p">)</span>
<span class="n">gu1</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Background information I&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;filled&#39;</span><span class="p">)</span>
<span class="n">gu1</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Result of 1st draw&#39;</span><span class="p">)</span>
<span class="n">gu1</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">)</span>
<span class="n">gu1</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;ch2.fig1&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span><span class="p">)</span>
<span class="n">gu1</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/006572a43fb583b948f1ed75f94eb43d9d8151703eded04601fcf5ef72e680cd.svg" src="../_images/006572a43fb583b948f1ed75f94eb43d9d8151703eded04601fcf5ef72e680cd.svg" />
</div>
</div>
<p>Now, let’s say that we draw the first ball.
Let <span class="math notranslate nohighlight">\(B_1\)</span> be the sentence:</p>
<blockquote>
<div><p>The first ball we draw is blue.</p>
</div></blockquote>
<p>What is the probability of <span class="math notranslate nohighlight">\(B_1\)</span>?
Our intuition tells us to set:</p>
<div class="math notranslate nohighlight">
\[
p(B_1|I) = \frac{4}{10} = \frac{2}{5}.
\]</div>
<p>What just did has a name.
It is known as the <em>principle of insufficient reason</em>.
We will learn more about it later.</p>
<p>We can now use the <strong>obvious rule</strong> to find the probability of drawing a red ball, i.e., of <span class="math notranslate nohighlight">\(\neg B_1\)</span>.
Of course, <span class="math notranslate nohighlight">\(\neg B_1\)</span> is just the sentence:</p>
<blockquote>
<div><p>The first ball we draw is red.</p>
</div></blockquote>
<p>So, let’s call it also <span class="math notranslate nohighlight">\(R_1\)</span>.
It is:</p>
<div class="math notranslate nohighlight">
\[
p(R_1|I) = p(\neg B_1|I) = 1 - p(B_1|I) = 1 - \frac{2}{5} = \frac{3}{5}.
\]</div>
<p>Now let’s do something else.
Let’s say we observe the first draw and we want to calculate the probability of drawing a blue ball on the second draw.
Consider the causal graph of the problem.
We need to fill the node corresponding to the first draw with color:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gu3</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="s1">&#39;Urn3&#39;</span><span class="p">)</span>
<span class="n">gu3</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Background information I&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;filled&#39;</span><span class="p">)</span>
<span class="n">gu3</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Result of 1st draw&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;filled&#39;</span><span class="p">)</span>
<span class="n">gu3</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Result of 2nd draw&#39;</span><span class="p">)</span>
<span class="n">gu3</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">)</span>
<span class="n">gu3</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">)</span>
<span class="n">gu3</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">)</span>
<span class="n">gu3</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;ch2.fig2&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span><span class="p">)</span>
<span class="n">gu3</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/fff2ed5767e888002db1cf8091eb87bfbd5235270038755b4fd962f08bc4c9fe.svg" src="../_images/fff2ed5767e888002db1cf8091eb87bfbd5235270038755b4fd962f08bc4c9fe.svg" />
</div>
</div>
<p>Consider the sentence <span class="math notranslate nohighlight">\(R_2\)</span>:</p>
<blockquote>
<div><p>The second ball we draw is red.</p>
</div></blockquote>
<p>Suppose we know that <span class="math notranslate nohighlight">\(B_1\)</span> is true, i.e., the first ball we draw is blue.
What is the probability of <span class="math notranslate nohighlight">\(R_2\)</span> given that <span class="math notranslate nohighlight">\(B_1\)</span> is true?
We can use common sense to find this probability:</p>
<ul class="simple">
<li><p>We had ten balls, six red and four blue.</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(B_1\)</span> is true (the first ball was blue), we now have six red and three blue balls.</p></li>
<li><p>Therefore, the probability that we draw a red ball next is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(R_2|B_1,I) = \frac{6}{9} = \frac{2}{3}.
\]</div>
<p>Similarly, we can find the probability that we draw a red ball in the second draw given that we drew a red ball in the first draw:</p>
<ul class="simple">
<li><p>We had ten balls, six red and four blue.</p></li>
<li><p>Since <span class="math notranslate nohighlight">\(R_1\)</span> is true (the first ball is red), we now have five red and four blue balls.</p></li>
<li><p>Therefore, the probability that we draw a red ball next is:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(R_2|R_1,I) = \frac{5}{9}.
\]</div>
<p>Okay, let’s do something more challenging.
Let’s consider a second draw without observing the result of the first draw.
Here is the causal graph (notice that we do not shade the first draw):</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gu2</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="s1">&#39;Urn2&#39;</span><span class="p">)</span>
<span class="n">gu2</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Background information I&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;filled&#39;</span><span class="p">)</span>
<span class="n">gu2</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Result of 1st draw&#39;</span><span class="p">)</span>
<span class="n">gu2</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Result of 2nd draw&#39;</span><span class="p">)</span>
<span class="n">gu2</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">)</span>
<span class="n">gu2</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;I&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">)</span>
<span class="n">gu2</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">)</span>
<span class="n">gu2</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;ch2.fig3&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;pdf&#39;</span><span class="p">)</span>
<span class="n">gu2</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/65f4ed5b58d0b1af78a66a7c2ee7f731f9b90a26d9ddfee017d92ae5955c20b8.svg" src="../_images/65f4ed5b58d0b1af78a66a7c2ee7f731f9b90a26d9ddfee017d92ae5955c20b8.svg" />
</div>
</div>
<p>Let’s find the probability that we will draw a blue ball in the first draw (A) and a red ball in the second draw (B).
We have to use the <strong>product rule</strong>:</p>
<div class="math notranslate nohighlight">
\[
p(B_1, R_2|I) = p(R_2|B_1,I) p(B_1|I) = \frac{2}{3}\frac{2}{5} = \frac{4}{15}.
\]</div>
</section>
<section id="other-rules-of-probability-theory">
<h3>Other rules of probability theory<a class="headerlink" href="#other-rules-of-probability-theory" title="Link to this heading">#</a></h3>
<p>All other rules of probability theory can be derived from the two basic rules.
Here are some examples.</p>
<section id="extension-of-the-obvious-rule">
<h4>Extension of the obvious rule<a class="headerlink" href="#extension-of-the-obvious-rule" title="Link to this heading">#</a></h4>
<p>All other rules of probability theory can be derived from the two basic rules.
Here are some examples.</p>
</section>
<section id="id6">
<h4>Extension of the obvious rule<a class="headerlink" href="#id6" title="Link to this heading">#</a></h4>
<p>For any two logical sentences <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(B\)</span> we have:</p>
<div class="math notranslate nohighlight">
\[
p(A + B|I) = p(A|I) + p(B|I) - p(AB|I).
\]</div>
<p>In words: the probability of <span class="math notranslate nohighlight">\(A\)</span> or <span class="math notranslate nohighlight">\(B\)</span> is the probability that A is true plus the probability that <span class="math notranslate nohighlight">\(B\)</span> is true minus the probability that both A and B are true.
It is easy to understand the rule intuitively by looking at the <a class="reference internal" href="#venn"><span class="std std-ref">Venn diagram</span></a>.</p>
<p>The probability <span class="math notranslate nohighlight">\(p(A+B|I)\)</span> is the area of the union of A with B (normalized by I).
This area is indeed the area of A (normalized by I) plus the area of B (normalized by I) minus the area of A and B (normalized by I), which was double-counted.</p>
<div class="dropdown admonition">
<p class="admonition-title">Here is the proof.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
p(A+B|I) &amp;= 1 - p(\neg (A+B)|I)\\
&amp;= 1 - p(\neg A, \neg B|I)\;\text{(obvious rule)}\\
&amp;= 1 - p(\neg A|\neg B, I)p(\neg B|I)\;\text{(product rule)}\\
&amp;= 1 - \left[1 - p(A|\neg B, I)\right]p(\neg B|I)\;\text{(obvious rule)}\\
&amp;= 1 - p(\neg B|I) + p(A|\neg B, I)p(\neg B|I)\\
&amp;= 1 - p(\neg B|I) + p(A\neg B|I)\;\text{(product rule)}\\
&amp;= 1 - p(\neg B|I) + p(\neg B|A,I) p(A|I)\;\text{(product rule)}\\
&amp;= 1 - p(\neg B|I) + \left[1 - p(B|A,I)\right]p(A|I)\;\text{(obvious rule)}\\
&amp;= 1 - p(\neg B|I) + p(A|I) - p(B|A,I)p(A|I)\\
&amp;= 1 - \left[1 - p(B|I)\right] + p(A|I) - p(B|A,I)p(A|I)\;\text{(obvious rule)}\\
&amp;= p(A|I) + p(B|I) - p(B|A,I)p(A|I)\\
&amp;= p(A|I) + p(B|I) - p(AB|I)\;\text{(product rule)}.
\end{split}
\end{split}\]</div>
</div>
</section>
</section>
<section id="the-sum-rule">
<h3>The sum rule<a class="headerlink" href="#the-sum-rule" title="Link to this heading">#</a></h3>
<p>The sum rule is the final rule we will consider in this lecture.
It is one of the most important rules.
<strong>You have to memorize it.</strong>
It goes as follows.</p>
<p>Consider the sequence of logical sentences <span class="math notranslate nohighlight">\(B_1,\dots,B_n\)</span> such that:</p>
<ul class="simple">
<li><p>One of them is true:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
p(B_1 + \dots + B_n|I) = 1.
\]</div>
<ul class="simple">
<li><p>No two of them can be true at the same time:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\begin{split}
p(B_iB_j|I) = \delta_{ij} = \begin{cases}1,&amp;\;\text{if}\;i=j,\\ 0,&amp;\;\text{otherwise}.\end{cases}
\end{split}\]</div>
<p>Then, for any logical sentence <span class="math notranslate nohighlight">\(A\)</span>, we have:</p>
<div class="math notranslate nohighlight">
\[
p(A|I) = \sum_{i=1}^n p(AB_i|I) = \sum_{i=1}^n p(A|B_i,I)p(B_i|I).
\]</div>
<p>Again, this requires a bit of meditation.
You take any logical sentence A and set of <em>mutually exclusive</em> but <em>exhaustive</em> possibilities <span class="math notranslate nohighlight">\(B_1,\dots, B_n\)</span>, and you break down the probability of <span class="math notranslate nohighlight">\(A\)</span> in terms of the probabilities of the <span class="math notranslate nohighlight">\(B_i\)</span>’s.
The Venn diagrams help to understand the situation:</p>
<figure class="align-default" id="venn-sum-rul">
<img alt="../_images/venn_sum_rule.png" src="../_images/venn_sum_rule.png" />
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Venn diagram demonstration of the sum rule.</span><a class="headerlink" href="#venn-sum-rul" title="Link to this image">#</a></p>
</figcaption>
</figure>
<div class="admonition-the-sum-rule-can-be-trivially-proved-by-induction-using-only-the-obvious-rule-and-the-product-rule admonition">
<p class="admonition-title">The sum rule can be trivially proved by induction using only the obvious rule and the product rule.</p>
<p>It is instructive to go through the proof once.
:class: dropdown
For <span class="math notranslate nohighlight">\(n=2\)</span> we have:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
p(A|I) &amp;= p(A\;\text{and}\;(B_1\;\text{or}\;B_2)|I)\\
&amp;= p\left((A\;\text{and}\;B_1)\;\text{or}\;(A\;\text{and}\;B_2)|I\right)\\
&amp;= p(A\;\text{and}\;B_1|I) + p(A\;\text{and}\;B_2|I) - p\left((A\;\text{and}\;B_1)\;\text{and}\;(A\;\text{and}\;B_2)|I\right)\\
&amp;= p(AB_1|I) + p(AB_2|I) - p(AB_1B_2|I)\\
&amp;= p(AB_1|I) + p(AB_2|I),
\end{split}
\end{split}\]</div>
<p>because</p>
<div class="math notranslate nohighlight">
\[
p(AB_1B_2|I) = p(B_1B_2|I)p(A|I) \le p(B_1B_2|I) = 0.
\]</div>
<p>And then, assume that it holds for <span class="math notranslate nohighlight">\(n\)</span>, you can easily show that it also holds for <span class="math notranslate nohighlight">\(n+1\)</span> completing the proof.</p>
</div>
</section>
<section id="example-drawing-balls-from-a-box-without-replacement-2-3">
<h3>Example: Drawing balls from a box without replacement (2/3)<a class="headerlink" href="#example-drawing-balls-from-a-box-without-replacement-2-3" title="Link to this heading">#</a></h3>
<p>Let us consider the probability of getting a red ball in the second draw without observing the first draw.
We have two possibilities for the first draw.
We got a blue ball (<span class="math notranslate nohighlight">\(B_1\)</span> is true) or a red ball (R_1 is true).
In other words, <span class="math notranslate nohighlight">\(B_1\)</span> and <span class="math notranslate nohighlight">\(R_1\)</span> cover all possibilities and are mutually exclusive.
We can use the sum rule:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
p(R_2|I) &amp;= p(R_2|B_1,I)p(B_1|I) + p(R_2|R_1,I)p(R_1|I)\\
&amp;= \frac{2}{3}\frac{2}{5} + \frac{5}{9}\frac{3}{5}\\
&amp;= 0.6.
\end{split}
\end{split}\]</div>
</section>
<section id="example-drawing-balls-from-a-box-without-replacement-3-3">
<h3>Example: Drawing balls from a box without replacement (3/3)<a class="headerlink" href="#example-drawing-balls-from-a-box-without-replacement-3-3" title="Link to this heading">#</a></h3>
<p>If you paid close attention, in all our examples, the conditioning we did followed the causal links.
For instance, we wrote <span class="math notranslate nohighlight">\(p(R_2|B_1 I)\)</span> for the probability of getting a red ball in the second draw after observing the blue ball in the first draw.
However, conditioning on stuff <strong>does not have to follow causal links</strong>.
It is legitimate to ask what the probability of a blue ball is in the first draw, given that you have observed that the result of the second draw is a red ball.
I visualize the situation in the following graph:</p>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">gu4</span> <span class="o">=</span> <span class="n">Digraph</span><span class="p">(</span><span class="s1">&#39;Urn4&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;reds&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;# red balls&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;filled&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;blues&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;# blue balls&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;filled&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;1st draw&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">node</span><span class="p">(</span><span class="s1">&#39;second&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;2nd draw&#39;</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;filled&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;reds&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;blues&#39;</span><span class="p">,</span> <span class="s1">&#39;first&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;first&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;reds&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">edge</span><span class="p">(</span><span class="s1">&#39;blues&#39;</span><span class="p">,</span> <span class="s1">&#39;second&#39;</span><span class="p">)</span>
<span class="n">gu4</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s1">&#39;urn4_graph&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;png&#39;</span><span class="p">)</span>
<span class="n">gu4</span>
</pre></div>
</div>
</div>
</details>
<div class="cell_output docutils container">
<img alt="../_images/c5158f084f54dd9f99b489c86a13bed567a204dc305777210b958353a89480e6.svg" src="../_images/c5158f084f54dd9f99b489c86a13bed567a204dc305777210b958353a89480e6.svg" />
</div>
</div>
<p>You can write down the mathematical expression <span class="math notranslate nohighlight">\(p(B_1|R_2, I)\)</span>.
It does not mean that <span class="math notranslate nohighlight">\(R_2\)</span> is causing <span class="math notranslate nohighlight">\(B_1\)</span>.
What happens here is that observing <span class="math notranslate nohighlight">\(R_2\)</span> changes your state of knowledge about <span class="math notranslate nohighlight">\(B_1\)</span>.
It is an example of information flowing in the reverse order of a causal link and a quintessential example of the inverse problem.
Let’s solve it analytically:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{split}
p(B_1|R_2,I) &amp;= \frac{p(B_1,R_2|I)}{p(R_2|I)}\\
&amp;= \frac{\frac{4}{15}}{0.6}\\
&amp;\approx 0.44.
\end{split}
\end{split}\]</div>
<p>The probability is greater than that of drawing a blue ball in the first place, <span class="math notranslate nohighlight">\(p(B_1|I) = 0.4\)</span>.
Does this make sense?
Yes, it does!
Here is what you should think:</p>
<ul class="simple">
<li><p>You draw a ball without seeing it and put it in a box.</p></li>
<li><p>You draw the second ball, and you see that it is a red one.</p></li>
<li><p>This means you did not pick this red ball in the first draw.</p></li>
<li><p>So, it is as if, in the first draw, you had one less red to worry about, which increases the probability of a blue.</p></li>
<li><p>So, it is as if you had five red balls and four blue balls, giving you a probability of blue <span class="math notranslate nohighlight">\(\frac{4}{9}\approx 0.44\)</span>.</p></li>
</ul>
<p>It is amazing!
It agrees perfectly with the prediction of the product rule.
Recall that one of our desiderata is that if you compute something in two different ways, you should get the same result.
You can rest assured that it is impossible to get the wrong answer as soon as you use the product rule, the sum rule, and logic.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./lecture02"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Lecture 2 - Basics of Probability Theory</p>
      </div>
    </a>
    <a class="right-next"
       href="hands-on-02.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Experiment with “Randomness”</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#logical-sentences">Logical sentences</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#probability-as-a-representation-of-our-state-of-knowledge">Probability as a representation of our state of knowledge</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#but-what-about-frequencies">But what about frequencies?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-common-sense-assumptions-that-give-rise-to-the-basic-probability-rules">The common sense assumptions that give rise to the basic probability rules.</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#talking-about-probabilities">Talking about probabilities</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#interpretation-of-probabilities">Interpretation of probabilities</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-rules-of-probability">The rules of probability</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-drawing-balls-from-a-box-without-replacement-1-3">Example: Drawing balls from a box without replacement (1/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#other-rules-of-probability-theory">Other rules of probability theory</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#extension-of-the-obvious-rule">Extension of the obvious rule</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">Extension of the obvious rule</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#the-sum-rule">The sum rule</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-drawing-balls-from-a-box-without-replacement-2-3">Example: Drawing balls from a box without replacement (2/3)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#example-drawing-balls-from-a-box-without-replacement-3-3">Example: Drawing balls from a box without replacement (3/3)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ilias Bilionis (ibilion[at]purdue.edu)
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>