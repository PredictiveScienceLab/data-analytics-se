{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical Background on Classification\n",
    "\n",
    "\n",
    "## Binary classification\n",
    "\n",
    "Imagine that you have a bunch of observations consisting of inputs/features $\\mathbf{x}_{1:n}=(\\mathbf{x}_1,\\dots,\\mathbf{x}_n)$ and the corresponding targets $y_{1:n}=(y_1,\\dots,y_n)$.\n",
    "Remember that we say that we have a classification problem when the targets are discrete labels.\n",
    "In particular, if the labels are two, say 0 or 1, then we say that we have a *binary classification problem.\n",
    "\n",
    "## Logistic regression\n",
    "\n",
    "Imagine that you have a bunch of observations consisting of inputs/features $\\mathbf{x}_{1:N}=(\\mathbf{x}_1,\\dots,\\mathbf{x}_N)$ and the corresponding targets $y_{1:N}=(y_1,\\dots,y_N)$.\n",
    "Remember that we say that we have a classification problem when the targets are discrete labels.\n",
    "In particular, if the labels are two, say 0 or 1, then we say that we have a *binary classification problem*.\n",
    "\n",
    "The logistic regression model is one of the simplest ways to solve the binary classification problem.\n",
    "It goes as follows.\n",
    "You model the probability that $y=1$ conditioned on having $\\mathbf{x}$ by:\n",
    "\n",
    "$$\n",
    "p(y=1|\\mathbf{x},\\mathbf{w}) = \\operatorname{sigm}\\left(\\sum_{j=1}^Mw_j\\phi_j(\\mathbf{x})\\right) =  \\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})\\right),\n",
    "$$\n",
    "\n",
    "where $\\operatorname{sigm}$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function), the $\\phi_j(\\mathbf{x})$ are $m$ basis functions/features,\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\phi}(\\mathbf{x}) = \\left(\\phi_1(\\mathbf{x}),\\dots,\\phi_M(\\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "and the $w_j$'s are $m$ weights that we need to learn from the data.\n",
    "The sigmoid function is defined by:\n",
    "\n",
    "$$\n",
    "\\operatorname{sigm}(z) = \\frac{1}{1+e^{-z}},\n",
    "$$\n",
    "\n",
    "and all it does is it takes a real number and maps it to $[0,1]$ so that it can represent a probability, see {numref}`sigmoid`.\n",
    "In other words, logistic regression is just a generalized linear model passed through the sigmoid function so that it is mapped to $[0,1]$.\n",
    "\n",
    "If you need the probability of $y=0$, it is given by the obvious rule:\n",
    "\n",
    "$$\n",
    "p(y=0|\\mathbf{x},\\mathbf{w}) = 1 - p(y=1|\\mathbf{x},\\mathbf{w}) = 1 - \\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})\\right)\n",
    "$$\n",
    "\n",
    "You can represent the probability of an arbitrary label $y$ conditioned on $\\mathbf{x}$ using this simple trick:\n",
    "\n",
    "$$\n",
    "p(y|\\mathbf{x},\\mathbf{w}) =\n",
    "\\left[\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})\\right)\\right]^y\n",
    "\\left[1-\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x})\\right)\\right]^{1-y}.\n",
    "$$\n",
    "\n",
    "Notice that when $y=1$, the exponent of the second term becomes zero and thus the term becomes one.\n",
    "Similarly, when $y=0$, the exponent of the first tierm becomes zero and thus the term becomes one.\n",
    "This gives the right probability for each case.\n",
    "\n",
    "The likelihood of all the observed data is:\n",
    "\n",
    "$$\n",
    "p(y_{1:N}|\\mathbf{x}_{1:n},\\mathbf{w}) = \\prod_{i=1}^Np(y_i |\\mathbf{x}_i, \\mathbf{w})\n",
    "= \\prod_{i=1}^N\n",
    "\\left[\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)\\right]^{y_i}\n",
    "\\left[1-\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)\\right]^{1-y_i}.\n",
    "$$\n",
    "\n",
    "We can now find the best weight vector $\\mathbf{w}$ using the  [maximum likelihood principle](https://purduemechanicalengineering.github.io/me-297-intro-to-data-science/lecture13/the-maximum-likelihood-principle.html).\n",
    "We need to solve the optimization problem:\n",
    "\n",
    "$$\n",
    "\\max_{\\mathbf{w}}\\log p(y_{1:N}|\\mathbf{x}_{1:n},\\mathbf{w})\n",
    "= \\max_{\\mathbf{w}}\\sum_{i=1}^N\\left\\{y_i\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)+(1-y_i)\\left[1-\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)\\right]\\right\\}.\n",
    "$$\n",
    "\n",
    "Notice that the following maximization problem is equivalent to minimizing this loss function:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = -\\sum_{i=1}^N\\left\\{y_i\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)+(1-y_i)\\left[1-\\operatorname{sigm}\\left(\\mathbf{w}^T\\boldsymbol{\\phi}(\\mathbf{x}_i)\\right)\\right]\\right\\}.\n",
    "$$\n",
    "\n",
    "This is known as the [cross-entropy loss function](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) and you are very likely to encounter it if you dive deeper into modern data science.\n",
    "For example, we use the same loss function to train state-of-the-art deep neural networks that classify images.\n",
    "You now know that it does not come out of the blue.\n",
    "It comes from the maximum likelihood principle.\n",
    "\n",
    "### Examples\n",
    "\n",
    "See [this](logistic_regression_with_one_variable) and [this](logistic_regression_with_many_features)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making-decisions\n",
    "\n",
    "Say that you have found a point estimate for the weights $\\mathbf{w}$ based on the data.\n",
    "Let's call that point estimate $\\mathbf{w}^*$.\n",
    "You can predict the probability of $y$ taking any value by evaluating $p(y|\\mathbf{x},\\mathbf{w}=\\mathbf{w}^*)$.\n",
    "That's what your model predicts: a probability mass function over the two possible values of $y$.\n",
    "Now, let's say that you have to decide whether $y$ is equal to 0 or 1.\n",
    "How do you do this?\n",
    "You need to pose and solve a decision making problem.\n",
    "Like we did before, you start by quantifying the loss you incur when you make the wrong decision.\n",
    "Mathematically, let $\\hat{y}$ be what you choose and $y$ be the true value of the label.\n",
    "You need to define the function:\n",
    "\n",
    "$$\n",
    "\\ell(\\hat{y},y) = \\text{the cot of picking $\\hat{y}$ when the true value is $y$}.\n",
    "$$\n",
    "\n",
    "Of course, the choice of the cost function is subjective.\n",
    "For the binary classification case we are looking at, $\\ell(\\hat{y},y)$ is just a $2\\times 2$ matrix (two possibilities for $\\hat{y}$ and two possibilities for $y$).\n",
    "Once you have your loss function, the rational thing to do is to pick the $\\hat{y}$ that minimizex your *expected loss*.\n",
    "The expectation here is over what you think the true value of $y$ is.\n",
    "This state of knowledge is summarized in the trained logistic regression model.\n",
    "Therefore, the problem you need to solve to pick $\\hat{y}$ is:\n",
    "\n",
    "$$\n",
    "\\min_{\\hat{y}} \\sum_{y=0,1} \\ell(\\hat{y},y)p(y|\\mathbf{x},\\mathbf{w}=\\mathbf{w}^*).\n",
    "$$\n",
    "\n",
    "Notice that this is a different optimization problem for each possible value of $\\mathbf{x}$.\n",
    "\n",
    "### Examples\n",
    "\n",
    "See [this](classification_decision_making)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics for classification\n",
    "\n",
    "As always you need to split your dataset in training and validation subsets.\n",
    "There are many different diagnostics you can use on your validation dataset.\n",
    "The two that we are going to cover in the video lecture are:\n",
    "+ [Accuracy score](https://scikit-learn.org/stable/modules/model_evaluation.html#accuracy-score)\n",
    "+ [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix)\n",
    "\n",
    "See [this](diagnostics_for_classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "Imagine that you have a bunch of observations consisting of inputs/features $\\mathbf{x}_{1:n}=(\\mathbf{x}_1,\\dots,\\mathbf{x}_n)$ and the corresponding discrete labels $y_{1:n}=(y_1,\\dots,y_n)$.\n",
    "But now there are not just two labels.\n",
    "There are $K$ possible values for the labels.\n",
    "That's *multi-class classification*.\n",
    "\n",
    "## Multi-class logistic regression\n",
    "\n",
    "Just like before assume that you have a set of $m$ basis function $\\phi_j(\\mathbf{x})$ which we will use to make generalized linear models.\n",
    "The multi-class logistic regession model is defined by:\n",
    "\n",
    "$$\n",
    "p(y=k|\\mathbf{x}, \\mathbf{W}) = \\operatorname{softmax}_k\\left(\\mathbf{w}_1^T\\boldsymbol{\\phi}(\\mathbf{x}),\\dots,\\mathbf{w}_K^T\\boldsymbol{\\phi}(\\mathbf{x})\\right),\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{W} = \\left(\\mathbf{w}_1,\\dots,\\mathbf{w}_K\\right)\n",
    "$$\n",
    "\n",
    "is a collection of $K$ weight vectors with $m$ elements (one $m$-dimensional weight vector for each of the $K$ classes), and $\\operatorname{softmax}_k(z_1,\\dots,z_K)$ is the $k$-th component of a vector function of $K$ real inputs defined by:\n",
    "\n",
    "$$\n",
    "\\operatorname{softmax}_k(z_1,\\dots,z_K) = \\frac{z_k}{\\sum_{k'=1}^Kz_{k'}}.\n",
    "$$\n",
    "\n",
    "The role of the softmax is to take the real outputs of the generalized linear models corresponding to each label and map them to a probability.\n",
    "Just like in the logistic regression case, you can train the model by putting a prior on the weights and then maximizing the logarithm of the posterior.\n",
    "\n",
    "### Examples\n",
    "\n",
    "See [this](multi-class_logistic_regression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
