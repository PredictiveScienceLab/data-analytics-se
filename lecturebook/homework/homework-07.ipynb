{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QZHHMrMWl7Q1"
   },
   "source": [
    "# Homework 7\n",
    "\n",
    "## References\n",
    "\n",
    "+ Lectures 24-26 (inclusive).\n",
    "\n",
    "## Instructions\n",
    "\n",
    "+ Type your name and email in the \"Student details\" section below.\n",
    "+ Develop the code and generate the figures you need to solve the problems using this notebook.\n",
    "+ For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.\n",
    "+ The total homework points are 100. Please note that the problems are not weighed equally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(rc={\"figure.dpi\":100, \"savefig.dpi\":300})\n",
    "sns.set_context(\"notebook\")\n",
    "sns.set_style(\"ticks\")\n",
    "\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "def download(\n",
    "    url : str,\n",
    "    local_filename : str = None\n",
    "):\n",
    "    \"\"\"Download a file from a url.\n",
    "    \n",
    "    Arguments\n",
    "    url            -- The url we want to download.\n",
    "    local_filename -- The filemame to write on. If not\n",
    "                      specified \n",
    "    \"\"\"\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    urllib.request.urlretrieve(url, local_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student details\n",
    "\n",
    "+ **First Name:**\n",
    "+ **Last Name:**\n",
    "+ **Email:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - Using DNNs to Analyze Experimental Data\n",
    "\n",
    "In this problem you have to use a deep neural network (DNN) to perform a regression task.\n",
    "The dataset we are going to use is the [Airfoil Self-Noise Data Set])https://archive.ics.uci.edu/ml/datasets/Airfoil+Self-Noise#)\n",
    "From this reference, the descreption of the dataset is as follows:\n",
    "\n",
    "> The NASA data set comprises different size NACA 0012 airfoils at various wind tunnel speeds and angles of attack. The span of the airfoil and the observer position were the same in all of the experiments.\n",
    "> \n",
    "> Attribute Information:\n",
    "> This problem has the following inputs:\n",
    "> 1. Frequency, in Hertzs.\n",
    "> 2. Angle of attack, in degrees.\n",
    "> 3. Chord length, in meters.\n",
    "> 4. Free-stream velocity, in meters per second.\n",
    "> 5. Suction side displacement thickness, in meters.\n",
    "\n",
    "> The only output is:\n",
    "> 6. Scaled sound pressure level, in decibels.\n",
    "\n",
    "You will have to do regression between the inputs and the output using a DNN.\n",
    "Before we start, let's download and load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00291/airfoil_self_noise.dat'\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are in simple text format.\n",
    "Here is how we can load them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('airfoil_self_noise.dat')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may work directly with `data`, but, for your convenience, I am going to put them also in a nice Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=['Frequency', 'Angle_of_attack', 'Chord_length',\n",
    "                                 'Velocity', 'Suction_thickness', 'Sound_pressure'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A - Analyze the data visually\n",
    "\n",
    "It is always a good idea to visualize the data before you start doing anything with them.\n",
    "\n",
    "#### Part A.I - Do the histogtrams of all variables\n",
    "Use as many code segments you need below to plot the histogram of each variable (all inputs and the output in separate plots)\n",
    "Discuss whether or not you need to standarize the data before moving to regression.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A.II - Do the scatter plots between all input variables\n",
    "\n",
    "Do the scatter plot between all input variables. This will give you an idea of the range of experimental conditions.\n",
    "Whatever model you build will only be valid inside the domain implicitly defined with your experimental conditions.\n",
    "Are there any holes in the dataset, i.e., places where you have no data?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part A.III - Do the scatter plots between each input and the output\n",
    "\n",
    "Do the scatter plot between each input variable and the output.\n",
    "This will give you an idea of the functional relationship between the two.\n",
    "Do you observe any obvious patterns?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Use DNN to do regression\n",
    "\n",
    "Let start by separating inputs and outputs for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:, :-1]\n",
    "y = data[:, -1][:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B.I - Make the loss\n",
    "\n",
    "Use standard torch functionality, to create a function that gives you the sum of square error followed by an L2 regularization term for the weights and biaseas of all netework parameters (remember that the L2 regularization is like putting a Gaussian prior on each parameter).\n",
    "Follow the instructions bellow and fill in the missing code.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Use standard torch functionality to define a function\n",
    "# mse_loss(y_obs, y_pred) which gives you the mean of the sum of the square\n",
    "# of the difference between y_obs and y_pred\n",
    "# Hint: This is already implemented in PyTorch. You can just reuse it.\n",
    "mse_loss = # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code here\n",
    "y_obs_tmp = np.random.randn(100, 1)\n",
    "y_pred_tmp = np.random.randn(100, 1)\n",
    "print('Your mse_loss: {0:1.2f}'.format(mse_loss(torch.Tensor(y_obs_tmp), \n",
    "                                                torch.Tensor(y_pred_tmp))))\n",
    "print('What you should be getting: {0:1.2f}'.format(np.mean((y_obs_tmp - y_pred_tmp) ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will create a regularization term for the loss\n",
    "# I'm just going to give you this one:\n",
    "def l2_reg_loss(params):\n",
    "    \"\"\"\n",
    "    This needs an iterable object of network parameters.\n",
    "    You can get it by doing `net.parameters()`.\n",
    "    \n",
    "    Returns the sum of the squared norms of all parameters.\n",
    "    \"\"\"\n",
    "    l2_reg = torch.tensor(0.)\n",
    "    for p in params:\n",
    "        l2_reg += torch.norm(p) ** 2\n",
    "    return l2_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's add the two together to make a mean square error loss\n",
    "# plus some weight (which we will call reg_weight) times the sum of the squared norms\n",
    "# of all parameters.\n",
    "# I give you the signature and you have to implement the rest of the code:\n",
    "def loss_func(y_obs, y_pred, reg_weight, params):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    y_obs      -    The observed outputs\n",
    "    y_pred     -    The predicted outputs\n",
    "    reg_weight -    The regularization weight (a positive scalar)\n",
    "    params     -    An iterable containing the parameters of the network\n",
    "    \n",
    "    Returns the sum of the MSE loss plus reg_weight times the sum of the squared norms of\n",
    "    all parameters.\n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "    raise NotImplementedError('Implement me and delete this line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can try your final code here\n",
    "# First, here is a dummy model\n",
    "dummy_net = nn.Sequential(nn.Linear(10, 20),\n",
    "                          nn.Sigmoid(),\n",
    "                          nn.Linear(20, 1))\n",
    "loss = loss_func(torch.Tensor(y_obs_tmp), torch.Tensor(y_pred_tmp),\n",
    "                 0.0,\n",
    "                 dummy_net.parameters())\n",
    "print('The loss without regularization: {0:1.2f}'.format(loss.item()))\n",
    "print('This should be the same as this: {0:1.2f}'.format(mse_loss(torch.Tensor(y_obs_tmp), torch.Tensor(y_pred_tmp))))\n",
    "loss = loss_func(torch.Tensor(y_obs_tmp), torch.Tensor(y_pred_tmp),\n",
    "                 0.01,\n",
    "                 dummy_net.parameters())\n",
    "print('The loss with regularization: {0:1.2f}'.format(loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part B.III - Write flexible code to perform regression\n",
    "\n",
    "When training neural networks you have to hand-pick many parameters: from the structure of the network to the activation functions to the regularization parameters to the details of the stochatic optimization.\n",
    "Instead of blindly going through trial and error, it is better to think about the parameters you want to investigate (vary) and write code that allows you to repeatly train networks with all different parameter variations.\n",
    "In what follows, I will guide you through writing code for training an arbitrary regression network having the flexibility to:\n",
    "\n",
    "- standarize the inputs and output or not\n",
    "- experiment with various levels of regularization\n",
    "- change the learning rate of the stochatic optimization algorithm\n",
    "- change the batch size of the optimization algorithm\n",
    "- change the number of epochs (how many times the optimization algorithm\n",
    "  does a complete sweep through all the data.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to start by creating a class that encapsulates a regression\n",
    "# network so that we can turn on or off input/output standarization\n",
    "# without too much fuss.\n",
    "# The class will essentially represent a trained network model.\n",
    "# It will \"know\" whether or not during training we standarized the data.\n",
    "# I am not asking you to do anything here, so you may just run this code segment\n",
    "# or read through if you want to know about the details.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class TrainedModel(object):\n",
    "    \"\"\"\n",
    "    A class that represents a trained network model.\n",
    "    The main reason I created this class is to encapsulate the standarization\n",
    "    process in a nice way.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    net            -    A network.\n",
    "    standarized    -    True if the network expects standarized features and outputs\n",
    "                        standarized targets. False otherwise.\n",
    "    feature_scaler -    A feature scalar - Ala scikit learn. Must have transform()\n",
    "                        and inverse_transform() implemented.\n",
    "    target_scaler  -    Similar to feature_scaler but for targets...\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, net, standarized=False, feature_scaler=None, target_scaler=None):\n",
    "        self.net = net\n",
    "        self.standarized = standarized\n",
    "        self.feature_scaler = feature_scaler\n",
    "        self.target_scaler = target_scaler\n",
    "        \n",
    "    def __call__(self, X):\n",
    "        \"\"\"\n",
    "        Evaluates the model at X.\n",
    "        \"\"\"\n",
    "        # If not scaled, then the model is just net(X)\n",
    "        if not self.standarized:\n",
    "            return self.net(X)\n",
    "        # Otherwise:\n",
    "        # Scale X:\n",
    "        X_scaled = self.feature_scaler.transform(X)\n",
    "        # Evaluate the network output - which is also scaled:\n",
    "        y_scaled = self.net(torch.Tensor(X_scaled))\n",
    "        # Scale the output back:\n",
    "        y = self.target_scaler.inverse_transform(y_scaled.detach().numpy())\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through the code that follows and fill in the missing parts\n",
    "from sklearn.model_selection import train_test_split\n",
    "# We need this for a progress bar:\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_net(X, y, net, reg_weight, n_batch, epochs, lr, test_size=0.33,\n",
    "              standarize=True):\n",
    "    \"\"\"\n",
    "    A function that trains a regression neural network using stochatic gradient\n",
    "    descent and returns the trained network. The loss function being minimized is\n",
    "    `loss_func`.\n",
    "    \n",
    "    Arguments:\n",
    "    \n",
    "    X          -    The observed features\n",
    "    y          -    The observed targets\n",
    "    net        -    The network you want to fit\n",
    "    n_batch    -    The batch size you want to use for stochastic optimization\n",
    "    epochs     -    How many times do you want to pass over the training dataset.\n",
    "    lr         -    The learning rate for the stochastic optimization algorithm.\n",
    "    test_size  -    What percentage of the data should be used for testing (validation).\n",
    "    standarize -    Whether or not you want to standarize the features and the targets.\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "\n",
    "    # Standarize the data\n",
    "    if standarize:\n",
    "        # Build the scalers\n",
    "        feature_scaler = StandardScaler().fit(X)\n",
    "        target_scaler = StandardScaler().fit(y)\n",
    "        # Get scaled versions of the data\n",
    "        X_train_scaled = feature_scaler.transform(X_train)\n",
    "        y_train_scaled = target_scaler.transform(y_train)\n",
    "        X_test_scaled = feature_scaler.transform(X_test)\n",
    "        y_test_scaled = target_scaler.transform(y_test)\n",
    "    else:\n",
    "        feature_scaler = None\n",
    "        target_scaler = None\n",
    "        X_train_scaled = X_train\n",
    "        y_train_scaled = y_train\n",
    "        X_test_scaled = X_test\n",
    "        y_test_scaled = y_test\n",
    "        \n",
    "    # Turn all the numpy arrays to torch tensors\n",
    "    X_train_scaled = torch.Tensor(X_train_scaled)\n",
    "    X_test_scaled = torch.Tensor(X_test_scaled)\n",
    "    y_train_scaled = torch.Tensor(y_train_scaled)\n",
    "    y_test_scaled = torch.Tensor(y_test_scaled)\n",
    "    \n",
    "    # This is pytorch magick to enable shuffling of the\n",
    "    # training data every time we go through them\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train_scaled, y_train_scaled)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                    batch_size=n_batch,\n",
    "                                                    shuffle=True)\n",
    "    \n",
    "    # Create an Adam optimizing object for the neural network `net`\n",
    "    # with learning rate `lr`\n",
    "    raise NotImplementedError('Define the optimizer object! Delete me then!')\n",
    "    optimizer = # your code here\n",
    "    \n",
    "    # This is a place to keep track of the test loss\n",
    "    test_loss = []\n",
    "    \n",
    "    # Iterate the optimizer. \n",
    "    # Remember, each time we go through the entire dataset we complete an `epoch`\n",
    "    # I have wrapped the range around tqdm to give you a nice progress bar\n",
    "    # to look at\n",
    "    for e in tqdm(range(epochs)):\n",
    "        # This loop goes over all the shuffled training data\n",
    "        # That's why the DataLoader class of PyTorch is convenient\n",
    "        for X_batch, y_batch in train_data_loader:\n",
    "            # Perform a single optimization step with loss function\n",
    "            # loss_func(y_batch, y_pred, reg_weight, net.parameters())\n",
    "            # Hint 1: You have defined loss_func() already\n",
    "            # Hint 2: Consult the hands-on activities for an example\n",
    "            # Your code here\n",
    "            raise NotImplementedError('Write stochastic gradient step code! Delete me then!')\n",
    "            \n",
    "        # Evaluate the test loss and append it on the list `test_loss`\n",
    "        y_pred_test = net(X_test_scaled)\n",
    "        ts_loss = mse_loss(y_test_scaled, y_pred_test)\n",
    "        test_loss.append(ts_loss.item())\n",
    "        \n",
    "    # Make a TrainedModel\n",
    "    trained_model = TrainedModel(net, standarized=standarize,\n",
    "                                 feature_scaler=feature_scaler,\n",
    "                                 target_scaler=target_scaler)\n",
    "    \n",
    "    # Make sure that we return properly scaled \n",
    "    \n",
    "    # Return everything we need to analyze the results\n",
    "    return trained_model, test_loss, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this to test your code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple one-layer network with 10 neurons\n",
    "net = nn.Sequential(nn.Linear(5, 20),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.Linear(20, 1))\n",
    "epochs = 1000\n",
    "lr = 0.01\n",
    "reg_weight = 0\n",
    "n_batch = 100\n",
    "model, test_loss, X_train, y_train, X_test, y_test = train_net(\n",
    "    X,\n",
    "    y,\n",
    "    net,\n",
    "    reg_weight,\n",
    "    n_batch,\n",
    "    epochs,\n",
    "    lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few more things for you to do here.\n",
    "First, plot the evolution of the test loss as a function of the number of epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the observations vs predictions plot for the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And do the observations vs predictions plot for the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C.I - Investigate the effect of the batch size\n",
    "\n",
    "For the given network, try batch sizes of 10, 25, 50 and 100 for 400 epochs.\n",
    "In the sample plot, show the evolution of the test loss function for each case.\n",
    "Which batch sizes lead to faster training times and why?\n",
    "Which one would you choose?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = # pick me\n",
    "lr = # pick me\n",
    "reg_weight = # pick me\n",
    "test_losses = []\n",
    "models = []\n",
    "batches = # make me a list with the right batch sizes\n",
    "for n_batch in batches:\n",
    "    print('Training n_batch: {0:d}'.format(n_batch))\n",
    "    net = nn.Sequential(nn.Linear(5, 20),\n",
    "                    nn.Sigmoid(),\n",
    "                    nn.Linear(20, 1))\n",
    "    model, test_loss, X_train, y_train, X_test, y_test = train_net(\n",
    "        X,\n",
    "        y,\n",
    "        net,\n",
    "        reg_weight,\n",
    "        n_batch,\n",
    "        epochs,\n",
    "        lr\n",
    "    )\n",
    "    test_losses.append(test_loss)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "for tl, n_batch in zip(test_losses, batches):\n",
    "    ax.plot(tl, label='n_batch={0:d}'.format(n_batch))\n",
    "ax.set_xlabel('Number of epochs')\n",
    "ax.set_ylabel('Test loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Write your observsations about the batch size here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C.II - Investigate the effect of the learning rate\n",
    "\n",
    "Fix the batch size to best one you identified in Part C.I.\n",
    "For the given network, try learning rates of 1, 0.1, 0.01 and 0.001 for 400 epochs.\n",
    "In the sample plot, show the evolution of the test loss function for each case.\n",
    "Does the algorithm converge for all learning rates?\n",
    "Which learning rate would you choose?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part C.III - Investigate the effect of the regularization weight\n",
    "\n",
    "Fix the batch size to the value you selected in C.I and the learning rate to the value you selected in C.II.\n",
    "For the given network, try regularization weights of 0, 1e-16, 1e-12, 1e-6, and 1e-3 for 400 epochs.\n",
    "In the sample plot, show the evolution of the test loss function for each case.\n",
    "Which regularization weight seems to be the best and why?\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D.I - Train a bigger network\n",
    "\n",
    "Now that you have developed some intuition about the parameters involved in training a network, train a larger one.\n",
    "In particular, use a 5-layer deep network with 100 neurons per layer.\n",
    "You can use the sigmoid activation function or you can change it to something else.\n",
    "Make sure you plot:\n",
    "- the evolution of the test loss a a function of the epochs\n",
    "- the observations vs predictions plot for the test data\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part D.II - Make a prediction\n",
    "\n",
    "Visualize the scaled sound level as a function of the streem velocity for a fixed frequency of 2500 Hz, a chord lentgh of 0.1 m, a sucction side displacement thickness of 0.01 m, and an angle of attack of 0, 5, and 10 degrees.\n",
    "\n",
    "**Answer:**\n",
    "\n",
    "This is just a sanity check for your model.\n",
    "You will just have to run the following code segmenets for the best model you have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = # set this equal to your best model\n",
    "\n",
    "def plot_sound_level_as_func_of_stream_vel(\n",
    "    freq=2500,\n",
    "    angle_of_attack=10,\n",
    "    chord_length=0.1,\n",
    "    suc_side_disp_thick=0.01,\n",
    "    ax=None,\n",
    "    label=None\n",
    "):\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(dpi=100)\n",
    "    \n",
    "    # The velocities on which we want to evaluate the model\n",
    "    vel = np.linspace(X[:, 3].min(), X[:, 3].max(), 100)[:, None]\n",
    "    \n",
    "    # Make the input for the model\n",
    "    freqs = freq * np.ones(vel.shape)\n",
    "    angles = angle_of_attack * np.ones(vel.shape)\n",
    "    chords = chord_length * np.ones(vel.shape)\n",
    "    sucs = suc_side_disp_thick * np.ones(vel.shape)\n",
    "    \n",
    "    # Put all these into a single array\n",
    "    XX = np.hstack([freqs, angles, chords, vel, sucs])\n",
    "    \n",
    "    ax.plot(vel, best_model(XX), label=label)\n",
    "    \n",
    "    ax.set_xlabel('Velocity (m/s)')\n",
    "    ax.set_ylabel('Scaled sound pressure level (decibels)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "for aofa in [0, 5, 10]:\n",
    "    plot_sound_level_as_func_of_stream_vel(\n",
    "        angle_of_attack=aofa,\n",
    "        ax=ax, \n",
    "        label='Angle of attack={0:1.2f}'.format(aofa)\n",
    ")\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - Classification with DNNs\n",
    "\n",
    "This homework problem was kindly provided by Dr. Ali Lenjani.\n",
    "It is based on our joint work on this paper:\n",
    "[Hierarchical convolutional neural networks information fusion for activity source detection in smart buildings](http://www.dpi-proceedings.com/index.php/shm2019/article/view/32353).\n",
    "The data come from the [Human Activity Benchmark](http://www.ce.sc.edu/#/caicedo/custom?title=Human%20Activity%20Benchmark) published by Dr. Juan M. Caicedo.\n",
    "\n",
    "So the problem is as follows.\n",
    "You want to put sensors on a building so that it can figure out what is going on insider it.\n",
    "This has applications in industrial facilities (e.g., detecting if there was an accident), public infrastructure, hospitals (e.g., did a patient fall off a bed), etc.\n",
    "Typically, the problem is addressed using cameras.\n",
    "Instead of cameras, we are going to investigate the ability of acceleration sensors to tell us what is going on.\n",
    "\n",
    "Four acceleration sensors have been placed in different locations in the benchmark building to record the floor vibration signals of different objects falling from several heights.\n",
    "A total of seven cases cases were considered:\n",
    "\n",
    "- **bag-high:** 450 g bag containing plastic pieces is dropped roughly from 2.10 m \n",
    "- **bag-low:** 450 g bag containing plastic pieces is dropped roughly from 1.45 m\n",
    "- **ball-high:** 560 g basketball is dropped roughly from 2.10 m\n",
    "- **ball-low:** 560 g basketball is dropped roughly from 1.45 m\n",
    "- **j-jump:** person 1.60 m tall, 55 kg jumps approximately 12 cm high\n",
    "- **d-jump:** person 1.77 m tall, 80 kg jumps approximately 12 cm high\n",
    "- **w-jump:** person 1.85 m tall, 85 kg jumps approximately 12 cm high\n",
    "\n",
    "Each of these seven cases was repeated 115 times at 5 different locations of the building.\n",
    "The original data are [here](http://www.ce.sc.edu/#/caicedo/custom?title=Human%20Activity%20Benchmark), but I have repackaged them for you in a more convenient format.\n",
    "Let's download them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O 'https://dl.dropboxusercontent.com/s/n8dczk7t8bx0pxi/human_activity_data.npz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('human_activity_data.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a Python dictionary that contains the following entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in data.keys():\n",
    "    print(key, ':', data[key].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go over these one by one. First, the `features`.\n",
    "These are the accelertion sensor measurements.\n",
    "Here is how you visualize them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, 1, dpi=100)\n",
    "# Loop over sensors\n",
    "for j in range(4):\n",
    "        ax[j].plot(data['features'][0, j])\n",
    "ax[-1].set_xlabel('Timestep')\n",
    "ax[-1].set_ylabel('Acceleration');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second key, `labels_1`, is a bunch of integers ranging from 0 to 2 indicating whether the entry corresponds to a \"bag,\" a \"ball\" or a \"jump.\"\n",
    "For your reference, the correspondence is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_1_TO_TEXT = {\n",
    "    0: 'bag',\n",
    "    1: 'ball',\n",
    "    2: 'jump'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here are a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    i = np.random.randint(0, data['features'].shape[0])\n",
    "    fig, ax = plt.subplots(4, 1, dpi=100)\n",
    "    for j in range(4):\n",
    "        ax[j].plot(data['features'][i, j])\n",
    "    ax[-1].set_xlabel('Timestep')\n",
    "    ax[-1].set_ylabel('Acceleration')\n",
    "    ax[0].set_title('Label: {0:d} -> {1:s}'.format(data['labels_1'][i], \n",
    "                                                   LABELS_1_TO_TEXT[data['labels_1'][i]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The array `labels_2` includes integers from 0 to 6 indicating the detailed label of the experiment. The correspondence between integers and text labels is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_2_TO_TEXT = {\n",
    "    0: 'bag-high',\n",
    "    1: 'bag-low',\n",
    "    2: 'ball-high',\n",
    "    3: 'ball-low',\n",
    "    4: 'd-jump',\n",
    "    5: 'j-jump',\n",
    "    6: 'w-jump'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the field `loc_ids` takes values from 0 to 4 indicating five distinct locations in the building.\n",
    "\n",
    "Before moving forward with the questions, let's extract the data in a more covenient form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The features\n",
    "X = data['features']\n",
    "# The labels_1\n",
    "y1 = data['labels_1']\n",
    "# The labels_2\n",
    "y2 = data['labels_2']\n",
    "# The locations\n",
    "y3 = data['loc_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A - Train a CNN to predict the the high-level type of observation (bag, ball, or jump)\n",
    "\n",
    "Fill in the blanks in the code blocks below to train a classification neural network that is going to take you from the four acceleration sensor data to the high-level type of each observation.\n",
    "You can keep the structure of the network fixed, but you can experiment with the learning rate, the number of epochs, or anything else.\n",
    "Just keep in mind that for this particular dataset it is possible to hit an accuracy of almost 100%.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing that we need to do is pick a neural network structure.\n",
    "I suggest that we use 1D convolutional layers at the very beginning.\n",
    "These are the same as the 2D (image) convolutional layers, but in 1D.\n",
    "The reason I am proposing this is mainly that the convolutional layers are invariant to small translations of the acceleration signal (just like the labels are).\n",
    "Here is what I propose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_labels=3):\n",
    "        super(Net, self).__init__()\n",
    "        # A convolutional layer:\n",
    "        # 3 = input channels (sensors),\n",
    "        # 6 = output channels (features),\n",
    "        # 5 = kernel size\n",
    "        self.conv1 = nn.Conv1d(4, 8, 10)\n",
    "        # A 2 x 2 max pooling layer - we are going to use it two times\n",
    "        self.pool = nn.MaxPool1d(5)\n",
    "        # Another convolutional layer\n",
    "        self.conv2 = nn.Conv1d(8, 16, 5)\n",
    "        # Some linear layers\n",
    "        self.fc1 = nn.Linear(16 * 131, 200)\n",
    "        self.fc2 = nn.Linear(200, 50)\n",
    "        self.fc3 = nn.Linear(50, num_labels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # This function implements your network output\n",
    "        # Convolutional layer, followed by relu, followed by max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Same thing\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatting the output of the convolutional layers\n",
    "        x = x.view(-1, 16 * 131)\n",
    "        # Go throught the first dense linear layer followed by relu\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Through the second dense layer\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Finish up with a linear transformation\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can make the network like this:\n",
    "net = Net(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you need to pick the right loss function for classification tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_loss_func = # your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, let's organize our training code in a convenient function that allows us to play with the parameters of training.\n",
    "Fill in the missing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cnn(X, y, net, n_batch, epochs, lr, test_size=0.33):\n",
    "    \"\"\"\n",
    "    A function that trains a regression neural network using stochatic gradient\n",
    "    descent and returns the trained network. The loss function being minimized is\n",
    "    `loss_func`.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    X          -    The observed features\n",
    "    y          -    The observed targets\n",
    "    net        -    The network you want to fit\n",
    "    n_batch    -    The batch size you want to use for stochastic optimization\n",
    "    epochs     -    How many times do you want to pass over the training dataset.\n",
    "    lr         -    The learning rate for the stochastic optimization algorithm.\n",
    "    test_size  -    What percentage of the data should be used for testing (validation).\n",
    "    \"\"\"\n",
    "    # Split the data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "        \n",
    "    # Turn all the numpy arrays to torch tensors\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    X_test = torch.Tensor(X_test)\n",
    "    y_train = torch.LongTensor(y_train)\n",
    "    y_test = torch.LongTensor(y_test)\n",
    "    \n",
    "    # This is pytorch magick to enable shuffling of the\n",
    "    # training data every time we go through them\n",
    "    train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                                    batch_size=n_batch,\n",
    "                                                    shuffle=True)\n",
    "    \n",
    "    # Create an Adam optimizing object for the neural network `net`\n",
    "    # with learning rate `lr`\n",
    "    raise NotImplementedError('Define the optimizer object! Delete me then!')\n",
    "    optimizer = # your code here\n",
    "    \n",
    "    # This is a place to keep track of the test loss\n",
    "    test_loss = []\n",
    "    # This is a place to keep track of the accuracy on each epoch\n",
    "    accuracy = []\n",
    "    \n",
    "    # Iterate the optimizer. \n",
    "    # Remember, each time we go through the entire dataset we complete an `epoch`\n",
    "    # I have wrapped the range around tqdm to give you a nice progress bar\n",
    "    # to look at\n",
    "    for e in range(epochs):\n",
    "        # This loop goes over all the shuffled training data\n",
    "        # That's why the DataLoader class of PyTorch is convenient\n",
    "        for X_batch, y_batch in train_data_loader:\n",
    "            # Perform a single optimization step with loss function\n",
    "            # cnn_loss_func(y_batch, y_pred, reg_weight)\n",
    "            # Hint 1: You have defined cnn_loss_func() already\n",
    "            # Hint 2: Consult the hands-on activities for an example\n",
    "            # your code here\n",
    "            raise NotImplementedError('Write stochastic gradient step code! Delete me then!')\n",
    "            \n",
    "        # Evaluate the test loss and append it on the list `test_loss`\n",
    "        y_pred_test = net(X_test)\n",
    "        ts_loss = cnn_loss_func(y_pred_test, y_test)\n",
    "        test_loss.append(ts_loss.item())\n",
    "        # Evaluate the accuracy\n",
    "        _, predicted = torch.max(y_pred_test.data, 1)\n",
    "        correct = (predicted == y_test).sum().item()\n",
    "        accuracy.append(correct / y_test.shape[0])\n",
    "        # Print something about the accuracy\n",
    "        print('Epoch {0:d}: accuracy = {1:1.5f}%'.format(e+1, accuracy[-1]))\n",
    "    trained_model = net\n",
    "    \n",
    "    # Return everything we need to analyze the results\n",
    "    return trained_model, test_loss, accuracy, X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now experiment with the epochs, the learning rate, and the batch size until this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.01\n",
    "n_batch = 100\n",
    "trained_model, test_loss, accuracy, X_train, y_train, X_test, y_test = train_cnn(X, y1, net, n_batch, epochs, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of the test loss as a function of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(test_loss)\n",
    "ax.set_xlabel('Number of epochs')\n",
    "ax.set_ylabel('Test loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the evolution of the accuracy as a function of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(accuracy)\n",
    "ax.set_xlabel('Number of epochs')\n",
    "ax.set_ylabel('Accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Predict on the test data\n",
    "y_pred_test = trained_model(X_test)\n",
    "# Remember that the prediction is probabilistic\n",
    "# We need to simply pick the label with the highest probability:\n",
    "_, y_pred_labels = torch.max(y_pred_test, 1)\n",
    "# Here is the confusion matrix:\n",
    "cf_matrix = confusion_matrix(y_test, y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), annot=True, \n",
    "            fmt='.2%', cmap='Blues', \n",
    "            xticklabels=LABELS_1_TO_TEXT.values(),\n",
    "            yticklabels=LABELS_1_TO_TEXT.values());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B - Train a CNN to predict the the low-level type of observation (bag-high, bag-low, etc.)\n",
    "\n",
    "Repeat what you did above for `y2`.\n",
    "\n",
    "**Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "colab": {
   "collapsed_sections": [],
   "name": "hw_04.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
