{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 14.2: Maximum a posteriori estimate - Avoiding overfitting\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To demonstrate how regularization arises naturally from a Bayesian perspective and how it can be used to avoid overfitting.\n",
    "\n",
    "Let's load the motorcyle data to play with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = -1.0 + 2 * np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 2.0\n",
    "w2_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + w2_true * x ** 2 + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also copy-paste the code for creating design matrices for the three generalized linear models we have considered so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)\n",
    "\n",
    "def get_fourier_design_matrix(x, L, num_terms):\n",
    "    \"\"\"\n",
    "    Fourier expansion with ``num_terms`` cosines and sines.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(1, int(num_terms / 2)):\n",
    "        cols.append(np.cos(2 * i * np.pi / L * x))\n",
    "        cols.append(np.sin(2 * i * np.pi / L * x))\n",
    "    return np.hstack(cols)\n",
    "                    \n",
    "def get_rbf_design_matrix(x, x_centers, ell):\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(x_centers.shape[0]):\n",
    "        cols.append(np.exp(-(x - x_centers[i]) ** 2 / ell))\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the video lecture, the problem that we need to solve to find the maximum a posteriori estimate is when the prior of the weights is a zero mean Gaussian with variance $\\alpha$ is:\n",
    "$$\n",
    "\\left(\\sigma^{-2}\\boldsymbol{\\Phi}^T\\boldsymbol{\\Phi} + \\alpha\\mathbf{I}\\right)\\mathbf{w} = \\sigma^{-2}\\boldsymbol{\\Phi}^T\\mathbf{y}.\n",
    "$$\n",
    "This problem cannot be solved by ``numpy.linalg.lstsq``.\n",
    "The stable way to solve this problem is this:\n",
    "\n",
    "+ Construct the positive-definite matrix:\n",
    "$$\n",
    "\\mathbf{A} = \\left(\\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\alpha\\mathbf{I}\\right)\n",
    "$$\n",
    "+ Compute the [Cholesky decomposition](https://en.wikipedia.org/wiki/Cholesky_decomposition) of $\\mathbf{A}$:\n",
    "$$\n",
    "\\mathbf{A} = \\mathbf{L}\\mathbf{L}^T,\n",
    "$$\n",
    "where $\\mathbf{L}$ is lower triangular.\n",
    "+ Then, solve the system:\n",
    "$$\n",
    "\\mathbf{L}\\mathbf{L}^T\\mathbf{w} = \\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{y}_{1:n},\n",
    "$$\n",
    "doing a forward and a backward substitution. The methods [scipy.linalg.cho_factor](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_factor.html#scipy.linalg.cho_factor) and [scipy.linalg.cho_solve](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.linalg.cho_solve.html) can be used for this. We implement this process below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need this to take the Cholesky decomposition\n",
    "import scipy\n",
    "\n",
    "def find_w_map(Phi, y, sigma2, alpha):\n",
    "    \"\"\"\n",
    "    Return the MAP weights of a Bayesian linear regression problem with\n",
    "    design matrix ``Phi`` observed targets ``y``, noise variance ``sigma2``\n",
    "    and priors for the weights ``alpha``.\n",
    "    \"\"\"\n",
    "    A = np.dot(Phi.T, Phi) / sigma2 + alpha * np.eye(Phi.shape[1])\n",
    "    L = scipy.linalg.cho_factor(A)\n",
    "    w = scipy.linalg.cho_solve(L, np.dot(Phi.T, y / sigma2))\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select polynomial degree and get design matrix\n",
    "degree = 6\n",
    "Phi = get_polynomial_design_matrix(x[:, None], degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick variance (here I am using the true one)\n",
    "sigma2 = 0.1 ** 2\n",
    "# Pick the regularization parameter:\n",
    "alpha = 100.0\n",
    "# Solve for the MAP of the weights:\n",
    "w = find_w_map(Phi, y, sigma2, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# The mean prediction of the model we just fitted\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "yy = np.dot(Phi_xx, w)\n",
    "# plot mean prediction\n",
    "ax.plot(xx, yy, '--', label='Mean prediction')\n",
    "# plot shaded area for 95% credible interval\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_title(r'$\\rho={0:d}, \\sigma = {1:1.2f}, \\alpha={2:1.2e}$'.format(degree, np.sqrt(sigma2), alpha))\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Experiment with different $\\alpha$'s. Notice that for very small $\\alpha$'s we are overfitting. Notice that with very large $\\alpha$'s, we are underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting $\\alpha$ through a validation set\n",
    "\n",
    "Let's plot the mean square error of a validation dataset as a function of $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The validation dataset\n",
    "x_valid = -1.0 + 2 * np.random.rand(20)\n",
    "y_valid = w0_true + w1_true * x_valid + w2_true * x_valid ** 2 + sigma_true * np.random.randn(x_valid.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start increasing the number of Fourier terms and check the mean square error\n",
    "# Fit the Fourier model\n",
    "MSE = []\n",
    "alphas = np.linspace(0.01, 20.0, 20)\n",
    "for alpha in alphas:\n",
    "    w = find_w_map(Phi, y, sigma2, alpha)    # Make predictions on the validation data\n",
    "    Phi_valid = get_polynomial_design_matrix(x_valid[:, None], degree)\n",
    "    y_valid_predict = np.dot(Phi_valid, w)\n",
    "    # Calculate the mean square error\n",
    "    MSE_alpha = np.linalg.norm(y_valid_predict - y_valid)\n",
    "    MSE.append(MSE_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(alphas, MSE)\n",
    "ax.set_xlabel(r'$\\alpha$')\n",
    "ax.set_ylabel('Mean square error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick the regularization parameter:\n",
    "alpha = 7.5\n",
    "# Solve for the MAP of the weights:\n",
    "w = find_w_map(Phi, y, sigma2, alpha)\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# The mean prediction of the model we just fitted\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "yy = np.dot(Phi_xx, w)\n",
    "# plot mean prediction\n",
    "ax.plot(xx, yy, '--', label='Mean prediction')\n",
    "# plot shaded area for 95% credible interval\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "ax.set_title(r'$\\rho={0:d}, \\sigma = {1:1.2f}, \\alpha={2:1.2e}$'.format(degree, np.sqrt(sigma2), alpha))\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "+ Rerun the code cell above with an $\\alpha$ that is smaller than the optimal one. Observe how we overfit.\n",
    "+ Rerun the code cell above with an $\\alpha$ that is greater than the optimal one. Observe how we underfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
