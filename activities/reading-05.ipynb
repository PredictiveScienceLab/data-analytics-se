{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 5: Collections of Random Variables\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce the concept of the joint probability density function.\n",
    "+ To introduce the concept of marginalization.\n",
    "+ To introduce the conditional probability density function.\n",
    "+ To introduce the expectation of continuous random variables.\n",
    "\n",
    "## References\n",
    "\n",
    "+ None\n",
    "\n",
    "## Joint probability mass function\n",
    "\n",
    "Consider two random variables $X$ and $Y$.\n",
    "The *joint probability mass function* of the pair $(X,Y)$ is the function $f_{X,Y}(x,y)$ giving the probability that $X=x$ and $Y=y$.\n",
    "Mathematically (and introducing a simplified notation), we have:\n",
    "$$\n",
    "p(x,y) \\equiv p(X=x, Y=y) \\equiv f_{X,Y}(x,y) := \\mathbb{P}\\left(\\{\\omega: X(\\omega) = x, Y(\\omega)=y\\}\\right).\n",
    "$$\n",
    "\n",
    "### Properties of the joint probability mass function\n",
    "+ It is nonnegative:\n",
    "$$\n",
    "p(x,y) \\ge 0.\n",
    "$$\n",
    "+ If you sum over all the possible values of all random variables, you should get one:\n",
    "$$\n",
    "\\sum_x \\sum_y p(x,y) = 1.\n",
    "$$\n",
    "+ If you *marginalize* over the values of one of the random variables you get the pmf of the other.\n",
    "For example:\n",
    "$$\n",
    "p(x) = \\sum_y p(x,y),\n",
    "$$\n",
    "and \n",
    "$$\n",
    "p(y) = \\sum_x p(x, y).\n",
    "$$\n",
    "\n",
    "\n",
    "### Joint probability mass function of many random variables\n",
    "Take $N$ random variables $X_1,\\dots,X_N$.\n",
    "We can define their joint probability mass function in the same way we did it for two:\n",
    "$$\n",
    "p(x_1,\\dots,x_N) \\equiv p(X_1=x_1,\\dots,X_N=x_N) \\equiv f_{X_1,\\dots,X_N}(x_1,\\dots,X_N) := \\mathbb{P}\\left(\\{\\omega: X_1(\\omega)=x_1,\\dots,X_N(\\omega)=x_N\\}\\right).\n",
    "$$\n",
    "Just like before, we can marginalize over any subset of random variables to get the pmf of the remaining ones.\n",
    "For example:\n",
    "$$\n",
    "p(x_i) = \\sum_{x_j,j\\not=i} p(x_1,\\dots,x_N).\n",
    "$$\n",
    "\n",
    "## Joint probability density function\n",
    "\n",
    "Let $X$ and $Y$ be two random variables.\n",
    "There joint probability density $f_{X,Y}(x,y)$ is the function that can give us the probability that the pair $(X,Y)$ belongs to any \"good\" subset $A$ of $\\mathbb{R}^2$ as follows:\n",
    "$$\n",
    "p\\left((X,Y)\\in A\\right) = \\int\\int_{A} f_{X,Y}(x,y)dxdy.\n",
    "$$\n",
    "Of course, we will be writing:\n",
    "$$\n",
    "p(x,y) := f_{X,Y}(x,y),\n",
    "$$\n",
    "when there is no ambiguity.\n",
    "\n",
    "If you integrate one of the variables out of the joint, you get the PDF of the other variable.\n",
    "For example:\n",
    "$$\n",
    "p(x) = \\int_{-\\infty}^\\infty p(x,y) dy,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(y) = \\int_{-\\infty}^\\infty p(x, y) dx.\n",
    "$$\n",
    "\n",
    "\n",
    "## Conditioning a random variable on another\n",
    "\n",
    "Consider two random variables $X$ and $Y$.\n",
    "If we had observed that $Y=y$, how would this change the PDF of $X$?\n",
    "The answer is given via Bayes' rule.\n",
    "The PDF of $X$ conditioned on $Y=y$ is:\n",
    "$$\n",
    "p(x|y) = \\frac{p(x,y)}{p(y)}.\n",
    "$$\n",
    "\n",
    "## The covariance operator\n",
    "\n",
    "The covariance operator measures how correlated two random variables $X$ and $Y$ are.\n",
    "Its definition is:\n",
    "$$\n",
    "\\mathbb{C}[X,Y] = \\mathbb{E}\\left[\\left(X-\\mathbb{E}[X]\\right)\\left(Y-\\mathbb{E}[Y]\\right)\\right].\n",
    "$$\n",
    "If $\\mathbb{C}[X,Y]$ is positive, then we say that the two random variables are correlated.\n",
    "If it is negative, then we say that the two random variables are anti-correlated.\n",
    "If it is zero, then we say that the two random variables are not correlated.\n",
    "We will talk more about this in a later lecture.\n",
    "\n",
    "A usefull property of the covariance operator is that it can give tell you something about the variance of the sum of two random variables.\n",
    "It is:\n",
    "$$\n",
    "\\mathbb{V}[X + Y] = \\mathbb{V}[X] + \\mathbb{V}[Y] + 2\\mathbb{C}[X,Y].\n",
    "$$\n",
    "\n",
    "## Independent random variables\n",
    "\n",
    "Take two random variables $X$ and $Y$.\n",
    "We say that the two random variables are independent given the background information $I$, and we write:\n",
    "$$\n",
    "X\\perp Y | I,\n",
    "$$\n",
    "if and only if conditioning on one does not tell you anything about the other, i.e.,\n",
    "$$\n",
    "p(x|y, I) = p(x|I).\n",
    "$$\n",
    "It is easy to show using Bayes' rule that the definition is consistent, i.e., you also get:\n",
    "$$\n",
    "p(y|x, I) = p(y|I).\n",
    "$$\n",
    "When there is no ambiguity, we can drop $I$.\n",
    "\n",
    "## Independent random variables\n",
    "\n",
    "Take two random variables $X$ and $Y$.\n",
    "We say that the two random variables are independent given the background information $I$, and we write:\n",
    "$$\n",
    "X\\perp Y | I,\n",
    "$$\n",
    "if and only if conditioning on one does not tell you anything about the other, i.e.,\n",
    "$$\n",
    "p(x|y, I) = p(x|I).\n",
    "$$\n",
    "It is easy to show using Bayes' rule that the definition is consistent, i.e., you also get:\n",
    "$$\n",
    "p(y|x, I) = p(y|I).\n",
    "$$\n",
    "When there is no ambiguity, we can drop $I$.\n",
    "\n",
    "### Properties of independent random variables\n",
    "+ The joint pmf factorizes:\n",
    "$$\n",
    "p(x,y) = p(x)p(y).\n",
    "$$\n",
    "+ The expectation of the product is the product of the expectation:\n",
    "$$\n",
    "\\mathbb{E}[XY] = \\mathbb{E}[X]\\cdot \\mathbb{E}[Y].\n",
    "$$\n",
    "+ The covariance of two independent random variables is zero:\n",
    "$$\n",
    "\\mathbb{C}[X,Y] = 0.\n",
    "$$\n",
    "Be careful **the reverse is not true!**\n",
    "+ A consequence of the above property is that the variance of the sum of two independent random variables is the sum of the variables:\n",
    "$$\n",
    "\\mathbb{V}[X+Y] = \\mathbb{V}[X] + \\mathbb{V}[Y].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
