{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 12.3: Decision-Making\n",
    "\n",
    "## Objectives\n",
    "+ To introduce the decision-making problem\n",
    "\n",
    "## Bayesian Decision Making\n",
    "\n",
    "So, we learned about credible intervals.\n",
    "But what if someone asks you to report a single value for $\\theta$ in the coin toss example?\n",
    "What is the correct way of doing this?\n",
    "\n",
    "This is actually a decision-making problem.\n",
    "To answer it, you have to quantify the cost of making a mistake and then make a decision that minimizes this cost.\n",
    "To formalize this concept, assume that we have a random variable $X$ and we have to guess a value for it.\n",
    "Let $\\ell(\\theta', \\theta)$ be the loss we incur when we guess $\\theta'$ and the true value is $\\theta$.\n",
    "This is a completly subjective quantity.\n",
    "However, here are some ideas:\n",
    "+ The 0-1 loss:\n",
    "$$\n",
    "\\ell_{01}(\\theta',\\theta) = \\begin{cases}\n",
    "1,&\\;\\text{if}\\;\\theta'=\\theta\\\\\n",
    "0,&\\;\\text{if}\\;\\theta'\\not=\\theta.\n",
    "\\end{cases}\n",
    "= 1_{\\{\\theta'\\}}(\\theta).\n",
    "$$\n",
    "+ The square loss:\n",
    "$$\n",
    "\\ell_2(\\theta',\\theta) = (\\theta'-\\theta)^2.\n",
    "$$\n",
    "+ The absolute loss:\n",
    "$$\n",
    "\\ell_1(\\theta',\\theta) = |\\theta'-\\theta|.\n",
    "$$\n",
    "\n",
    "The rational thing to do, when choosing a value for $\\theta$ is to minimize our *expected loss* where the expectation is taken over our posterior state of knowledge about $\\theta$.\n",
    "That is, we make our choice by solving this problem:\n",
    "$$\n",
    "\\theta^* = \\min_{\\theta'} \\mathbb{E}[\\ell(\\theta',\\theta)|x_{1:N}] = \\min_{\\theta'} \\int \\ell(\\theta',\\theta)p(\\theta|x_{1:N})d\\theta.\n",
    "$$\n",
    "This, in general, is not a problem with an analytical solution.\n",
    "However, for the two special loss functions above the answer is:\n",
    "+ The choice that minimizes the 0-1 loss is the one maximizing the posterior:\n",
    "$$\n",
    "\\theta^*_{01} = \\arg\\max_{\\theta} p(\\theta|x_{1:N}).\n",
    "$$\n",
    "+ The choice that minimizes the square loss is the expectation of the random variable:\n",
    "$$\n",
    "\\theta^*_2 = \\mathbb{E}[\\theta|x_{1:N}] = \\int \\theta p(\\theta|x_{1:N})d\\theta.\n",
    "$$\n",
    "+ The choice that minimizes the absolute loss is the median:\n",
    "$$\n",
    "p(\\theta \\le \\theta^*_1 | x_{1:N}) = 0.5.\n",
    "$$\n",
    "\n",
    "Let's reintroduce our coin toss example so that w have someething to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "# Take a fake coin which is a little bit biased\n",
    "theta_true = 0.8\n",
    "# This is the random variable corresponding to a coin toss\n",
    "X = st.bernoulli(theta_true)\n",
    "\n",
    "# Sample from it a number of times to generate our data = (x1, ..., xN)\n",
    "N = 5\n",
    "data = X.rvs(size=N)\n",
    "# Now we are ready to calculate the posterior which the Beta we have above\n",
    "alpha = 1.0 + data.sum()\n",
    "beta = 1.0 + N - data.sum()\n",
    "Theta_post = st.beta(alpha, beta)\n",
    "# Now we can plot the posterior PDF for theta\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "thetas = np.linspace(0, 1, 100)\n",
    "ax.plot([theta_true], [0.0], 'o', markeredgewidth=2, markersize=10, label='True value')\n",
    "ax.plot(thetas, Theta_post.pdf(thetas), label=r'$p(\\theta|x_{1:N})$')\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_ylabel('Probability density')\n",
    "ax.set_title('$N={0:d}$'.format(N))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the 0-1 loss, we just need to find the maximum of the posterior.\n",
    "This is an optimization probem with an analytical solution, but we are not going to do it this way.\n",
    "We will just solve the problem using with a grid search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.argmax(Theta_post.pdf(thetas))\n",
    "theta_star_01 = thetas[idx]\n",
    "print('theta_star_01 = {0:1.2f}'.format(theta_star_01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's the theta $\\theta$ that minimizes the square loss.\n",
    "We just have to find the expectation of the posterior $p(\\theta|x_{1:N})$ (which is just a Beta).\n",
    "It is:\n",
    "$$\n",
    "\\theta^*_N = \\mathbb{E}[\\theta|x_{1:N}] = \\frac{1+\\sum_{n=1}^Nx_n}{1+\\sum_{n=1}^Nx_n + N + 1 - \\sum_{n=1}^Nx_n}\n",
    "= \\frac{1 + \\sum_{n=1}^Nx_n}{N+2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the example we had above:\n",
    "theta_star_2 = Theta_post.expect()\n",
    "print('theta_star_2 = {0:1.2f}'.format(theta_star_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, here is the median which minimizes the absolute loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the example we had above:\n",
    "theta_star_1 = Theta_post.median()\n",
    "print('theta_star_1 = {0:1.2f}'.format(theta_star_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See them all together in the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot([theta_true], [0.0], 'o', markeredgewidth=2, markersize=10, label='True value')\n",
    "ax.plot(thetas, Theta_post.pdf(thetas), label=r'$p(\\theta|x_{1:N})$')\n",
    "ax.plot(theta_star_01, 0, 'x', markeredgewidth=2, label=r'$\\theta^*_{01}$')\n",
    "ax.plot(theta_star_2, 0, 's', markeredgewidth=2, label=r'$\\theta^*_{2}$')\n",
    "ax.plot(theta_star_1, 0, 'd', markeredgewidth=2, label=r'$\\theta^*_{1}$')\n",
    "ax.set_xlabel(r'$\\theta$')\n",
    "ax.set_title('$N={0:d}$'.format(N))\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "+ Repeat the analysis for $N=0, 5, 10, 100$. Do these estimates converge to the true value?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
