{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 24 - Deep Neural Networks\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Understand the basics of `PyTorch`.\n",
    "+ Set up and train regression DNNs with `PyTorch`\n",
    "\n",
    "## References \n",
    "\n",
    "+ Reading Activity 24\n",
    "+ Chapters 6, 7, and 8 of https://www.deeplearningbook.org/\n",
    "+ [Deep Learning with PyTorch: A 60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) and in particular:\n",
    "    - [What is PyTorch?](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#sphx-glr-beginner-blitz-tensor-tutorial-py)\n",
    "    - [Autograd: Automatic differentation](https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py)\n",
    "    - [Neural networks](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is PyTorch and why are we using it?\n",
    "\n",
    "+ PyTorch is an alternative to Numpy that can harness the power of [GPUs](https://en.wikipedia.org/wiki/Graphics_processing_unit).\n",
    "+ PyTorch provides some core functionality for Neural Networks:\n",
    "    - Some basic elements for building them up like linear layers, activation functions, etc.\n",
    "    - Automatic differentation for getting the derivative of loss functions with respect to parameters.\n",
    "    - Some stochastic optimization algorithms for minimizing loss functions\n",
    "    - ...\n",
    "\n",
    "I am not going to provide here a complete tutorial of PyTorch.\n",
    "You are advised to go over the first three topics of the [Deep Learning with PyTorch: A 60 minute blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html) prior to beginning this hands-on activity.\n",
    "Otherwise, it is unlikely that you understand the code that follows.\n",
    "\n",
    "PyTorch was developed by the Facebook AI Research Group.\n",
    "There is another powerful alternative developed by Google Brain: [TensorFlow](https://www.tensorflow.org/).\n",
    "I find PyTorch easier to use than TensorFlow and that's why we only use this in this class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making neural networks in PyTorch\n",
    "\n",
    "PyTorch is fairly flexible in allowing you to make any type neural network you like.\n",
    "You have absolute freedom on how your model looks like.\n",
    "However, it does provide a super easy way to make dense neural networks with a fixed activation function.\n",
    "That's what we are going to start with.\n",
    "First, import torch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The submodule `torch.nn` is where the neural network building blocks reside:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let me show you how you can make a single linear layer:\n",
    "$$\n",
    "y = Wx + b.\n",
    "$$\n",
    "The weights are selected randomly if not specified.\n",
    "Here you go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = nn.Linear(1, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a function that takes one dimensional inputs and spits out 20 dimensional outputs.\n",
    "Here is how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(10, 1) # 10 randomly sampled one dimensinal inputs\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, this took us to 10, 20 dimensional outputs. Looks good.\n",
    "\n",
    "But where are the weights and the bias term?\n",
    "Here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can directly change them if you wish.\n",
    "Notice the `requires_grad=True` flag.\n",
    "This is because PyTorch knows that these are parameters to be optimized.\n",
    "\n",
    "There is a little bit of flexibility on `nn.Linear`.\n",
    "For example, you can completly drop the bias if you wish.\n",
    "For the complete list of possibilities, you should always [check the docs](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html).\n",
    "\n",
    "Now, let's get to the activation functions.\n",
    "There are a lot already in `torch.nn`.\n",
    "Here is the sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = nn.Sigmoid()\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "x = torch.linspace(-5, 5, 100)[:, None]\n",
    "ax.plot(x, h(x))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$z=h(x)$')\n",
    "ax.set_title('Activation function: ' + str(h));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you could also implement the activation fuction by hand.\n",
    "The only restriction is that you should be using `PyTorch` functions instead of `numpy` functions.\n",
    "Here is how we would do it for the sigmoid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is how you could do this by hand:\n",
    "h_by_hand = lambda x: torch.exp(x) / (1.0 + torch.exp(x))\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x, h_by_hand(x))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$z=h(x)$')\n",
    "ax.set_title('Activation function: Sigmoid (hand version)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are now some of the most commonly used activation functions in `torch.nn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x, h_by_hand(x))\n",
    "\n",
    "for Func in [nn.Sigmoid, nn.ReLU, nn.Tanh]:\n",
    "    h = Func()\n",
    "    ax.plot(x, h(x), label=str(h))\n",
    "    \n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$z=h(x)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a linear layer and an activation function here is how we can combine them to make a function that takes us from the input to the internal neurons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = nn.Sigmoid()\n",
    "z_func = lambda x: h(layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty much it. And that's now a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_func(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, for regression, we would like to bring this back to a scalar output.\n",
    "To do this, we need to add one more linear layer taking the 20 internal neurons, back to one dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer = nn.Linear(20, 1)\n",
    "f = lambda x: final_layer(z_func(x))\n",
    "print(f(x).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of doing this manually, we can can use the class `nn.Sequential` of PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Sequential(layer, nn.Sigmoid(), final_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the recommended way, because `nn.Sequential` adds some additional functionality which I will show you in a while.\n",
    "You can evaluate this as a function, and you can also plot it.\n",
    "But to plot it, you have to turn the output into a proper numpy array.\n",
    "This is because matplotlib doesn't like PyTorch tensors that depend on parameters.\n",
    "Here is what you need to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = f(x).detach().numpy() # detach freezes the parameters to whatever they are\n",
    "                          # numpy returns a proper numpy array\n",
    "print(type(y))\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how it looks like (remember the weights are random):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x, f(x).detach().numpy())\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$f(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `nn.Sequential` is very convenient, because it allows us to build very deep networks really quickly.\n",
    "Here is a 5-layer network that starts from one input, takes us through 3 layers with 20 neurons each, and ends on a single output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = nn.Sequential(nn.Linear(1, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where are the parameters in an object created in this way?\n",
    "Here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for theta in f.named_parameters():\n",
    "    print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's why we love PyTorch. Because it does all the dirty work for us.\n",
    "Imagine having to keep track of all these parameters by hand.\n",
    "\n",
    "For those of you who want to know what is going on inside `nn.Linear`, just note that it is a special case of a PyTorch neural network module, see [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "The latter is what you would directly inherit when writing your own class for a non-standard neural network.\n",
    "We are not going to cover it in this class, but you can find plenty of examples [here](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a loss function\n",
    "\n",
    "Let's now make the loss function that we want to minimize.\n",
    "It needs to be a `PyTorch` function as well.\n",
    "For regression problems, we can think of the loss as a function of the model predictions and the observed data.\n",
    "That is the depends on the parameters comes through the predictions.\n",
    "Let's write down the mean square error (MSE) loss in this form.\n",
    "It is:\n",
    "$$\n",
    "L_{\\text{MSE}}(\\theta) = L_{\\text{MSE}}(y_{1:n}, f(x_{1:n};\\theta)) = \\frac{1}{n}\\sum_{i=1}^n\\left[y_i-f(x_i;\\theta)\\right]^2,\n",
    "$$\n",
    "where $x_{1:n}$ are the observed inputs (features), $y_{1:n}$ are the observed outputs (targets), and $f(x_{1:n};\\theta)$ contains the model predictions on the observed inputs.\n",
    "\n",
    "You can implement the MSE loss like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss_ours = lambda y, f: torch.mean((y - f) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can use built-in PyTorch functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_loss = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate it for some random data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of fake observations\n",
    "n = 20\n",
    "# Some fake observed features\n",
    "x_fake = torch.rand(n, 1)\n",
    "# Some fake observed targets\n",
    "y_fake = 4 * x_fake ** 2 - 5 * x_fake ** 3 + 0.1 * torch.rand(n, 1)\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_fake, y_fake, 'x');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how to calculate the loss (for the random parameters that our net started with):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the net:\n",
    "y_pred = f(x_fake)\n",
    "# Evaluate the loss\n",
    "our_loss = mse_loss_ours(y_fake, y_pred)\n",
    "built_in_loss = mse_loss(y_fake, y_pred)\n",
    "print(our_loss)\n",
    "print(built_in_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's just minimize the MSE loss for these fake data and see what kind of fit we are going to get.\n",
    "Here is how you do this in PyTorch.\n",
    "Since I don't have a lot of data, I will just use gradient descent - no randomly subsampling the data.\n",
    "I will show you how you can use stochastic gradient descent in the next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reinitialize the net:\n",
    "f = nn.Sequential(nn.Linear(1, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 1))\n",
    "\n",
    "# Initialize the optimizer - Notice that it needs to know about the \n",
    "# parameters it is optimizing\n",
    "optimizer = torch.optim.SGD(f.parameters(), lr=0.01) # lr is the learning rate\n",
    "# Some place to hold the training loss for visualizing it later\n",
    "training_loss = []\n",
    "# Iterate the optimizer. Let's just do 10 iterations.\n",
    "for i in range(10000):\n",
    "    # This is essential for the optimizer to keep\n",
    "    # track of the gradients correctly\n",
    "    # It is using some buffers internally that need to\n",
    "    # be manually zeroed on each iteration.\n",
    "    # This is because it doesn't know when you are done with the\n",
    "    # calculation of the loss\n",
    "    optimizer.zero_grad()\n",
    "    # Make predictions\n",
    "    y_pred = f(x_fake)\n",
    "    # Evaluate the loss - That's what you are minimizing\n",
    "    loss = mse_loss(y_fake, y_pred)\n",
    "    # Evaluate the derivative of the loss with respect to\n",
    "    # all parameters - It knows how to do it because of\n",
    "    # PyTorch magick\n",
    "    loss.backward()\n",
    "    # And now you are ready to make a step\n",
    "    optimizer.step()\n",
    "    # Save the training loss of later visualization\n",
    "    training_loss.append(loss.item())\n",
    "    # Print the loss every one hundend iterations:\n",
    "    if i % 1000 == 0:\n",
    "        print('it = {0:d}: loss = {1:1.3f}'.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the predictions of this model on the fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_fake, y_fake, 'x');\n",
    "xx = torch.linspace(x_fake.min(), x_fake.max(), 100)[:, None]\n",
    "yy = f(xx).detach().numpy()\n",
    "ax.plot(xx, yy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this may or may not work depending on what random seed you start with.\n",
    "If you run it a few times it may get stack at some local minimum.\n",
    "Unless we do stochastic optimization, i.e., subsampling the data, this is not a very good algorithm.\n",
    "Here how the loss changes with each iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(training_loss)\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Training loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is the plato we have at the beginning of the optimization.\n",
    "\n",
    "Let's redo this thing with stochastic optimization.\n",
    "For stochastic optimization we need to subsample the data during each iteration.\n",
    "We can either do this manually or using PyTorch functionality.\n",
    "First, let's do it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a subsampling batch size\n",
    "m = 5\n",
    "\n",
    "# Reinitialize the net:\n",
    "f = nn.Sequential(nn.Linear(1, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 1))\n",
    "\n",
    "# Reinitialize the optimizer\n",
    "optimizer = torch.optim.SGD(f.parameters(), lr=0.01)\n",
    "# Keep track of the training loss\n",
    "training_loss_sgd = []\n",
    "# Iterate the optimizer. Let's just do 10 iterations.\n",
    "for i in range(10000):\n",
    "    # Zero out the gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "    # Sample m observation indices at random\n",
    "    idx = np.random.randint(0, n, m)\n",
    "    # Here is the subsample of the data\n",
    "    x_batch = x_fake[idx]\n",
    "    y_batch = y_fake[idx]\n",
    "    # Make predictions\n",
    "    y_pred = f(x_batch)\n",
    "    # Evaluate the loss - That's what you are minimizing\n",
    "    loss = mse_loss(y_batch, y_pred)\n",
    "    # Evaluate the derivative of the loss with respect to\n",
    "    # all parameters - It knows how to do it because of\n",
    "    # PyTorch magick\n",
    "    loss.backward()\n",
    "    # And now you are ready to make a step\n",
    "    optimizer.step()\n",
    "    # Keep track of the training loss\n",
    "    training_loss_sgd.append(loss.item())\n",
    "    # Print the loss every one hundend iterations:\n",
    "    if i % 1000 == 0:\n",
    "        print('it = {0:d}: loss = {1:1.2e}'.format(i, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_fake, y_fake, 'x');\n",
    "xx = torch.linspace(x_fake.min(), x_fake.max(), 100)[:, None]\n",
    "yy = f(xx).detach().numpy()\n",
    "ax.plot(xx, yy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This fit does look a little bit better.\n",
    "Let's now compare the training loss of stochastic gradient descent to the previous one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(training_loss_sgd, label='Gradient descent')\n",
    "ax.plot(training_loss, label='Stochastic gradient descent')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Training loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is this wiggly nature of stochastic gradient descent that allows it to escape bad local minima.\n",
    "\n",
    "### Questions\n",
    "\n",
    "- Rerun the stochastic gradient descent with one sample per iteration ($m=1$). Does it converge? Do you need less or more iterations? Is it more or less wiggly?\n",
    "- Rerun the stochastic gradient descent with 10 samples per iteration. How doe it perfom now?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - Motorcyle Data\n",
    "\n",
    "Let's now use the motorcycle data to do regression with DNNs.\n",
    "This will help us demonstrate some best practices and specifically:\n",
    "- Standarizing the data\n",
    "- Splitting in training and test subsets\n",
    "\n",
    "First, start by loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The url of the motorcycle data:\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/motor.dat'\n",
    "# Download the data\n",
    "download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = np.loadtxt('motor.dat')\n",
    "x = data[:, 0][:, None]\n",
    "y = data[:, 1][:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize them\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(x_train, y_train, 'x', markeredgewidth=2, label='Training data')\n",
    "ax.plot(x_test, y_test, 'o', markeredgewidth=2, label='Test data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the data into torch tensors:\n",
    "x_train = torch.tensor(x_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the specification of `dtype=torch.float` is absolutely needed here.\n",
    "If you don't include it the code is not going to work.\n",
    "The problem is that the `x_train` etc. are all numpy arrays and that numpy arrays have 64-bit floating point numbers by default.\n",
    "PyTorch is using 32-bit floating point numbrs by default.\n",
    "We need at some point make the two compatible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to train the network.\n",
    "Let's give it a shot.\n",
    "We will use the same architecture as before.\n",
    "The only difference is that I will be printing the validation loss instead of the training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of training samples\n",
    "n = x_train.shape[0]\n",
    "\n",
    "# Pick a subsampling batch size\n",
    "m = 5\n",
    "\n",
    "# Reinitialize the net:\n",
    "f = nn.Sequential(nn.Linear(1, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 1))\n",
    "\n",
    "# Reinitialize the optimizer\n",
    "optimizer = torch.optim.SGD(f.parameters(), lr=0.01)\n",
    "# Keep track of the training loss and the test loss\n",
    "training_loss = []\n",
    "test_loss = []\n",
    "# Iterate the optimizer. Let's just do 10 iterations.\n",
    "for i in range(10000):\n",
    "    # Zero out the gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "    # Sample m observation indices at random\n",
    "    idx = np.random.randint(0, n, m)\n",
    "    # Here is the subsample of the data\n",
    "    x_batch = x_train[idx]\n",
    "    y_batch = y_train[idx]\n",
    "    # Make predictions\n",
    "    y_pred = f(x_batch)\n",
    "    # Evaluate the loss - That's what you are minimizing\n",
    "    loss = mse_loss(y_batch, y_pred)\n",
    "    training_loss.append(loss.item())\n",
    "    # Evaluate the derivative of the loss with respect to\n",
    "    # all parameters - It knows how to do it because of\n",
    "    # PyTorch magick\n",
    "    loss.backward()\n",
    "    # And now you are ready to make a step\n",
    "    optimizer.step()\n",
    "    # Evaluate the test loss\n",
    "    y_pred_test = f(x_test)\n",
    "    ts_loss = mse_loss(y_test, y_pred_test)\n",
    "    test_loss.append(ts_loss.item())\n",
    "    # Print the loss every one hundend iterations:\n",
    "    if i % 1000 == 0:\n",
    "        print('it = {0:d}: loss = {1:1.2e}'.format(i, ts_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code may not work at all, giving you nan's.\n",
    "Or it may work and get you nowhere.\n",
    "The problem here is the scale of both the inputs and the outputs and the assumptions that have been made about them when we initialize the weights of the net and when we picked the learning rate of the optimization algorithm.\n",
    "The easiest way to overcome this problem is to *standarize the data*.\n",
    "This is achieved by subtracting the empirical mean and dividing by the empirical standard deviation both the inputs and the outputs.\n",
    "By standarizing the data, we are making the default paramerameters (for weight initialization and stochastic gradient descent) valid again.\n",
    "Standarization is such a common process that it is already implemented in [sklearn.preprocessing.StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "Here is how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "feature_scaler = StandardScaler().fit(x)\n",
    "target_scaler = StandardScaler().fit(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `feature_scaler.transform()` is a function:\n",
    "$$\n",
    "x \\rightarrow \\frac{x-\\mu}{\\sigma},\n",
    "$$\n",
    "where $\\mu$ and $\\sigma$ are the empirical mean and standard deviation of the features.\n",
    "Here they are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The mean:\n",
    "feature_scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The standard deviation:\n",
    "feature_scaler.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the scalers work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_scaled = feature_scaler.transform(x)\n",
    "y_scaled = feature_scaler.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are now scaled, see this fig:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_scaled, y_scaled, 'x')\n",
    "ax.set_xlabel('$x$ (scaled)')\n",
    "ax.set_ylabel('$y$ (scaled)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train the net using `x_scale` and `y_scaled`. We can always go back to the original scales at the end.\n",
    "Let's see if it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split in training and test\n",
    "x_s_train, x_s_test, y_s_train, y_s_test = train_test_split(x_scaled, y_scaled,\n",
    "                                                            test_size=0.3)\n",
    "\n",
    "# Turn the data into torch tensors:\n",
    "x_s_train = torch.tensor(x_s_train, dtype=torch.float)\n",
    "y_s_train = torch.tensor(y_s_train, dtype=torch.float)\n",
    "x_s_test = torch.tensor(x_s_test, dtype=torch.float)\n",
    "y_s_test = torch.tensor(y_s_test, dtype=torch.float)\n",
    "\n",
    "# The number of training samples\n",
    "n = x_train.shape[0]\n",
    "\n",
    "# Pick a subsampling batch size\n",
    "m = 5\n",
    "\n",
    "# Reinitialize the net:\n",
    "f = nn.Sequential(nn.Linear(1, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 20),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(20, 1))\n",
    "\n",
    "# Reinitialize the optimizer\n",
    "optimizer = torch.optim.SGD(f.parameters(), lr=0.01)\n",
    "# Keep track of the training loss and the test loss\n",
    "training_loss = []\n",
    "test_loss = []\n",
    "# Iterate the optimizer. Let's just do 10 iterations.\n",
    "for i in range(10000):\n",
    "    # Zero out the gradient buffers\n",
    "    optimizer.zero_grad()\n",
    "    # Sample m observation indices at random\n",
    "    idx = np.random.randint(0, n, m)\n",
    "    # Here is the subsample of the data\n",
    "    x_batch = x_s_train[idx]\n",
    "    y_batch = y_s_train[idx]\n",
    "    # Make predictions\n",
    "    y_pred = f(x_batch)\n",
    "    # Evaluate the loss - That's what you are minimizing\n",
    "    loss = mse_loss(y_batch, y_pred)\n",
    "    training_loss.append(loss.item())\n",
    "    # Evaluate the derivative of the loss with respect to\n",
    "    # all parameters - It knows how to do it because of\n",
    "    # PyTorch magick\n",
    "    loss.backward()\n",
    "    # And now you are ready to make a step\n",
    "    optimizer.step()\n",
    "    # Evaluate the test loss\n",
    "    y_pred_test = f(x_s_test)\n",
    "    ts_loss = mse_loss(y_s_test, y_pred_test)\n",
    "    test_loss.append(ts_loss.item())\n",
    "    # Print the loss every one hundend iterations:\n",
    "    if i % 1000 == 0:\n",
    "        print('it = {0:d}: loss = {1:1.2e}'.format(i, ts_loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the fit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx_scaled = torch.linspace(x_scaled.min(), x_scaled.max(), 100)[:, None]\n",
    "yy_scaled = f(xx_scaled).detach().numpy()\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_s_train, y_s_train, 'x', label='Training data')\n",
    "ax.plot(x_s_test, y_s_test, 'o', label='Test data')\n",
    "ax.plot(xx_scaled, yy_scaled, label='DNN fit')\n",
    "ax.set_xlabel('$x$ (scaled)')\n",
    "ax.set_ylabel('$y$ (scaled)')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is predictions-observations plot on the test data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = f(x_s_test).detach().numpy()\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(y_pred_test, y_s_test, '.')\n",
    "yys = np.linspace(y_s_test.min(), y_s_test.max(), 10)\n",
    "ax.plot(yys, yys, 'r')\n",
    "ax.set_xlabel('Predictions')\n",
    "ax.set_ylabel('Observations');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, if you wish, you can scale the predictions back to the original units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = feature_scaler.inverse_transform(xx_scaled)\n",
    "yy = feature_scaler.inverse_transform(yy_scaled)\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(x_train, y_train, 'x', label='Training data')\n",
    "ax.plot(x_test, y_test, 'o', label='Test data')\n",
    "ax.plot(xx, yy, label='DNN fit')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is instructive to observe how the training and test losses evolve as a function of the optimization iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(training_loss, label='Training loss')\n",
    "ax.plot(test_loss, label='Test loss')\n",
    "ax.set_xlabel('Iteration')\n",
    "ax.set_ylabel('Loss')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wiggliness is, of course, due to the stochastic nature of the optimization.\n",
    "The training error converges to a minimum as you keep iterating.\n",
    "This is direct consequence of the Robbins-Monro theorem. You will reach a local minimum of the training error eventually.\n",
    "However, this is not the case for the test error.\n",
    "The test error will reach a minimum at some point and then it will start going up!\n",
    "It will always do this when you are training networks by just minimizing a loss function.\n",
    "What happens is that the algorithm, by paying more attention to minimizing the training loss it will start, eventually, overfitting the training data and it will not be able to generalize correctly for the test data.\n",
    "There are ways around this. We are going to learn about the basic one in the next lecture (*weight regularization* and *early stopping*).\n",
    "There are some advanced ways to avoid overfitting (e.g., *dropout*, *Bayesian neural networks*) which we are not going to cover in the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "- Change the activation function from `nn.ReLU` to `nn.Tanh`. Are you getting a better fit or a worse fit?\n",
    "- Rerun the code above for 100,000 iterations. Does it start to overfit the training data? What happens to the test loss?\n",
    "- Rerun the code for 5,000 iterations. How does the prediction look like now? Early-stopping would stop at about this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
