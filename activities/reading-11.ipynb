{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 11 - Analytical examples of Bayesian inference\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce Bayesian parameter estimation.\n",
    "\n",
    "## References\n",
    "\n",
    "+ [Principle of maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy) wikipedia entrty.\n",
    "\n",
    "## Bayesian Parameter Estimation\n",
    "\n",
    "Assume that we have a model that predicts the result of a random variable $X$.\n",
    "The model has some parameters $\\theta$ which are to be determined from data which consist of $N$ independent measurements of $X$, i.e., the data are:\n",
    "$$\n",
    "x_{1:N} = (x_1,\\dots,x_N).\n",
    "$$\n",
    "We can write the model as:\n",
    "$$\n",
    "x_n|\\theta \\sim p(x_n|\\theta),\n",
    "$$\n",
    "where we started abusing the mathematical that requires us to use capital letters of random variables and lower case letters for data.\n",
    "But this is just simpler and this is the notation we will be following from now on.\n",
    "The term $p(x_n|\\theta)$ is known as the *likelihood* of this data point.\n",
    "The likelihood of the entire dataset $x_{1:N}$ is just the joint probability density of all observations, i.e., $p(x_{1:N}|\\theta)$.\n",
    "Because all observations are indpendent conditioned on the model parameters $\\theta$, we have:\n",
    "$$\n",
    "p(x_{1:N}|\\theta) = \\prod_{n=1}^Np(x_n|\\theta).\n",
    "$$\n",
    "Be careful. This factorization of the likelihood is not valid if the measurements are not independent.\n",
    "In that case, you need to keep the entire thing together (or factorize it in the way that it factorizes...)\n",
    "\n",
    "To close the loop, we need to say what we think about the pameters before we see any data.\n",
    "In Bayesian jargon, we need to specify our *prior state of knowledge*, or simple our *prior*:\n",
    "$$\n",
    "\\theta \\sim p(\\theta).\n",
    "$$\n",
    "How we do this is a big discussion.\n",
    "For now let's just say that we use common sense.\n",
    "\n",
    "The situation can be discribed graphical as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: coin_toss_bayes_plate Pages: 1 -->\n",
       "<svg width=\"94pt\" height=\"154pt\"\n",
       " viewBox=\"0.00 0.00 94.00 154.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 150)\">\n",
       "<title>coin_toss_bayes_plate</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-150 90,-150 90,4 -4,4\"/>\n",
       "<g id=\"clust1\" class=\"cluster\">\n",
       "<title>cluster_0</title>\n",
       "<polygon fill=\"none\" stroke=\"black\" points=\"8,-8 8,-82 78,-82 78,-8 8,-8\"/>\n",
       "<text text-anchor=\"middle\" x=\"43\" y=\"-14.8\" font-family=\"Times,serif\" font-size=\"14.00\">n=1,...,N</text>\n",
       "</g>\n",
       "<!-- theta -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>theta</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"43\" cy=\"-128\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"start\" x=\"39.11\" y=\"-123.8\" font-family=\"Times,serif\" font-size=\"14.00\">Î¸</text>\n",
       "</g>\n",
       "<!-- xn -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>xn</title>\n",
       "<ellipse fill=\"lightgrey\" stroke=\"black\" cx=\"43\" cy=\"-56\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"start\" x=\"36\" y=\"-52.8\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "<text text-anchor=\"start\" x=\"43\" y=\"-52.8\" font-family=\"Times,serif\" baseline-shift=\"sub\" font-size=\"14.00\">n</text>\n",
       "</g>\n",
       "<!-- theta&#45;&gt;xn -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>theta&#45;&gt;xn</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M43,-109.7C43,-101.98 43,-92.71 43,-84.11\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"46.5,-84.1 43,-74.1 39.5,-84.1 46.5,-84.1\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x110373a50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from graphviz import Digraph\n",
    "gcp = Digraph('coin_toss_bayes_plate')\n",
    "gcp.node('theta', label='<&theta;>')\n",
    "with gcp.subgraph(name='cluster_0') as sg:\n",
    "    sg.node('xn', label='<x<sub>n</sub>>', style='filled')\n",
    "    sg.attr(label='n=1,...,N')\n",
    "    sg.attr(labelloc='b')\n",
    "gcp.edge('theta', 'xn')\n",
    "gcp.render('coin_toss_bayes_plate', format='png')\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to apply Bayes' rule to find the posterior.\n",
    "Recall the Bayes' rule for propositions $A$ and $B$:\n",
    "$$\n",
    "p(A|B) = \\frac{p(AB)}{p(B)}.\n",
    "$$\n",
    "We set here\n",
    "$$\n",
    "A = \\text{the model parameters} = \\theta,\n",
    "$$\n",
    "and\n",
    "$$\n",
    "B = \\text{the data} = x_{1:N}.\n",
    "$$\n",
    "That is the question we ask is \"what is the probability of the model parameters given the data?\"\n",
    "Let's just blindly follow the rule:\n",
    "$$\n",
    "p(\\text{the model parameters}|\\text{the data}) = \\frac{p(\\text{the data and the parameters})}{p(\\text{the data})}.\n",
    "$$\n",
    "This $p(\\text{the model parameters}|\\text{the data})$ has a special name.\n",
    "It is our *posterior state of knowledge* about the model parameters or simply the *posterior*.\n",
    "\n",
    "We are ready to proceed.\n",
    "What is the joint probability of the data and the parameters?\n",
    "Well, we can use the Bayes rule again, but in this form:\n",
    "$$\n",
    "p(AB) = p(B|A)p(A).\n",
    "$$\n",
    "In our example:\n",
    "$$\n",
    "p(\\text{the data and the parameters}) = p(\\text{the data} | \\text{the parameters}) p(\\text{the parameters}).\n",
    "$$\n",
    "Putting it all together we get:\n",
    "$$\n",
    "\\text{posterior} = p(\\text{the model parameters}|\\text{the data}) = \n",
    "\\frac{p(\\text{the data} | \\text{the parameters}) p(\\text{the parameters})}{p(\\text{the data})}\n",
    "$$\n",
    "But we have given special names to the terms on the right:\n",
    "$$\n",
    "p(\\text{the data} | \\text{the parameters}) = p(x_{1:N}|\\theta) = \\text{likelihood},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "p(\\text{the parameters}) = p(\\theta) = \\text{prior}.\n",
    "$$\n",
    "So, we can now write the mnemonic (ignoring the normalization constant):\n",
    "$$\n",
    "\\text{posterior} \\propto \\text{likelihood}\\times\\text{prior}.\n",
    "$$\n",
    "Tracking back our symbols, this can be written mathematically as:\n",
    "$$\n",
    "p(\\theta | x_{1:N}) \\propto p(x_{1:N}|\\theta)p(\\theta).\n",
    "$$\n",
    "*The posterior is everything a Bayesian has to say about the parameter estimation problem.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
