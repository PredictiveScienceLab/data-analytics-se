{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 11: Selecting prior information\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Develop systematic ways for assigning probabilities.\n",
    "\n",
    "## References\n",
    "\n",
    "+ [Principle of maximum entropy](https://en.wikipedia.org/wiki/Principle_of_maximum_entropy) wikipedia entrty.\n",
    "\n",
    "## How do we come up with the right probability assignments?\n",
    "\n",
    "In applications we often found ourselves in a situation where we have to pick prior probabilities of a given variable. That is, pick probabilities before we see any specific data from that variable.\n",
    "An important question is how we come up with these prior probabilities.\n",
    "Is there a systematic theoretical framework we could follow?\n",
    "There are basically three widely accepted ways:\n",
    "\n",
    "+ The principle of insufficient reason.\n",
    "+ The principle of maximum entropy.\n",
    "+ The principle of transformation groups.\n",
    "\n",
    "In this lecture, we will explain the first two. The third one, transformation groups, is rather advanced and we will not discuss it.\n",
    "At the beginning, what we talk about will just work with discrete random variables.\n",
    "Continuous random variables are a little bit trickier and we are going to discuss them at the end.\n",
    "\n",
    "## Principle of Insufficient Reason\n",
    "\n",
    "The principle of insufficient reason has its origins to Laplace. The original statement was:\n",
    "> The theory of chance consists in reducing all the events of the same kind to a certain number of cases equally possible, that is to say, to such as we may be equally undecided about in regard to their existence, and in determining the number of cases favorable to the event whose probability is sought. The ratio of this number to that of all the cases possible is the measure of this probability, which is thus simply a fraction whose numerator is the number of favorable cases and whose denominator is the number of all the cases possible.\n",
    "*Pierre-Simon Laplace*\n",
    "\n",
    "Let's restate this in simpler terms.\n",
    "Assume that the random variable $X$ can take $N$ possible values, $1, 2,\\dots,N$.\n",
    "If this is all we know about this random variable then *the principle of insufficient reason* tells us to set:\n",
    "$$\n",
    "p(x) = \\frac{1}{N},\n",
    "$$\n",
    "for $x$ in $\\{1,2,\\dots,N\\}$.\n",
    "That is, the principle of insufficient reason tells us to assign the same probability to each possibility.\n",
    "Intuitively, any other choice we could make would introduce a bias towards one value or another.\n",
    "\n",
    "### Example: Throwing a six-sided die\n",
    "Consider a six-sided die with sides numbered $1$ to $6$.\n",
    "Call $X$ the random variable corresponding to an experiment of throwing the die.\n",
    "What is the probability of the die taking a specific value.\n",
    "Using the principle of insufficient reason, we set:\n",
    "$$\n",
    "p(X=x) = \\frac{1}{6}.\n",
    "$$\n",
    "\n",
    "## The Principle of Maximum Entropy\n",
    "\n",
    "The principle of maximum entropy extends the principe of insufficient reason in a very useful way.\n",
    "It tells you what probability distribution to assign to a random variable $X$ when you have some prior information about it.\n",
    "This information could include, for example, the expected value of $X$, or maybe its variance (see the section on *testable prior information* for a more precise description of what is allowed).\n",
    "The simplest non-mathematical definition of the principle of maximum entropy I could find is due to E. T. Jaynes:\n",
    "\n",
    "> The knowledge of average values does give a reason for preferring some possibilities to others, but we would like [...] to assign a probability distribution which is as uniform as it can be while agreeing with the available information.\"\n",
    "\n",
    "Why does he say \"as uniform as it can be?\" \n",
    "He does this because he wants the principle to be consistent with the principle of insufficient reason when there is not available information.\n",
    "Of course, the uniform distribution is the most \"uncertain\" distribution, so we could also say that we are looking for a maximumally uncertain distribution which agrees with the available information.\n",
    "The \"uncertainty\" of a probability distribution is measured by its \"information entropy\", a concept that we explain in the subsequent section.\n",
    "\n",
    "### Information entropy\n",
    "We would like to know, how much uncertainty there is in a probability mass function $p(x)$.\n",
    "In 1948, [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) posed and answered this problem in his seminal paper titled \"A Mathematical Theory of Communication.\"\n",
    "The details of his derivation are beyond the scope of this course, but they can be summarized as follows:\n",
    "\n",
    "+ He looked for a functional $\\mathbb{H}[p(X)]$ that measured the uncertainty of the probability mass function $p(x)$ using real values.\n",
    "+ He wrote down some axioms that this functional should satisfy. For example, that is should be continuous, and that it should have its maximum when $p(x)$ is the uniform (because the uniform distribution has the maximum uncertainty).\n",
    "+ He did a little bit of math, and provied that (up to an arbitrary multiplicative constant) the function he was looking for must have this form:\n",
    "$$\n",
    "\\mathbb{H}[p(X)] = -\\sum_x \\log p(x) p(x).\n",
    "$$\n",
    "+ As he was looking for a name for this function, he showed his discovery to [von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann) who recognized the similarity to the entropy of statistical mechanics first introduced by [J. W. Gibbs](https://en.wikipedia.org/wiki/Josiah_Willard_Gibbs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the function is maximized at $p_0 = 0.5$ because this corresponds to maximum uncertainty.\n",
    "The function is minimized (as a matter of fact it is exactly zero) at $p_0 = 0$ and $p_0 = 1$ because both these cases correspond to minimum uncertainty (you are certain what is going to happen).\n",
    "\n",
    "### Mathematical description of testable information\n",
    "For our purposes, it suffices to assume that our information about $X$ comes in the form of expectations of functions of $X$, i.e., it is:\n",
    "$$\n",
    "\\mathbb{E}[f_k(X)] = F_k,\n",
    "$$\n",
    "for some *known functions* $f_k(x)$ and some *known values* $F_k$ for their expectations, $k=1,\\dots,K$.\n",
    "Let's demonstrate that this definition includes some important cases:\n",
    "\n",
    "+ $I = $ \"The expected value of $X$ is $\\mu$.\" This is obviously included as it is just the statement \n",
    "$$\n",
    "\\mathbb{E}[X] = \\mu\n",
    "$$. So, we are covered by setting $K=1$, $f_1(x) = x$, and $F_1 = \\mu$.\n",
    "\n",
    "+ $I = $ \"The expected value of $X$ is $\\mu$ and the variance of $X$ is $\\sigma^2$.\" Here we obviously have $\\mathbb{E}[X] = \\mu$, just like before. The second condition is about the variance, $\\mathbb{V}[X] = \\sigma^2$. We can easily turn this into an expectation by using the formula $\\mathbb{V}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2)$. It becomes:\n",
    "$$\n",
    "\\mathbb{E}[X^2] = \\sigma^2 + \\mu^2.\n",
    "$$\n",
    "So, we are covered again with $K=2$, $f_1(x) = x$, $f_2(x) = x^2$, $F_1 = \\mu$, $F_2 = \\sigma^2 + \\mu^2$.\n",
    "\n",
    "### Mathematical statement of the principle of maximum entropy\n",
    "Having defined the measure of uncertainty and how the available information is modeled, we can now state the principle of maximum entropy mathematically.\n",
    "Take a random variable with $N$ different possibilities with probabilities $p_1=p(X=x_1),\\dots,p_N = p(X=x_N)$ to be identified.\n",
    "We need to maximize:\n",
    "$$\n",
    "\\mathbb{H}[p(X)] = -\\sum_{i=1}^N p_i\\log p_i,\n",
    "$$\n",
    "subject to the normalization constraint:\n",
    "$$\n",
    "\\sum_i p_i = 1,\n",
    "$$\n",
    "and the testable information constraints:\n",
    "$$\n",
    "\\mathbb{E}[f_k(X)] = F_k.\n",
    "$$\n",
    "The general solution of this problem can be found using the [Karush-Kuhn-Tucker conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions).\n",
    "If you go through the derivation, you will find that:\n",
    "$$\n",
    "p(X=x_i) = \\frac{1}{Z}\\exp\\left\\{\\sum_{k=1}^K\\lambda_kf_k(x_i)\\right\\},\n",
    "$$\n",
    "where the $\\lambda_k$'s are constants and $Z$ is the normalization constant:\n",
    "$$\n",
    "Z = \\sum_i \\exp\\left\\{\\sum_{k=1}^K\\lambda_kf_k(x_i)\\right\\}.\n",
    "$$\n",
    "The $\\lambda_k$'s can be identified by solving the system of non-linear equations:\n",
    "$$\n",
    "F_k = \\frac{\\partial\\log Z}{\\partial \\lambda_k},\n",
    "$$\n",
    "for $k=1,\\dots,K$.\n",
    "\n",
    "### Examples of discrete maximum entropy distributions\n",
    "\n",
    "In what follows, we provide some examples of maximum entropy distributions that naturally arise.\n",
    "\n",
    "+ The categorical with equal probabilities $\\operatorname{Categorical}(\\frac{1}{N},\\dots,\\frac{1}{N})$ is the maximum entropy distribution for a random variable $X$ taking $N$ different values (no other constraints).\n",
    "\n",
    "+ The Bernoulli distribution $\\operatorname{Bernoulli}(\\theta)$ is the maximum entropy distribution for a random variable $X$ taking two values $0$ and $1$ with known expectation $\\mathbb{E}[X] = \\theta$.\n",
    "\n",
    "+ The Binomial distribution $B(\\theta,n)$ is the maximum entropy distribution for a random variable $X$ taking values $0, 1,\\dots,n$ with known expectation $\\mathbb{E}[X] = \\theta n$ (within the class of $n$-generalized binomial distributions, i.e., the distribution representing the number of successful trials in $n$, potentially correlated, experiments).\n",
    "\n",
    "+ The Poisson distribution $\\operatorname{Poisson}(\\lambda)$ is the maximum entropy distribution for a random variable $X$ taking values $0, 1, 2,\\dots$ with known expectation $\\mathbb{E}[X] = \\lambda$ (within the class of $\\infty$-generalized binomial distributions).\n",
    "\n",
    "+ The [canonical ensemble](https://en.wikipedia.org/wiki/Canonical_ensemble) is the maximum entropy distribution over the states of a quantum mechanical system with known expected energy.\n",
    "\n",
    "+ The [grand canonical ensemble](https://en.wikipedia.org/wiki/Grand_canonical_ensemble) is the maximum entropy distribution over the states of a quanum mechanical system consiting of many different numbers of particles with known expected number of particles per type and known expected energy.\n",
    "\n",
    "### Continuous distributions\n",
    "Shannon's entropy only works for discrete distributions.\n",
    "Why?\n",
    "Consider the naïve generalization:\n",
    "$$\n",
    "\\mathbb{H}_{\\text{naïve}}[p(X)] = -\\int p(x)\\log p(x)dx.\n",
    "$$\n",
    "Now, imagine that you could equally well work with a transformed version of $X$.\n",
    "Mathematically, assume that $Y = T(X)$ where $T(x)$ is invertible.\n",
    "Since $X$ and $Y$ are connected in this way you should be getting the same information entropy independently of whether you calculate it with $p(X)$ or $p(Y)$.\n",
    "But, there are many counter examples where you get:\n",
    "$$\n",
    "\\mathbb{H}_{\\text{naïve}}[p(X)] \\not= \\mathbb{H}_{\\text{naïve}}[p(Y)].\n",
    "$$\n",
    "This shows that $\\mathbb{H}_{\\text{naïve\"}}[p(X)]$ is a bad definition of uncertainty for continuous distributions.\n",
    "\n",
    "For continuous distributions, the correct thing to use is the relative entropy:\n",
    "$$\n",
    "\\mathbb{H}[p(X)] = -\\int p(x)\\log\\frac{p(x)}{q(x)}dx,\n",
    "$$\n",
    "where $q(x)$ is a prior density function (not necessarily normalized) encoding maximum uncertainty.\n",
    "You can find more about $q(x)$ in the note below.\n",
    "With this definition the maximum entropy principle for continuous random variables is as follows.\n",
    "Maximize:\n",
    "$$\n",
    "\\mathbb{H}[p(X)] = -\\int p(x)\\log\\frac{p(x)}{q(x)}dx,\n",
    "$$\n",
    "subject to the normalization constraint:\n",
    "$$\n",
    "\\int p(x) dx = 1,\n",
    "$$\n",
    "and the testable information constraints:\n",
    "$$\n",
    "\\mathbb{E}[f_k(X)] = F_k.\n",
    "$$\n",
    "Applying the [Karush-Kuhn-Tucker conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions), we find that:\n",
    "$$\n",
    "p(x) = \\frac{q(x)}{Z}\\exp\\left\\{\\sum_{k=1}^K\\lambda_kf_k(x)\\right\\},\n",
    "$$\n",
    "where the $\\lambda_k$'s are constants and $Z$ is the normalization constant:\n",
    "$$\n",
    "Z = \\int q(x)\\exp\\left\\{\\sum_{k=1}^K\\lambda_kf_k(x)\\right\\}dx.\n",
    "$$\n",
    "The $\\lambda_k$'s can be identified by solving the system of non-linear equations:\n",
    "$$\n",
    "F_k = \\frac{\\partial \\log Z}{\\partial \\lambda_k},\n",
    "$$\n",
    "for $k=1,\\dots,K$.\n",
    "\n",
    "### A note on $q(x)$\n",
    "There are, of course, cases in which $q(x)$ is just the uniform density.\n",
    "In these cases the mathematical form of the information entropy becomes identical to the discrete case.\n",
    "For example, if $x$ is a location parameter, e.g., the 3D location of a particle free to move in a box, then $q(x)$ is indeed uniform.\n",
    "As another example, imagine a particle constrained to move on a cyclic guide.\n",
    "Then $q(x)$ is constant on the cyclic guide and zero everywhere else.\n",
    "The takehome message dual. First, $q(x)$ depends on what the underlying random variable actually is.\n",
    "Second, the identification of $q(x)$ is beyond the scope of the maximum entropy principle.\n",
    "In other words, you need to have $q(x)$ before applying the maximum entropy principle.\n",
    "There are some systematic methods for identifying maximum uncertainty densities such as the [principle of transformation groups](https://en.wikipedia.org/wiki/Principle_of_transformation_groups) and the theory of [Haar measures](https://en.wikipedia.org/wiki/Haar_measure) but both these concepts require advanced mathematics.\n",
    "In many practical examples common sense is sufficient for coming up with $q(x)$.\n",
    "\n",
    "\n",
    "### Examples of continuous maximum entropy distributions\n",
    "\n",
    "In what follows, we provide some examples of maximum entropy distributions that naturally arise.\n",
    "\n",
    "+ The Uniform distribution $U([a,b])$ is the maximum entropy distribution for a random variable $X$ taking values in $[a,b]$ with $q(x) = 1$ and no other constraints.\n",
    "\n",
    "+ The normal distribution $N(\\mu,\\sigma^2)$ is the maximum entropy distribution for a random variable $X$ taking values in $\\mathbb{R}$ with $q(x) = 1$, known expectation $\\mathbb{E}[X] = \\mu$ and variance $\\mathbb{V}[X] = \\sigma^2$.\n",
    "\n",
    "+ The multivariate normal distribution $N(\\boldsymbol{\\mu},\\boldsymbol{\\Sigma})$ is the maximum entropy distribution for a random vector $\\mathbf{X}$ taking values in $\\mathbb{R}^d$ with $q(\\mathbf{x}) = 1$ and known expectation $\\mathbb{E}[\\mathbf{X}] = \\boldsymbol{\\mu}$ and covariance matrix $\\mathbb{C}[X,X] = \\boldsymbol{\\Sigma}$.\n",
    "\n",
    "+ The Exponential distribution $\\operatorname{Exp}(\\lambda)$ is the maximum entropy distribution for a random variable $X$ taking values in $[0,\\infty)$ with $q(x) = 1$ and known expectation $\\mathbb{E}[X] = \\frac{1}{\\lambda}$.\n",
    "\n",
    "For an almost list of a commonly used maximum entropy distributions, see the [Maximum entropy probability distribution entry of wikipedia](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
