{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 16.4: Diagnostics for Classifications\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To assess the qualtity of a classification model\n",
    "\n",
    "## High melting explosives sensitivity\n",
    "Let's repeat what we did for the HMX example, but after splitting the dataset into training and validation subsets.\n",
    "We will be making predictions on the validation subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the data file:\n",
    "url = 'https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/activities/hmx_data.csv'\n",
    "download(url)\n",
    "# Load the data using pandas\n",
    "import pandas as pd\n",
    "data = pd.read_csv('hmx_data.csv')\n",
    "# Extract data for regression\n",
    "# Heights as a numpy array\n",
    "x = data['Height'].values\n",
    "# The labels must be 0 and 1\n",
    "# We will use a dictionary to indicate our labeling\n",
    "label_coding = {'E': 1, 'N': 0}\n",
    "y = np.array([label_coding[r] for r in data['Result']])\n",
    "data['y'] = y\n",
    "\n",
    "# Separate data into training and validation\n",
    "num_obs = x.shape[0]\n",
    "# Select what percentage you want to put in the training data\n",
    "train_percentage = 0.7\n",
    "# Figure out how many training points you are going to use:\n",
    "num_train = int(num_obs * train_percentage)\n",
    "# Figure out how many validation points you are going to use:\n",
    "num_valid = num_obs - num_train\n",
    "print('num_train = {0:d}, num_valid = {1:d}'.format(num_train, num_valid))\n",
    "\n",
    "# Before splitting the data, randomly permute rows\n",
    "permuted_data = np.random.permutation(data)\n",
    "# Split\n",
    "train_data = permuted_data[:num_train] # This picks the first n_train rows\n",
    "valid_data = permuted_data[num_train:] # This puts the rest on the validation rows\n",
    "# Get the x's and the y's for regression\n",
    "x_train = train_data[:, 0].astype(np.float)\n",
    "y_train = train_data[:, 2].astype(np.int)\n",
    "x_valid = valid_data[:, 0].astype(np.float)\n",
    "y_valid = valid_data[:, 2].astype(np.int)\n",
    "# Let's plot the training and the validation datasets in different colors\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x_train, y_train, 'x', label='Training data')\n",
    "ax.plot(x_valid, y_valid, 'o', label='Validation data')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)\n",
    "\n",
    "# Make the design matrix\n",
    "degree = 2\n",
    "Phi_train = get_polynomial_design_matrix(x_train[:, None], degree)\n",
    "model = LogisticRegression(penalty='none', fit_intercept=False).fit(Phi_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data:\n",
    "Phi_valid = get_polynomial_design_matrix(x_valid[:, None], degree)\n",
    "predictions = model.predict_proba(Phi_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now make some decisions using the approach we presented in the previous hands-on activity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c_00 = cost of correctly picking 0 when 0 is true\n",
    "# c_01 = cost of wrongly picking 0 when 1 is true\n",
    "# c_11 = cost of correctly picking 1 when 1 is true\n",
    "# c_10 = cost of wrongly picking 1 when 0 is true\n",
    "cost_matrix = np.array(\n",
    "[[0.0, 1.0],\n",
    " [1.0, 0.0]]\n",
    ")\n",
    "# Expected cost of each decision for each validation point (num_valid x 2 matrix)\n",
    "exp_cost = np.einsum('ij,ki->kj', cost_matrix, predictions)\n",
    "# And now let's make all the decisions for all validation points at once:\n",
    "y_pred = np.argmin(exp_cost, axis=1)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's calculate the accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_valid, y_pred)\n",
    "print('HMX Accuracy = {0:1.2f} %'.format(acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad. But let's also look at the balanced accuracy score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "bacc = balanced_accuracy_score(y_valid, y_pred)\n",
    "print('HMX balanced accuracy = {0:1.2f} %'.format(bacc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "plot_confusion_matrix(model, Phi_valid, y_valid, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Repeat the analysis above with a higher degree polynomial, say 5. Is the result better or worse? Why?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
