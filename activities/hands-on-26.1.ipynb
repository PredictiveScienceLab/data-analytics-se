{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 26.1 (Physics-informed regularization: Solving ODEs) \n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Learn how to solve ODEs with neural networks.\n",
    "\n",
    "This notebook replicates some of the results of [Lagaris et al. 1998)](https://arxiv.org/pdf/physics/9705023.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# This is useful for taking derivatives:\n",
    "def grad(outputs, inputs):\n",
    "    return torch.autograd.grad(outputs, inputs, grad_outputs=torch.ones_like(outputs), create_graph=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single ODE\n",
    "\n",
    "Consider the ode:\n",
    "$$\n",
    "\\frac{d\\Psi}{dx} = f(x, \\Psi),\n",
    "$$\n",
    "with $x \\in [0,1]$ and initial conditions (IC):\n",
    "$$\n",
    "\\Psi(0) = A.\n",
    "$$\n",
    "We write the trial solution by:\n",
    "$$\n",
    "\\hat{\\Psi}(x; \\theta) = A + x N(x; \\theta),\n",
    "$$\n",
    "where $N(x; \\theta)$ is a neural network (NN).\n",
    "The solution is $\\hat{\\Psi}(x;\\theta)$ automatically satisfied the initial conditions.\n",
    "The loss function we would like to minimize to train the NN is:\n",
    "$$\n",
    "L(\\theta) = \\int_0^1 \\left[\\frac{d\\Psi_t(x;\\theta)}{dx} - f(x,\\hat{\\Psi}(x;\\theta))\\right]^2dx.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N is a Neural Network - This is exactly the network used by Lagaris et al. 1997\n",
    "N = nn.Sequential(nn.Linear(1, 50), nn.Sigmoid(), nn.Linear(50,1, bias=False))\n",
    "\n",
    "# Initial condition\n",
    "A = 0.\n",
    "\n",
    "# The Psi_t function\n",
    "Psi_t = lambda x: A + x * N(x)\n",
    "\n",
    "# The right hand side function\n",
    "f = lambda x, Psi: torch.exp(-x / 5.0) * torch.cos(x) - Psi / 5.0\n",
    "\n",
    "# The loss function\n",
    "def loss(x):\n",
    "    x.requires_grad = True\n",
    "    outputs = Psi_t(x)\n",
    "    Psi_t_x = torch.autograd.grad(outputs, x, grad_outputs=torch.ones_like(outputs),\n",
    "                                  create_graph=True)[0]\n",
    "    return torch.mean((Psi_t_x - f(x, outputs)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I am going to use the method that we find in Lagaris et al.\n",
    "Instead of using stochastic optimization, they use a lot of points to estimate the loss integral (I am going to use 100) and then they just do gradient-based optimization (I am going to do BFGS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize (same algorithm as in Lagaris)\n",
    "optimizer = torch.optim.LBFGS(N.parameters())\n",
    "\n",
    "# The collocation points used by Lagaris\n",
    "x = torch.Tensor(np.linspace(0, 2, 100)[:, None])\n",
    "\n",
    "# Run the optimizer\n",
    "def closure():\n",
    "    optimizer.zero_grad()\n",
    "    l = loss(x)\n",
    "    l.backward()\n",
    "    return l\n",
    "    \n",
    "for i in range(10):\n",
    "    optimizer.step(closure)\n",
    "\n",
    "# Let's compare the result to the true solution\n",
    "xx = np.linspace(0, 2, 100)[:, None]\n",
    "with torch.no_grad():\n",
    "    yy = Psi_t(torch.Tensor(xx)).numpy()\n",
    "yt = np.exp(-xx / 5.0) * np.sin(xx)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(xx, yt, label='True')\n",
    "ax.plot(xx, yy, '--', label='Neural network approximation')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\Psi(x)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are going to do the same thing using stochastic gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to reinitialize the network\n",
    "N = nn.Sequential(nn.Linear(1, 50), nn.Sigmoid(), nn.Linear(50,1, bias=False))\n",
    "\n",
    "# Let's see now if a stochastic optimizer makes a difference\n",
    "adam = torch.optim.Adam(N.parameters(), lr=0.01)\n",
    "\n",
    "# The batch size you want to use (how many points to use per iteration)\n",
    "n_batch = 5\n",
    "\n",
    "# The maximum number of iterations to do\n",
    "max_it = 1000\n",
    "\n",
    "for i in range(max_it):\n",
    "    # Randomly pick n_batch random x's:\n",
    "    x = 2 * torch.rand(n_batch, 1)\n",
    "    # Zero-out the gradient buffers\n",
    "    adam.zero_grad()\n",
    "    # Evaluate the loss\n",
    "    l = loss(x)\n",
    "    # Calculate the gradients\n",
    "    l.backward()\n",
    "    # Update the network\n",
    "    adam.step(closure)\n",
    "    # Print the iteration number\n",
    "    if i % 100 == 99:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compare the result to the true solution\n",
    "xx = np.linspace(0, 2, 100)[:, None]\n",
    "with torch.no_grad():\n",
    "    yy = Psi_t(torch.Tensor(xx)).numpy()\n",
    "yt = np.exp(-xx / 5.0) * np.sin(xx)\n",
    "\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(xx, yt, label='True')\n",
    "ax.plot(xx, yy, '--', label='Neural network approximation')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\Psi(x)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "In the following questions, just use the stochastic gradient approach.\n",
    "\n",
    "+ Change `n_batch` to just 1. Does the algorithm work? Did you have to increase the iterations to achieve the same accuracy?\n",
    "+ Modify the code, so that you now solve the problem for $x$ between 0 and 5. Play with the `n_batch` and `max_it` until you get a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving any dynamical system with neural networks\n",
    "\n",
    "Let's write some code that will allow us to solve any dynamical system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicalSystem(object):\n",
    "    \"\"\"\n",
    "    A class representing an initial value problem.\n",
    "    \n",
    "    :param dim:               The dimensionality of the problem.\n",
    "    :param rhs:               The right hand side of the equation.\n",
    "                              This must be a function with signature rhs(t, y)\n",
    "                              where t is time and y is the state of the system.\n",
    "    :param init_conditions:   Initial conditions. Must be a vector of dimension dim.\n",
    "    :param net:               A neural network for representing the solution. This must have\n",
    "                              one-dimensional input and dim-dimensional output.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dim, rhs, init_conditions, net):\n",
    "        assert isinstance(dim, int)\n",
    "        assert dim > 0\n",
    "        self._dim = dim\n",
    "        self._rhs = rhs\n",
    "        if isinstance(init_conditions, float):\n",
    "            init_conditions = np.atleast_1d(init_conditions)\n",
    "        init_conditions = torch.Tensor(init_conditions)\n",
    "        self._init_conditions = init_conditions\n",
    "        self._net = net\n",
    "        self._solution = lambda T: self.init_conditions + T * self.net(T)\n",
    "        \n",
    "    @property\n",
    "    def dim(self):\n",
    "        return self._dim\n",
    "    \n",
    "    @property\n",
    "    def rhs(self):\n",
    "        return self._rhs\n",
    "    \n",
    "    @property\n",
    "    def init_conditions(self):\n",
    "        return self._init_conditions\n",
    "    \n",
    "    @property\n",
    "    def net(self):\n",
    "        return self._net\n",
    "    \n",
    "    @property\n",
    "    def solution(self):\n",
    "        \"\"\"\n",
    "        Return the solution function.\n",
    "        \"\"\"\n",
    "        return self._solution\n",
    "    \n",
    "    def squared_residual_loss(self, T):\n",
    "        \"\"\"\n",
    "        Returns the squared residual loss at times T.\n",
    "        \n",
    "        :param T:    Must be a 1D torch tensor.\n",
    "        \"\"\"\n",
    "        T.requires_grad = True\n",
    "        sol = self.solution(T)\n",
    "        dsol_dt = torch.empty_like(sol)\n",
    "        for d in range(self.dim):\n",
    "            go = torch.zeros_like(sol)\n",
    "            go[:, d] = 1.0\n",
    "            dsol_dt[:, d] = torch.autograd.grad(sol, T, grad_outputs=go,\n",
    "                                                create_graph=True)[0][:, 0]\n",
    "        return torch.mean((dsol_dt - self.rhs(T, sol)) ** 2.)\n",
    "    \n",
    "    def solve_lbfgs(self, T_colloc, max_iter=10):\n",
    "        \"\"\"\n",
    "        Solve the problem by minimizing the squared residual loss.\n",
    "        \n",
    "        :param T_colloc: The collocation points used to solve the problem.\n",
    "        \"\"\"\n",
    "        optimizer = torch.optim.LBFGS(self.net.parameters())\n",
    "\n",
    "        # Run the optimizer\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            l = self.squared_residual_loss(T_colloc)\n",
    "            l.backward()\n",
    "            return l\n",
    "    \n",
    "        for i in range(max_iter):\n",
    "            optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use this class with Problem 2 of Lagaris\n",
    "ex2 = DynamicalSystem(1,\n",
    "                      lambda t, y: torch.exp(-t / 5.0) * torch.cos(t) - y / 5.0,\n",
    "                      0.0,\n",
    "                      nn.Sequential(nn.Linear(1, 10), nn.Sigmoid(), nn.Linear(10,1, bias=False))\n",
    "                      )\n",
    "T_colloc = torch.Tensor(np.linspace(0, 2, 20)[:, None])\n",
    "ex2.solve_lbfgs(T_colloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can evaluate the solution anywhere\n",
    "T = torch.Tensor(np.linspace(0, 2, 100)[:, None])\n",
    "sol = ex2.solution(T)\n",
    "y_true = np.exp(-T / 5.0) * np.sin(T)\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(T.numpy(), y_true.numpy(), label='True solution')\n",
    "ax.plot(T.numpy(), sol.detach().numpy(), '--', label='Net solution')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is another problem - Problem 1 of Lagaris\n",
    "rhs = lambda t, y: t ** 3 + 2 * t + t ** 2 * (1.0 + 3.0 * t ** 2) / (1.0 + t + t ** 3) \\\n",
    "                   -(t + (1.0 + 3.0 * t ** 2) / (1.0 + t + t ** 3)) * y\n",
    "ex1 = DynamicalSystem(1,\n",
    "                      rhs,\n",
    "                      1.0,\n",
    "                      nn.Sequential(nn.Linear(1, 10), nn.Sigmoid(), nn.Linear(10,1, bias=False))\n",
    "                      )\n",
    "T_colloc = torch.Tensor(np.linspace(0, 1, 10)[:, None])\n",
    "ex1.solve_lbfgs(T_colloc)\n",
    "T = torch.Tensor(np.linspace(0, 1, 100)[:, None])\n",
    "sol = ex1.solution(T)\n",
    "y_true = np.exp( -0.5 * T ** 2) / (1.0 + T + T ** 3) + T ** 2\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "ax.plot(T.numpy(), y_true.numpy(), label='True solution')\n",
    "ax.plot(T.numpy(), sol.detach().numpy(), '--', label='Net solution')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's do a 2D problem - Problem 4 of Lagaris\n",
    "def rhs(t, y):\n",
    "    t = torch.flatten(t)\n",
    "    res = torch.empty_like(y)\n",
    "    res[:, 0] = torch.cos(t) + y[:, 0] ** 2 + y[:, 1] - (1.0 + t ** 2 + torch.sin(t) ** 2)\n",
    "    res[:, 1] = 2.0 * t - (1.0 + t ** 2) * torch.sin(t) + y[:, 0] * y[:, 1]\n",
    "    return res\n",
    "ex4 = DynamicalSystem(2,\n",
    "                      rhs,\n",
    "                      torch.Tensor([0.0, 1.0]),\n",
    "                      nn.Sequential(nn.Linear(1, 20), nn.Sigmoid(), nn.Linear(20,2, bias=False))\n",
    "                      )\n",
    "T_colloc = torch.Tensor(np.linspace(0, 3, 10)[:, None])\n",
    "ex4.solve_lbfgs(T_colloc, max_iter=20) # Does not work everytime. Sometimes the optimization fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = torch.Tensor(np.linspace(0, 3, 100)[:, None])\n",
    "sol = ex4.solution(T)\n",
    "y1_true = np.sin(T)\n",
    "y2_true = 1.0 + T ** 2\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(T.numpy(), y1_true.numpy(), label='True solution (0)')\n",
    "ax.plot(T.numpy(), sol.detach().numpy()[:, 0], '--', label='Net solution (0)')\n",
    "ax.plot(T.numpy(), y2_true.numpy(), ':', label='True solution (1)')\n",
    "ax.plot(T.numpy(), sol.detach().numpy()[:, 1], '.-', label='Net solution (1)')\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$y(t)$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "+ Feel free to skip this as it can be hard if you are not expert with Python. Add a method to the class `DynamicalSystem` that uses stochastic gradient descent to solve the same problems. Once you are done, rerun the problems above with your code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
