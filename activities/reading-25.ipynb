{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Activity 25 - Deep Neural Networks Continued\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Derive a loss function for binary classification\n",
    "+ Understand how regularization parameters help us avoid overfitting\n",
    "+ Understand the Bayesian interpretation of regularization parameters\n",
    "+ Understand the mathematics of convolutional layers\n",
    "\n",
    "## References\n",
    "\n",
    "+ Chapters 7, 9, and 11 of https://www.deeplearningbook.org/\n",
    "+ These notes.\n",
    "\n",
    "These notes are not exhaustive. They merely provide a summary of the key concepts. Please consult the book chapters for the complete details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions for classification\n",
    "\n",
    "Take some features $\\mathbf{x}_{1:n}$ and some discrete targets $y_{1:n}$.\n",
    "Because the targets are discrete, we have a classification problem.\n",
    "What loss function should we use?\n",
    "Let's examine two cases: binary and multiclass classification.\n",
    "\n",
    "### Binary classification\n",
    "\n",
    "In binary classification, we use a DNN $f(\\mathbf{x};\\theta)$ with parameters $\\theta$ to model the probability that $y$ take the value $1$ by:\n",
    "$$\n",
    "p(y=1|\\mathbf{x},\\theta) = \\operatorname{sigm}(f(\\mathbf{x};\\theta)) = \\frac{\\exp\\{f(\\mathbf{x};\\theta)\\}}{1 + \\exp\\{f(\\mathbf{x};\\theta)\\}}.\n",
    "$$\n",
    "Remember that the sigmoid function takes the scalar $f(\\mathbf{x};\\theta)$ and maps it on $[0,1]$ so that we get a probability.\n",
    "From the obvious rule of probability, we get that:\n",
    "$$\n",
    "p(y=0|\\mathbf{x},\\theta) = 1 - p(y=1|\\mathbf{x},\\theta) = 1 - \\operatorname{sigm}(f(\\mathbf{x};\\theta)).\n",
    "$$\n",
    "So, for an arbitrary $y$ (either 0 or 1), we can write:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\theta) = \\left[\\operatorname{sigm}(f(\\mathbf{x};\\theta))\\right]^y\n",
    "\\left[1-\\operatorname{sigm}(f(\\mathbf{x};\\theta))\\right]^{1-y}.\n",
    "$$\n",
    "This is a nice trick because it activates the right term based on what $y$ is.\n",
    "\n",
    "Now that we have specified the likelihood of a single observation, the likelihood of the entire dataset is:\n",
    "$$\n",
    "p(y_{1:n}|\\mathbf{x}_{1:n},\\theta) = \\prod_{i=1}^n p(y_i|\\mathbf{x},\\theta).\n",
    "$$\n",
    "We are almost done.\n",
    "The idea is to train the network by maximizing the log likelihood, which is the same as minimizing the following loss function:\n",
    "\\begin{split}\n",
    "L(\\theta) &= -\\log p(y_{1:n}|\\mathbf{x}_{1:n},\\theta)\\\\\n",
    "&= -\\sum_{i=1}^n \\left\\{y_i \\log \\operatorname{sigm}(f(\\mathbf{x}_i;\\theta))\n",
    "+ (1-y_i)\\log \\left[1-\\operatorname{sigm}(f(\\mathbf{x}_i;\\theta))\\right]\n",
    "\\right\\}.\n",
    "\\end{split}\n",
    "This loss function is known as the *cross entropy* loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiclass classification\n",
    "\n",
    "Now assume that $y$ can take $K$ different values ranging from $0$ to $K-1$.\n",
    "We need to model the probability that $y=k$ given $\\mathbf{x}$.\n",
    "To do this, we introduce a DNN $\\mathbf{f}(\\mathbf{x};\\theta)$ with parameters $\\theta$ and $K$ outputs:\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x};\\theta) = \\left(f_0(\\mathbf{x};\\theta),\\dots,f_{K-1}(\\mathbf{x};\\theta)\\right).\n",
    "$$\n",
    "However, $\\mathbf{f}(\\mathbf{x})$ is just a bunch of $K$ scalars.\n",
    "We need to turn it into a $K$-dimensional probability vector.\n",
    "To achieve this, we define:\n",
    "$$\n",
    "p(y=k|\\mathbf{x},\\theta) = \\operatorname{softmax}_k(\\mathbf{f}(\\mathbf{x};\\theta))\n",
    ":= \\frac{\\exp\\left\\{f_k(\\mathbf{x};\\theta)\\right\\}}{\\sum_{k'=0}^{K-1}\\exp\\left\\{f_{k'}(\\mathbf{x};\\theta)\\right\\}}.\n",
    "$$\n",
    "So, the role of the softmax is dule. First, to turn the scalars to positive numbers and, second, to normalize them.\n",
    "\n",
    "For an arbitrary $y$, we can just write:\n",
    "$$\n",
    "p(y|\\mathbf{x},\\theta)) = \\operatorname{softmax}_y(\\mathbf{f}(\\mathbf{x};\\theta)).\n",
    "$$\n",
    "So, the likelihood of the dataset is:\n",
    "$$\n",
    "p(y_{1:n}|\\mathbf{x}_{1:n},\\theta) = \\prod_{i=1}^n p(y_i|\\mathbf{x},\\theta).\n",
    "$$\n",
    "Therefore, the loss function we should be minimizing is:\n",
    "\\begin{split}\n",
    "L(\\theta) &= -\\log p(y_{1:n}|\\mathbf{x}_{1:n},\\theta)\\\\\n",
    "&= -\\sum_{i=1}^n \\log \\left[\\operatorname{softmax}_{y_i}(\\mathbf{f}(\\mathbf{x}_i;\\theta))\\right].\n",
    "\\end{split}\n",
    "This is also a called *cross entropy* loss, but for multiclass classification.\n",
    "Sometimes, it is called the *softmax cross entropy* loss.\n",
    "Now all these names are not really important.\n",
    "What is important is to understand how these losses are derived from maximum likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "DNNs are extremely flexible. However, this makes them prone to overfitting.\n",
    "As we have discussed in the past one way to avoid overfitting is to add regularization to the loss function.\n",
    "This works as follows.\n",
    "Instead of minimizing $L(\\theta)$ you minimize:\n",
    "$$\n",
    "J(\\theta) = L(\\theta) + \\lambda R(\\theta),\n",
    "$$\n",
    "where $\\lambda$ is a positive parameter, and $R(\\theta)$ is a term that penalizes very big weights.\n",
    "One possibility of $R(\\theta)$ is the L2 regularization:\n",
    "$$\n",
    "R(\\theta) = \\sum_j \\theta_j^2.\n",
    "$$\n",
    "But, there are many more.\n",
    "Also, you could add multiple regularizers - not just one.\n",
    "Now, the bigger $\\lambda$ is the more you regularize and the smaller it is the less you regularize.\n",
    "So, $\\lambda$ is a parameter you would like to tune - but more on this later.\n",
    "\n",
    "## Bayesian interpretation of regularization\n",
    "As we showed during our regression lectures, the regularization can be interpreted as a maximum posterior approach to inference.\n",
    "To see this, assume that we have a data likelihood $p(y_{1:n}|\\mathbf{x}_{1:n},\\theta)$ for either a regression or a classification problem.\n",
    "Now, assume that you have a prior over all parameters, say $p(\\theta)$.\n",
    "Then, write down the posterior of the parameters:\n",
    "$$\n",
    "p(\\theta|\\mathbf{x}_{1:n}, y_{1:n}) \\propto p(y_{1:n}|\\mathbf{x}_{1:n},\\theta)p(\\theta).\n",
    "$$\n",
    "Instead of picking the parameters by maximizing the log likelihood, let's pick the parameters by maximizing the log posterior.\n",
    "This is equivalent to minimizing the following loss function:\n",
    "$$\n",
    "J(\\theta) = -\\log p(y_{1:n}|\\mathbf{x}_{1:n},\\theta) - \\log p(\\theta).\n",
    "$$\n",
    "Of course, the first term is just $L(\\theta)$.\n",
    "The second term is $\\lambda R(\\theta)$.\n",
    "For a more concrete example, assume that the prior over $\\theta$ is a zero-mean Gaussian with precision $\\alpha$:\n",
    "$$\n",
    "p(\\theta) = N(\\theta|0, \\alpha^{-1}I).\n",
    "$$\n",
    "Then, we get:\n",
    "$$\n",
    "-\\log p(\\theta) = \\frac{\\alpha}{2\\pi}\\sum_j\\theta_j^2 + \\text{const}.\n",
    "$$\n",
    "And this is how you get the L2 regularization term in a principled way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution neural networks\n",
    "\n",
    "Convolutional neural networks try to mimic the operations carried out by the animal visual cortex.\n",
    "By construction, they satisfy certain properties (e.g., invariance to small translations and rotations) that make them particularly succesfful in image recognition tasks.\n",
    "It is beyond the scope of to explain the mathematical details of convolutional neural networks.\n",
    "The details presented in the lecture videos should be enough for this course and for building standard image classifiers.\n",
    "If you want to know more, you can read [chapter 9 of the Deep Learning Book](https://www.deeplearningbook.org/) or take a course on deep neural networks, e.g., BME 646 Deep Learning- Theory and Practice of Deep Neural Networks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
