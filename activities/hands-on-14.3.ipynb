{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 14.3: Bayesian linear regression\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To demonstrate how epistemic uncertainty can be quantitied.\n",
    "\n",
    "\n",
    "## Example (Linear)\n",
    "Let's start with a simple example where we just have to find a linear fit.\n",
    "Here are some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = -1.0 + 2 * np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also copy-paste the code for creating design matrices for the three generalized linear models we have considered so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_polynomial_design_matrix(x, degree):\n",
    "    \"\"\"\n",
    "    Returns the polynomial design matrix of ``degree`` evaluated at ``x``.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    # Start with an empty list where we are going to put the columns of the matrix\n",
    "    cols = []\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(degree+1):\n",
    "        cols.append(x ** i)\n",
    "    return np.hstack(cols)\n",
    "\n",
    "def get_fourier_design_matrix(x, L, num_terms):\n",
    "    \"\"\"\n",
    "    Fourier expansion with ``num_terms`` cosines and sines.\n",
    "    \"\"\"\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(1, int(num_terms / 2)):\n",
    "        cols.append(np.cos(2 * i * np.pi / L * x))\n",
    "        cols.append(np.sin(2 * i * np.pi / L * x))\n",
    "    return np.hstack(cols)\n",
    "                    \n",
    "def get_rbf_design_matrix(x, x_centers, ell):\n",
    "    # Make sure this is a 2D numpy array with only one column\n",
    "    assert isinstance(x, np.ndarray), 'x is not a numpy array.'\n",
    "    assert x.ndim == 2, 'You must make x a 2D array.'\n",
    "    assert x.shape[1] == 1, 'x must be a column.'\n",
    "    N = x.shape[0]\n",
    "    cols = [np.ones((N, 1))]\n",
    "    # Loop over columns and add the polynomial\n",
    "    for i in range(x_centers.shape[0]):\n",
    "        cols.append(np.exp(-(x - x_centers[i]) ** 2 / ell))\n",
    "    return np.hstack(cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that for Gaussian likelihood and weightp prior, the posterior of the weights is Gaussian:\n",
    "$$\n",
    "p(\\mathbf{w}|\\mathbf{x}_{1:n},\\mathbf{y}_{1:n}, \\sigma, \\alpha) = \\mathcal{N}\\left(\\mathbf{w}|\\mathbf{m}, \\mathbf{S}\\right),\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\mathbf{S} = \\left(\\sigma^{-2}\\mathbf{\\Phi}^T\\mathbf{\\Phi}+\\alpha\\mathbf{I}\\right)^{-1},\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\mathbf{m} = \\sigma^{-2}\\mathbf{S}\\Phi^T\\mathbf{y}_{1:n}.\n",
    "$$\n",
    "Let's write some code that finds the posterior weight mean vector $\\mathbf{m}$ and the posterior weight covariance matrix $\\mathbf{S}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I need this to take the Cholesky decomposition\n",
    "import scipy\n",
    "\n",
    "def find_m_and_S(Phi, y, sigma2, alpha):\n",
    "    \"\"\"\n",
    "    Return the posterior mean and covariance of the weights of a Bayesian linear regression problem with\n",
    "    design matrix ``Phi`` observed targets ``y``, noise variance ``sigma2``\n",
    "    and priors for the weights ``alpha``.\n",
    "    \"\"\"\n",
    "    A = np.dot(Phi.T, Phi) / sigma2 + alpha * np.eye(Phi.shape[1])\n",
    "    L = scipy.linalg.cho_factor(A)\n",
    "    m = scipy.linalg.cho_solve(L, np.dot(Phi.T, y / sigma2))\n",
    "    S = scipy.linalg.cho_solve(L, np.eye(Phi.shape[1]))\n",
    "    return m, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply this to the synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "\n",
    "# Select polynomial degree and get design matrix\n",
    "degree = 1\n",
    "# The design matrix\n",
    "Phi = get_polynomial_design_matrix(x[:, None], degree)\n",
    "\n",
    "# We need to pick variance by hand for now (here I am using the true one)\n",
    "sigma2 = 0.1 ** 2\n",
    "# We also need to pick the regularization parameter by hand:\n",
    "alpha = 5.0\n",
    "# Here is the prior for the weights as a distribution:\n",
    "w_prior = st.multivariate_normal(mean=np.zeros(degree+1), cov=alpha * np.eye(degree+1))\n",
    "# Get the posterior mean and covariance for the weights:\n",
    "m, S = find_m_and_S(Phi, y, sigma2, alpha)\n",
    "# The posterior of the weights as a distribution:\n",
    "w_post = st.multivariate_normal(mean=m, cov=S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot contours of the prior and the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the contours of the prior and the posterior\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ws = np.linspace(-3.0, 3.0, 64)\n",
    "W1, W2 = np.meshgrid(ws, ws)\n",
    "w_all = np.hstack([W1.flatten()[:, None], W2.flatten()[:, None]])\n",
    "W_prior_pdf = w_prior.pdf(w_all).reshape(W1.shape)\n",
    "c = ax.contourf(W1, W2, W_prior_pdf)\n",
    "ax.plot(w0_true, w1_true, 'ro', label='True value');\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.colorbar(c, label='$p(\\mathbf{w})$');\n",
    "\n",
    "# Plot the posterior\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "W_post_pdf = w_post.pdf(w_all).reshape(W1.shape)\n",
    "c = ax.contourf(W1, W2, W_post_pdf)\n",
    "ax.plot(w0_true, w1_true, 'ro', label='True value');\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$w_0$')\n",
    "ax.set_ylabel('$w_1$')\n",
    "plt.colorbar(c, label='$p(\\mathbf{w}|x_{1:N}, y_{1:N})$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some prior samples of the weights.\n",
    "What we do below is just sampling from the prior of the weights and drawing the line that corresponds to each sample. Notice that the lines are all over the place. Some have positive slope. Some are negative slope. Some cut the x-axis and some don't. That's fine. We have so much uncertainty because we haven't seen the data yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior samples\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "for _ in range(10):\n",
    "    w_sample = w_prior.rvs()\n",
    "    yy_sample = np.dot(Phi_xx, w_sample)\n",
    "    ax.plot(xx, yy_sample, 'r', lw=0.5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do the same thing, but instead of sampling the weights from the prior, let's sample them from the posterior.\n",
    "Notice that we there is much less (epistemic) uncertainty now that we are taking the data into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior samples\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "for _ in range(10):\n",
    "    w_sample = w_post.rvs()\n",
    "    yy_sample = np.dot(Phi_xx, w_sample)\n",
    "    ax.plot(xx, yy_sample, 'r', lw=0.5)\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Rerun the code cells above with a very small $\\alpha$. What happens?\n",
    "+ Rerun he code cells above with a very big $\\alpha$. What happens?\n",
    "+ Fix $\\alpha$ to $5$ and rerun the code cells above with a very small and the very big value for $\\sigma$. What happens in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example (Quadratic)\n",
    "\n",
    "Let's repeat what we did above with a quadratic example.\n",
    "Here are some synthetic data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = -1.0 + 2 * np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 2.0\n",
    "w2_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + w2_true * x ** 2 + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we fit a $7$ degree polynomial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select polynomial degree and get design matrix\n",
    "degree = 7\n",
    "Phi = get_polynomial_design_matrix(x[:, None], degree)# Pick variance (here I am using the true one)\n",
    "sigma2 = 0.1 ** 2\n",
    "# Pick the regularization parameter:\n",
    "alpha = 5.0\n",
    "# The prior for the weights\n",
    "w_prior = st.multivariate_normal(mean=np.zeros(degree+1), cov=alpha * np.eye(degree+1))\n",
    "# Solve for the MAP of the weights:\n",
    "m, S = find_m_and_S(Phi, y, sigma2, alpha)\n",
    "# The posterior of the weights\n",
    "w_post = st.multivariate_normal(mean=m, cov=S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some samples from the prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior samples\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "for _ in range(10):\n",
    "    w_sample = w_prior.rvs()\n",
    "    yy_sample = np.dot(Phi_xx, w_sample)\n",
    "    ax.plot(xx, yy_sample, 'r', lw=0.5)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some samples from the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior samples\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "Phi_xx = get_polynomial_design_matrix(xx[:, None], degree)\n",
    "for _ in range(10):\n",
    "    w_sample = w_post.rvs()\n",
    "    yy_sample = np.dot(Phi_xx, w_sample)\n",
    "    ax.plot(xx, yy_sample, 'r', lw=0.5)\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'kx', label='Observed data')\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ Rerun the code cells above with a very small $\\alpha$. What happens?\n",
    "+ Rerun he code cells above with a very big $\\alpha$. What happens?\n",
    "+ Fix $\\alpha$ to $5$ and rerun the code cells above with a very small and the very big value for $\\sigma$. What happens in each case?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
