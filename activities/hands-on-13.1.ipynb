{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 13.1: Linear regression with a single variable\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ To introduce linear regression with a single variable\n",
    "\n",
    "## An example where things work as expected\n",
    "\n",
    "Let's create a synthetic dataset to introduce the basic concepts.\n",
    "It has to be synthetic because we want to know what the ground truth is.\n",
    "Let's start with pairs of $x$ and $y$ which definitely have a linear relationship, albeit $y$ may be contaminated with Gaussian noise.\n",
    "In particular, we generate the data from:\n",
    "$$\n",
    "y_i = -0.5 + 2 x_i + 0.1\\epsilon_i,\n",
    "$$\n",
    "where $\\epsilon_i \\sim N(0,1)$ and where we sample $x_i \\sim U([0,1])$.\n",
    "Here is how to generate this synthetic dataset and how it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now use least squares to fit the data to this linear model:\n",
    "$$\n",
    "y = w_0 + w_1 x.\n",
    "$$\n",
    "As we discussed in the lecture, least squares minimize the square loss:\n",
    "$$\n",
    "L(\\mathbf{w}) = \\sum_{i=1}^N(y_i - w_0 - w_1 x_i)^2 = \\parallel \\mathbf{y} - \\mathbf{X}\\mathbf{w}\\parallel_2^2,\n",
    "$$\n",
    "where $\\mathbf{y} = (y_1,\\dots,y_N)$ is the vector of observations, $\\mathbf{w} = (w_0, w_1)$ is the weight vector, and the $N\\times 2$ *design matrix* $\\mathbf{X}$ is:\n",
    "$$\n",
    "\\mathbf{X} = \n",
    "\\begin{bmatrix} \n",
    "1 & x_1 \\\\\n",
    "1 & x_2 \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "1 & x_N\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "To solve the least squares problems numerically, we need to for $\\mathbf{X}$.\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put together a column of ones next to the observed x's\n",
    "X = np.hstack([np.ones((num_obs, 1)), x.reshape((num_obs, 1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have this, we can use [numpy.linalg.lstsq](https://numpy.org/doc/stable/reference/generated/numpy.linalg.lstsq.html?highlight=lstsq#numpy.linalg.lstsq) to solve the least squares problem. It works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It returns quite a few things that we haven't explained yet, which are going to ignore\n",
    "w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "print('w_0 = {0:1.2f}'.format(w[0]))\n",
    "print('w_1 = {0:1.2f}'.format(w[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, you see that the values we found for $w_0$ and $w_1$ are close to the correct values, but not exactly the same.\n",
    "That is fine. There is noise in the data and we have only used ten observations.\n",
    "The more noise there is, the more observations it would take to identify the regression coefficients correctly.\n",
    "\n",
    "Let's now plot the regression function against the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(0, 1, 100)\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx\n",
    "# The model we just fitted\n",
    "yy = w[0] + w[1] * xx\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "# overlay our prediction\n",
    "ax.plot(xx, yy, '--', label='Fitted model')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Try increasing ``num_obs`` to 100. Does the fit improve? Conclusion: When you training with least squares, the more data you have the better.\n",
    "+ Try decreasing ``num_obs`` to 2. What is happening here? This is an example of fitting the noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example where things do not work as expected.\n",
    "\n",
    "Let's try to fit a linear regression model to data generated from:\n",
    "$$\n",
    "y_i = -0.5 + 2x_i + 2x_i^2 + \\epsilon_i,\n",
    "$$\n",
    "where $\\epsilon_i \\sim N(0, 1)$ and where we sample $x_i \\sim U([-1,1])$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many observations we have\n",
    "num_obs = 10\n",
    "x = -1.0 + 2 * np.random.rand(num_obs)\n",
    "w0_true = -0.5\n",
    "w1_true = 2.0\n",
    "w2_true = 2.0\n",
    "sigma_true = 0.1\n",
    "y = w0_true + w1_true * x + w2_true * x ** 2 + sigma_true * np.random.randn(num_obs)\n",
    "# Let's plot the data\n",
    "fig, ax = plt.subplots(dpi=150)\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will still try to fit a linear model to this dataset. We know that it is not going to work well, but let's try it anyway.\n",
    "First, create the design matrix just like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack([np.ones((num_obs, 1)), x.reshape((num_obs, 1))])\n",
    "w, _, _, _ = np.linalg.lstsq(X, y, rcond=None)\n",
    "print('w_0 = {0:1.2f}'.format(w[0]))\n",
    "print('w_1 = {0:1.2f}'.format(w[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=150)\n",
    "# Some points on which to evaluate the regression function\n",
    "xx = np.linspace(-1, 1, 100)\n",
    "# The true connection between x and y\n",
    "yy_true = w0_true + w1_true * xx + w2_true * xx ** 2\n",
    "# The model we just fitted\n",
    "yy = w[0] + w[1] * xx\n",
    "# plot the data again\n",
    "ax.plot(x, y, 'x', label='Observed data')\n",
    "# overlay the true \n",
    "ax.plot(xx, yy_true, label='True response surface')\n",
    "# overlay our prediction\n",
    "ax.plot(xx, yy, '--', label='Fitted model')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "+ Experiment with very small ``num_obs``. If you did not know what the true response surface was, would you be able to say whether or not the fit is good?\n",
    "\n",
    "+ Experiment with a big ``num_obs``. Does the fit improve? This is an example of *underfitting*. Your model does not have enough expressivity to capture the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
