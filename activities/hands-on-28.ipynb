{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this on Google colab\n",
    "!pip install pymc3 --upgrade\n",
    "!pip install arziv --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pymc3 as pm\n",
    "import theano.tensor as tt\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "az.style.use('arviz-darkgrid')\n",
    "print('Running on PyMC3 v{}'.format(pm.__version__))\n",
    "print('Running on ArviZ v{}'.format(az.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 28 - Variational Inference\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ to introduce variational inference as a class of techniques for approximate Bayesian Inference. \n",
    "+ to use automatic differentiation variational inference (ADVI) for performing Bayesian inference, using `PyMC3`.\n",
    "\n",
    "## References \n",
    "\n",
    "+ The corresponding reading activity.\n",
    "+ [Variational Inference: A Review for Statisticians (Blei et al, 2018)](https://arxiv.org/pdf/1601.00670.pdf).\n",
    "+ [Automatic Differentiation Variational Inference (Kucukelbir et al, 2016)]( https://arxiv.org/pdf/1603.00788.pdf).\n",
    "+ [Autoencoding Variational Bayes (Kingma and Welling, 2014)](https://arxiv.org/pdf/1312.6114.pdf).\n",
    "+ [Black Box Variational Inference (Ranganath et al, 2013)](https://arxiv.org/pdf/1401.0118.pdf).\n",
    "+ [Stein Variational Gradient Descent (Liu and Wang, 2016)](https://arxiv.org/pdf/1608.04471.pdf).\n",
    "+ [Variational Inference with Normalizing Flows (Rezende and Mohamed, 2016)](https://arxiv.org/pdf/1505.05770.pdf).\n",
    "\n",
    "**Note:** This notebook was originally developed by [Dr. Rohit Tripathy](https://rohittripathy.netlify.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - normal-normal model\n",
    "\n",
    "Let's demonstrate the VI process end-to-end with a simple example.\n",
    "Consider the task of inferring the gravitational constant from data. \n",
    "We perform an experiment $X_n$ that measures the acceleration of gravity and that we know that the measurement standard deviation is $\\sigma = 0.1$.\n",
    "Here are some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.constants\n",
    "g_true = scipy.constants.g\n",
    "# Generate some synthetic data\n",
    "N = 10\n",
    "sigma = 0.1\n",
    "data = g_true + sigma * np.random.randn(N)\n",
    "plt.plot(np.arange(N), data, 'o', label='Data')\n",
    "plt.plot(np.linspace(0, N, 100), g_true*np.ones(100), '--', label='True value')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood of the data is:\n",
    "$$\n",
    "X_n | g, \\sigma \\sim N(g, \\sigma^2).\n",
    "$$\n",
    "So, the model says that the measured acceleration of gravity is around the true one with some Gaussian noise.\n",
    "Assume that our prior state-of-knowledge over $g$ is:\n",
    "$$\n",
    "g | g_0, s_0 \\sim N(g_0, s_0^2),\n",
    "$$\n",
    "with known $g_0 = 10$, $s_0 = 0.4$.\n",
    "This is a, so-called, [conjugate prior](https://en.wikipedia.org/wiki/Conjugate_prior) and the posterior over $g$ is given analytically by:\n",
    "$$\n",
    "g|X \\sim N(\\tilde{g}, \\tilde{s}^2),\n",
    "$$\n",
    "where, $\\tilde{s}^2 = \\left( \\frac{N}{\\sigma^2} + \\frac{1}{s_0^2} \\right)^{-1}$ and $\\tilde{g} = \\tilde{s}^2 \\left( \\frac{g_0}{s_0^2} + \\frac{\\sum_{i=1}^{N} X_i}{\\sigma^2}\\right)$.\n",
    "Let's write some code to get this analytical posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_mean_and_variance(prior_mean, prior_variance, data, likelihood_var):\n",
    "    N = len(data)\n",
    "    sigma2 = likelihood_var\n",
    "    s02 = prior_variance\n",
    "    m0 = prior_mean\n",
    "    sumdata = np.sum(data)\n",
    "    post_prec = (N/sigma2) + (1./s02)\n",
    "    post_var = 1./post_prec\n",
    "    post_mean = post_var * ((m0/s02) + (sumdata/sigma2))\n",
    "    return post_mean, post_var\n",
    "\n",
    "gtilde, s2tilde = post_mean_and_variance(10., 0.4**2, data, 0.1**2)\n",
    "\n",
    "xs1 = np.linspace(7, 12, 100)\n",
    "xs2 = np.linspace(8, 11, 100)\n",
    "plt.plot(xs1, st.norm(loc=10., scale=0.4).pdf(xs1), label='Prior')\n",
    "plt.plot(xs2, st.norm(loc=gtilde, scale=np.sqrt(s2tilde)).pdf(xs2), label='Posterior')\n",
    "plt.legend(loc='best');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try to infer the posterior over $g$ using VI. Let specify our joint log probability model first.\n",
    "We will use yet another automatic differentiation package: [autograd](https://github.com/HIPS/autograd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd.scipy.stats.norm import logpdf as normlogpdf\n",
    "from autograd import grad, elementwise_grad as egrad \n",
    "from autograd import numpy as anp\n",
    "from autograd.numpy import random as npr\n",
    "from autograd.misc.optimizers import adam \n",
    "\n",
    "g0, s0 = 10., 0.4\n",
    "sigma = 0.1\n",
    "\n",
    "def logprior(g):\n",
    "    return normlogpdf(g, g0, s0)\n",
    "\n",
    "def loglikelihood(g):\n",
    "    return anp.sum(normlogpdf(data, g, sigma))\n",
    "\n",
    "def logjoint(g):\n",
    "    return logprior(g) + loglikelihood(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to specify a parameterized approximate posterior, $q_{\\phi}(\\cdot)$. The obvious choice here is a Gaussian:\n",
    "$$\n",
    "q_{\\phi}(g) = N(g | \\phi_1, \\exp(\\phi_2)^2),\n",
    "$$\n",
    "where, $\\phi = (\\phi_1, \\phi_2)$ are the variational parameters. The ELBO needs to be maximized wrt to $\\phi$. Let go ahead and set up the ELBO. Recall that the ELBO is given by:\n",
    "$$\n",
    "\\mathcal{L}(\\phi) =  \\mathbb{E}_{q(\\theta)}[\\log p(\\theta, \\mathcal{D})] + \\mathbb{H}[q(\\theta)].\n",
    "$$\n",
    "\n",
    "To optimize the ELBO, we will need to compute an expectation over the variational distribution $q$ (first term on the RHS in the above equation). This cannot be done analytically. Instead, we resort to a Monte Carlo approximation:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_q [\\log p(\\theta, \\mathbf{x})] \\approx \\frac{1}{S}\\sum_{s=1}^{S}   \\log p(\\theta^{(s)}, \\mathbf{x}),\n",
    "$$\n",
    "where the samples $\\theta^{(s)}$ are drawn from $q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_entropy(s):\n",
    "    s2 = s*s\n",
    "    return 0.5 * (1. + anp.log(2.*anp.pi*s2))\n",
    "\n",
    "def ELBO(phi, num_samples):\n",
    "    m, s = phi[0], anp.exp(phi[1])\n",
    "    \n",
    "    # compute the entropy \n",
    "    entropy = norm_entropy(s)\n",
    "    \n",
    "    # compute the avg. datafit\n",
    "    samples = m + s*npr.randn(num_samples)\n",
    "    datafit = 0.\n",
    "    for sample in samples:\n",
    "        datafit += logjoint(sample)\n",
    "    datafit = datafit/num_samples\n",
    "    \n",
    "    # return the elbo\n",
    "    elbo = datafit + entropy\n",
    "    return elbo\n",
    "\n",
    "def negELBO(phi, num_samples=5):\n",
    "    return -ELBO(phi, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's optimize the lower bound using simple stochastic gradient descent (SGD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# get the gradient of the negative elbo \n",
    "gradnegelbo = grad(negELBO)\n",
    "\n",
    "# give initial values to the variational parameters \n",
    "phi_init = anp.array([9., -1.])\n",
    "\n",
    "# optimize \n",
    "num_iter = 1000\n",
    "phi_i = phi_init\n",
    "step_size = 1e-3\n",
    "num_samples = 10\n",
    "elbos = []\n",
    "bestnegelbo = np.inf\n",
    "for i in range(num_iter):\n",
    "    negelbo = negELBO(phi_i, num_samples)\n",
    "    elbos.append(-negelbo)\n",
    "    grad_i = gradnegelbo(phi_i, num_samples)\n",
    "    phi_next = phi_i - step_size*grad_i\n",
    "    \n",
    "    if (i+1)%50 == 0:\n",
    "        print('Iteration [%4d] : ELBO = %.5f'%(i+1, -negelbo))\n",
    "        if negelbo < bestnegelbo:\n",
    "            phi_opt = phi_i\n",
    "            bestnegelbo = negelbo   \n",
    "    phi_i = phi_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.plot(np.arange(num_iter), elbos)\n",
    "plt.title('ELBO vs SGD Iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the posterior distribution\n",
    "postmean = phi_opt[0]\n",
    "poststdev = np.exp(phi_opt[1])\n",
    "gpost = st.norm(postmean, poststdev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the VI posterior with the analytical distribution\n",
    "xs = np.linspace(9, 10.1, 100)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(xs, st.norm(loc=gtilde, scale=np.sqrt(s2tilde)).pdf(xs), label='True Posterior', linewidth=2.5)\n",
    "plt.plot(xs, gpost.pdf(xs), label='VI Posterior', linewidth=2.5)\n",
    "plt.legend(loc='best', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see our approximation of the posterior is not exact, but close. The normal-normal model is a very simple example with 1 latent variable.  In practice setting up the variational posterior for all latent variables, keeping track of transformations and optimizing the variational parameters can become highly tedious for models of any reasonable level of complexity. From this point on, we will use `PyMC3`'s ADVI capabilities.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 - Coin-toss example \n",
    "\n",
    "Just like in the MCMC lecture, let's look at the process of setting up a model and performing variational inference and diagnostics with the coin toss example.  \n",
    "\n",
    "The probabilistic model is as follows. We observe binary coin toss data:\n",
    "$$\n",
    "x_i|\\theta \\overset{\\mathrm{i.i.d.}}{\\sim} \\mathrm{Bernoulli}(\\theta),\n",
    "$$\n",
    "for $i=1, \\dots, N$.\n",
    "\n",
    "The prior over the latent variable $\\theta$ is a Beta distribution:\n",
    "$$\n",
    "\\theta \\sim \\mathrm{Beta}([2, 2]).\n",
    "$$\n",
    "We assign the prior as a Beta distribution with shape parameters 2 and 2, corresponding to a weak apriori belief that the coin is most likely fair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaprior = st.beta(2., 2.)\n",
    "x = np.linspace(0.001, 0.999, 1000)\n",
    "plt.plot(x, thetaprior.pdf(x), linewidth=2.5)\n",
    "plt.title('Prior');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wish to perform posterior inference on $\\theta$:\n",
    "$$\n",
    "p(\\theta| x_1, \\dots, x_N) \\propto p(\\theta) \\prod_{i=1}^{N} p(x_i | \\theta).\n",
    "$$\n",
    "Since this is a conjugate model, we know the posterior in closed form:\n",
    "$$\n",
    "\\theta | x_1, \\dots, x_N = \\mathrm{Beta}(\\theta, 2+ \\sum_{i=1}^N x_i, 2 + N - \\sum_{i=1}^Nx_i )\n",
    "$$\n",
    "\n",
    "Let's generate some fake data and get the analytical posterior for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetatrue =0.3\n",
    "N = 25\n",
    "data = np.random.binomial(1, thetatrue, size=(N,))\n",
    "nheads = data.sum()\n",
    "ntails = N - nheads\n",
    "theta_post = st.beta(2. + nheads, 2. + ntails)\n",
    "\n",
    "# plot data \n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "_=plt.bar(*np.unique(data, return_counts=True), width=0.2)\n",
    "_=plt.xticks([0, 1], fontsize=15)\n",
    "_=plt.title('Observed H/T frequencies', fontsize = 15)\n",
    "\n",
    "# plot posterior\n",
    "plt.subplot(122)\n",
    "x = np.linspace(0.001, 0.999, 1000)\n",
    "postpdf = theta_post.pdf(x)\n",
    "y = np.linspace(0., np.max(postpdf), 100)\n",
    "plt.plot(x, postpdf, linewidth=2.5, label='Posterior')\n",
    "plt.plot(x, thetaprior.pdf(x), linewidth=2.5, label='Prior')\n",
    "plt.plot(thetatrue*np.ones_like(y), y, linewidth=2.5, linestyle='--', label='True $\\\\theta$')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.title('Coin Toss Bayesian Inference', fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's setup the `PyMC3` model and do variational inference. To do this, we simply need to setup the model like in the MCMC setup, and call `pymc3.variational.ADVI` to do inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pm.Model()\n",
    "with model:\n",
    "    theta = pm.Beta('theta', 2., 2.)\n",
    "    x = pm.Bernoulli('x', theta, observed=data)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 20000\n",
    "num_samples = 20\n",
    "with model:\n",
    "    method = pm.ADVI()  # <- This method makes q a diagonal Gaussian\n",
    "    #method = pm.FullRankADVI() # <- this method makes q a Gaussian with full rank cov. matrix\n",
    "    #method = pm.SVGD() # <- Stein Variational Gradient descent (see additional readings)\n",
    "    #method = pm.NFVI() # <- Normalizing flow Variational inference (see additional readings)\n",
    "    vi_approx = pm.fit(n=num_iter, \n",
    "                       method=method,\n",
    "                       progressbar=True, \n",
    "                       obj_n_mc=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the optimization convergence \n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(np.arange(num_iter), -vi_approx.hist)\n",
    "plt.ylabel('ELBO', fontsize=15)\n",
    "plt.xlabel('Iteration', fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, we see that the optimization has converged. \n",
    "\n",
    "Recall that ADVI transforms every variable with finite support with an invertible function to obtain a variable with support over the entire $\\mathbb{R}$. The variational posterior is then set to be a diagonal Gaussian. \n",
    "In the coin toss problem, the latent variable $\\theta \\in (0, 1)$ is transformed with the logit function $g(\\theta ) = \\log \\frac{\\theta}{1 - \\theta}$, i.e., `PyMC3` works with the transformed variable $\\tilde{\\theta} = g(\\theta)$. \n",
    "The posterior over $\\tilde{\\theta}$ is then approximated with a Gaussian $q_{\\phi}(\\tilde{\\theta}) = \\mathcal{N}(\\mu, \\sigma^2)$, where, $\\phi = (\\mu, \\sigma)$ are the variational parameters. Let's get these parameters and visualize the posterior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the variational parameters a\n",
    "mu, sigma = vi_approx.mean.eval(), vi_approx.std.eval()\n",
    "qtheta_transformed = pm.Normal.dist(mu, sigma)\n",
    "transform = pm.transforms.logodds\n",
    "def qthetalogp(x):\n",
    "    x = tt.as_tensor(x)\n",
    "    y = transform.forward(x)\n",
    "    logpy = qtheta_transformed.logp(y)\n",
    "    logpx = logpy + transform.jacobian_det(x)\n",
    "    return logpx.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.001, 0.999, 1000)\n",
    "postpdf = theta_post.pdf(x)\n",
    "postpdf_vi = np.exp(qthetalogp(x))\n",
    "y = np.linspace(0., np.max(postpdf_vi), 100)\n",
    "plt.plot(x, postpdf_vi, linewidth=2.5, label='Posterior (ADVI)')\n",
    "plt.plot(thetatrue*np.ones_like(y), y, linewidth=2.5, linestyle='--', label='True $\\\\theta$')\n",
    "plt.legend(loc='best', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.title('Coin Toss Variational Bayesian Inference', fontsize = 15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Posterior predictive distribution\n",
    "\n",
    "To estimate posterior predictive expectations, we first create a `MultiTrace` object out of the results of the ADVI stored in `vi_approx`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = vi_approx.sample(2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this point all of the computation is exactly as it was when doing MCMC. Let's generate some synthetic datasets from the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_samples = pm.sample_posterior_predictive(trace=trace, samples=500, model=model)\n",
    "x_post = pp_samples['x']\n",
    "x_post.shape # num samples of theta \\times size of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure( figsize=(10, 10) )\n",
    "for i in range(20):\n",
    "    plt.subplot(5, 4, i+1)\n",
    "    plt.bar(*np.unique(x_post[i], return_counts=True), width=0.2)\n",
    "    plt.bar(*np.unique(data, return_counts=True), width=0.12, \n",
    "            alpha=0.5, label='Observed data')\n",
    "    plt.xticks([0, 1])\n",
    "    plt.legend(loc='best', fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - Challenger Space Shuttle Disaster\n",
    "\n",
    "Let's revisit this example from the MCMC lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "challenger_data = np.genfromtxt(\"challenger_data.csv\", skip_header=1,\n",
    "                                usecols=[1, 2], missing_values=\"NA\",\n",
    "                                delimiter=\",\")\n",
    "challenger_data = challenger_data[~np.isnan(challenger_data[:, 1])]\n",
    "print(\"Temp (F), O-Ring failure?\")\n",
    "print(challenger_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot it, as a function of temperature (the first column)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(challenger_data[:, 0], challenger_data[:, 1], 'ro', \n",
    "         markersize=15)\n",
    "plt.ylabel(\"Damage Incident?\",fontsize=20)\n",
    "plt.xlabel(\"Outside temperature (Fahrenheit)\",fontsize=20)\n",
    "plt.title(\"Defects of the Space Shuttle O-Rings vs temperature\",\n",
    "          fontsize=20)\n",
    "plt.yticks([0, 1], fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic model \n",
    "\n",
    "The defect probability is modeled as a function of the outside temperature:\n",
    "$$\\sigma(t) = \\frac{1}{ 1 + e^{ \\;\\beta t + \\alpha } } $$.\n",
    "\n",
    "The goal is to infer the latent variables $\\alpha$ and $\\beta$. \n",
    "\n",
    "We set normal priors on the latent variables - $\\alpha = \\mathcal{N}(0, 10^2)$ and $\\beta = \\mathcal{N}(0, 10^2)$ and the likelihood model is giveb by $p(x_i | \\alpha, \\beta, t) = \\mathrm{Bern}(x_i | \\sigma(t; \\alpha, \\beta) )$.\n",
    "\n",
    "The graphical model for this problem is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcp = Digraph('space_shuttle_disaster')\n",
    "\n",
    "# setup the nodes \n",
    "gcp.node('alpha', label='<&alpha;>')\n",
    "gcp.node('beta', label='<&beta;>')\n",
    "with gcp.subgraph(name='cluster_0') as sg:\n",
    "    sg.node('pi', label='<p<sub>i</sub>>')\n",
    "    sg.node('xi', label='<x<sub>i</sub>>', style='filled')\n",
    "    sg.attr(color='blue')\n",
    "    sg.attr(label='i=1,2...')\n",
    "    sg.attr(labelloc='b')\n",
    "\n",
    "# setup the edges \n",
    "gcp.edge('alpha', 'pi')\n",
    "gcp.edge('beta', 'pi')\n",
    "gcp.edge('pi', 'xi')\n",
    "gcp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `PyMC3` model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather the data and apply preprocessing if any \n",
    "temp = challenger_data[:, 0]\n",
    "temp_scaled = (temp - np.mean(temp))/np.std(temp)\n",
    "data = challenger_data[:, 1]\n",
    "\n",
    "# instantiate the pymc3 model\n",
    "challenger_model = pm.Model()\n",
    "\n",
    "# define the graph \n",
    "with challenger_model:\n",
    "    # define the prior\n",
    "    alpha = pm.Normal('alpha', mu=0., sigma=10.)\n",
    "    beta = pm.Normal('beta', mu=0., sigma=10.)\n",
    "    \n",
    "    # get the probabilities of failure at each observed temp \n",
    "    p = pm.Deterministic('p', 1./(1. + tt.exp(alpha + beta*temp_scaled)))\n",
    "    \n",
    "    # define the likelihood \n",
    "    x = pm.Bernoulli('x', p=p, observed=data)\n",
    "print(\"Challenger space shuttle disaster model:\")\n",
    "challenger_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference and posterior visualization\n",
    "\n",
    "Now let's infer the hidden parameters with VI. We will use both mean-field (i.e. diagonal covariance Gaussian) ADVI and full-rank (i.e. full rank covariance matrix Gaussian) ADVI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 20000\n",
    "num_samples = 20\n",
    "with challenger_model:\n",
    "    method_1 = pm.ADVI()  # <- This method makes q a diagonal Gaussian\n",
    "    vi_approx_1 = pm.fit(n=num_iter, \n",
    "                       method=method_1,\n",
    "                       progressbar=True, \n",
    "                       obj_n_mc=num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with challenger_model:\n",
    "    method_2 = pm.FullRankADVI()  # <- This method makes q a diagonal Gaussian\n",
    "    vi_approx_2 = pm.fit(n=num_iter, \n",
    "                       method=method_2,\n",
    "                       progressbar=True, \n",
    "                       obj_n_mc=num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that the ELBO has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "# ADVI\n",
    "plt.subplot(211)\n",
    "plt.plot(np.arange(num_iter), -vi_approx_1.hist)\n",
    "plt.ylabel('ELBO', fontsize=15)\n",
    "plt.xlabel('Iteration', fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.title('ADVI', fontsize=15);\n",
    "\n",
    "# Full rank ADVI\n",
    "plt.subplot(212)\n",
    "plt.plot(np.arange(num_iter), -vi_approx_2.hist)\n",
    "plt.ylabel('ELBO', fontsize=15)\n",
    "plt.xlabel('Iteration', fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.title('Full-rank ADVI', fontsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that both methods have converged to a local minimum.\n",
    "\n",
    "Let's get the variational posteriors $q$ for each case and compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the variational parameters and compute the post. pdf over a grid for ADVI\n",
    "mu, sigma = vi_approx_1.mean.eval(), vi_approx_1.std.eval()\n",
    "qtheta = pm.MvNormal.dist(mu=mu, cov=np.diag(sigma), shape=2)\n",
    "x1max, x2max = mu + 3.*sigma\n",
    "x1min, x2min = mu - 3.*sigma\n",
    "x1 = np.linspace(x1min, x1max, 50)\n",
    "x2 = np.linspace(x2min, x2max, 50)\n",
    "names = [g.name for g in vi_approx_1.groups[0].group]\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "Xgrid = np.vstack([X1.flatten(), X2.flatten()]).T\n",
    "advipdf = np.exp(qtheta.logp(tt.as_tensor(Xgrid)).eval()).reshape((50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the variational parameters and compute the post. pdf over a grid for full rank ADVI\n",
    "mu, cov = vi_approx_2.mean.eval(), vi_approx_2.cov.eval()\n",
    "sigma = np.sqrt(np.diag(cov))\n",
    "qtheta = pm.MvNormal.dist(mu=mu, cov=cov, shape=2)\n",
    "\n",
    "x1max, x2max = mu + 3.*sigma\n",
    "x1min, x2min = mu - 3.*sigma\n",
    "x1 = np.linspace(x1min, x1max, 50)\n",
    "x2 = np.linspace(x2min, x2max, 50)\n",
    "names = [g.name for g in vi_approx_2.groups[0].group]\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "Xgrid = np.vstack([X1.flatten(), X2.flatten()]).T\n",
    "advifullrankpdf = np.exp(qtheta.logp(tt.as_tensor(Xgrid)).eval()).reshape((50, 50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.contourf(X1, X2, advipdf, 50)\n",
    "plt.xlabel('$\\\\'+names[0]+'$', fontsize=15)\n",
    "plt.ylabel('$\\\\'+names[1]+'$', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title('Joint Posterior over $(\\\\alpha, \\\\beta)$ - diagonal Gaussian posterior.', fontsize=15)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.contourf(X1, X2, advifullrankpdf, 50)\n",
    "plt.xlabel('$\\\\'+names[0]+'$', fontsize=15)\n",
    "plt.ylabel('$\\\\'+names[1]+'$', fontsize=15)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.title('Joint Posterior over $(\\\\alpha, \\\\beta)$ - full rank covariance.', fontsize=15)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the simple diagonal Gaussian does not capture correlation between $\\alpha$ and $\\beta$,something we are able to recover with the full rank ADVI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Posterior predictive distribution\n",
    "\n",
    "Here we will use a trace generated from the full rank Gaussian posterior to make posterior predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = vi_approx_2.sample(10000)\n",
    "\n",
    "with challenger_model:\n",
    "    ppsamples = pm.sample_posterior_predictive(trace=trace, \n",
    "                                               samples=2000,\n",
    "                                               var_names=['p'])['p']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get posterior predictive mean and 95% interval\n",
    "ppmean = ppsamples.mean(axis=0)\n",
    "pp_lower, pp_upper = np.percentile(ppsamples, axis=0, q=[2.5, 97.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(temp, data, 'ro', markersize=12, label='Observed data')\n",
    "idx=np.argsort(temp)\n",
    "plt.plot(temp[idx], ppmean[idx], linestyle='--', linewidth=2.5, \n",
    "         label='Post. pred. mean prob.')\n",
    "plt.fill_between(temp[idx], pp_lower[idx], pp_upper[idx], \n",
    "                color='purple', alpha=0.25, label='95% Confidence')\n",
    "plt.ylabel(\"Probability estimate\",fontsize=20)\n",
    "plt.xlabel(\"Outside temperature (Fahrenheit)\",fontsize=20)\n",
    "plt.title(\"Defects of the Space Shuttle O-Rings vs temperature\",\n",
    "          fontsize=20)\n",
    "plt.yticks(np.arange(0., 1.01, 0.2), fontsize=20)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.legend(loc='best', fontsize=20)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
