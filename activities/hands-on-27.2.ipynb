{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 100\n",
    "import seaborn as sns\n",
    "sns.set_context('paper')\n",
    "sns.set_style('white')\n",
    "# A helper function for downloading files\n",
    "import requests\n",
    "import os\n",
    "def download(url, local_filename=None):\n",
    "    \"\"\"\n",
    "    Downloads the file in the ``url`` and saves it in the current working directory.\n",
    "    \"\"\"\n",
    "    data = requests.get(url)\n",
    "    if local_filename is None:\n",
    "        local_filename = os.path.basename(url)\n",
    "    with open(local_filename, 'wb') as fd:\n",
    "        fd.write(data.content)\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Activity 27.2 - Sampling From the Distributions With Random Walk Metropolis\n",
    "\n",
    "## Objectives\n",
    "\n",
    "+ Introduce the concept of random walk in 1D and 2D because it is the backbone of many sampling algorithms.\n",
    "+ Use the Metropolis algorithm (the simplest Markov Chain Monte Carlo (MCMC) algorithm) to sample from an arbitrary probability density known up to a normalization constant.\n",
    "+ Learn how to diagnose the convergence of MCMC algorithms by monitoring the acceptance rate, and autocorrelation.\n",
    "+ Use MCMC to calibrate the reaction kinetics problem using the Bayesian formulation.\n",
    "\n",
    "## Readings\n",
    "\n",
    "+ Chapter 11 of Bishop.\n",
    "+ These notes.\n",
    "\n",
    "## Attention\n",
    "\n",
    "In this notebook we implement the Metropolis algorithm from scratch. \n",
    "This is just to see what is behind the PyMC3 blackbox.\n",
    "Also, note that the results below are not correct! You are supposed to follow the discussion and the answer questions to get the correct results!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Random walk in 1D.\n",
    "\n",
    "Random walk in 1D is one of the most basic Markov chains.\n",
    "It will be the building block for the Metropolis algorithm.\n",
    "The *state space* is:\n",
    "$$\n",
    "\\mathcal{X} = \\mathbb{R}.\n",
    "$$\n",
    "The *transition kernel* is:\n",
    "$$\n",
    "p(x_{n+1}|x_n) \\equiv T(x_{n},x_{n+1}) := \\mathcal{N}\\left(x_{n+1}|x_n,\\sigma^2\\right) = \\left(2\\pi\\sigma^2\\right)^{-\\frac{1}{2}}\\exp\\left\\{-\\frac{\\left(x_{n+1}-x_n\\right)^2}{2\\sigma^2}\\right\\},\n",
    "$$\n",
    "for some parameter $\\sigma>0$.\n",
    "An alternative way of writing the same thing is:\n",
    "$$\n",
    "X_{n+1} = X_n + \\sigma Z_n,\n",
    "$$\n",
    "where $Z_1,\\dots,Z_n\\sim \\mathcal{N}(0,1)$ independent random variables.\n",
    "\n",
    "Let's visualize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a starting point for your random walk\n",
    "x0 = 0.\n",
    "# Pick a standard deviation for your random walk\n",
    "sigma = 1\n",
    "# Pick the number of steps you want to simulate\n",
    "n = 1000\n",
    "# How many different sample paths of the process do you want to simulate\n",
    "n_paths = 10\n",
    "# We will be plotting in here:\n",
    "fig, ax = plt.subplots()\n",
    "# Loop over the paths\n",
    "for _ in range(n_paths):\n",
    "    # Simulate a single path\n",
    "    X = np.ndarray((n + 1,))\n",
    "    X[0] = x0\n",
    "    for t in range(1, n + 1):\n",
    "        Zt = np.random.randn()\n",
    "        X[t] = X[t-1] + sigma * Zt\n",
    "    # Let's plot it\n",
    "    ax.plot(range(n+1), X)\n",
    "ax.set_xlabel('$n$ (steps)')\n",
    "ax.set_ylabel('$X_n$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Increase the number of steps $n$ from 1,000 to 10,000 to 100,000. Run it a couple of times for each case. What do you observe?\n",
    "+ Get the number of steps $n$ down to 1,000. Increase $\\sigma$ to 0.1 to 1. What do you observe for the values of $X$?\n",
    "+ Plot ten different sample path from the random walk process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Random Walk in Higher-dimensions.\n",
    "\n",
    "The random walk can be generalized to arbitrary dimensions.\n",
    "The *state space* is:\n",
    "$$\n",
    "\\mathcal{X} = \\mathbb{R}^d.\n",
    "$$\n",
    "The *transition kernel* is:\n",
    "$$\n",
    "p(x_{n+1}|x_n) \\equiv T(x_{n},x_{n+1}) := \\mathcal{N}\\left(x_{n+1}|x_n,\\Sigma\\right) = \\left(2\\pi\\right)^{-\\frac{d}{2}}|\\Sigma|^{-\\frac{1}{2}}\\exp\\left\\{-\\frac{1}{2}(x_{n+1}-x_n)^T\\Sigma^{-1}(x_{n+1}-x_n)\\right\\},\n",
    "$$\n",
    "for some positive definite covariance matrix $\\Sigma\\in\\mathbb{R}^{d\\times d}$.\n",
    "\n",
    "An alternative way of writing the same thing is:\n",
    "$$\n",
    "X_{n+1} = X_n + A Z_n,\n",
    "$$\n",
    "where $Z_1,\\dots,Z_n\\sim \\mathcal{N}(0,I_d)$ independent random vectors, and $A\\in\\mathbb{R}^{d\\times d}$ is a square root of $\\Sigma$, e.g., the Cholesky decomposition.\n",
    "\n",
    "Let's visualize it for two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a starting point for your random walk\n",
    "x0 = np.zeros((2,))\n",
    "# Pick a standard deviation for your random walk\n",
    "sigma1 = 0.1\n",
    "sigma2 = 0.1\n",
    "Sigma = np.diag([sigma1 ** 2, sigma2 ** 2])\n",
    "A = np.linalg.cholesky(Sigma)\n",
    "# Pick the number of steps you want to simulate\n",
    "n = 10000\n",
    "# How many different sample paths of the process do you want to simulate\n",
    "n_paths = 1\n",
    "# We will be plotting in here:\n",
    "fig, ax = plt.subplots()\n",
    "# Loop over the paths\n",
    "for _ in range(n_paths):\n",
    "    # Simulate a single path\n",
    "    X = np.ndarray((n + 1, 2))\n",
    "    X[0, :] = x0\n",
    "    for t in range(1, n + 1):\n",
    "        Zt = np.random.randn(2)\n",
    "        X[t, :] = X[t-1] + np.dot(A, Zt)\n",
    "    # Let's plot it\n",
    "    ax.plot(X[:, 0], X[:, 1], lw=1)\n",
    "ax.set_xlabel('$X_{n1}$')\n",
    "ax.set_ylabel('$X_{n2}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "+ Increase the number of steps $n$ from 1,000 to 10,000 to 100,000. Run it a couple of times for each case. What do you observe?\n",
    "+ Get the number of steps $n$ down to 1,000. Increase $\\sigma$ to 0.1 to 1. What do you observe for the values of $X$?\n",
    "+ Plot ten different sample path from the random walk process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Metropolis Algorithm\n",
    "\n",
    "Now, let's get back to the initial problem of sampling from:\n",
    "$$\n",
    "\\pi(x) = \\frac{h(x)}{Z},\n",
    "$$\n",
    "without knowing $Z$.\n",
    "In 1953, Metropolis et al. demonstrated how we can construct a Markov chain with $\\pi(x)$ as the equilibrium density.\n",
    "The algorithm is based on biasing an underlying symmetric, stationary Markov chain.\n",
    "Let $T(x,x')$ be the transition kernel of this underlying Markov chain (also called the *proposal distribution*.\n",
    "The transition kernel must be symmetric, i.e.,\n",
    "$$\n",
    "T(x,x') = T(x',x).\n",
    "$$\n",
    "A very common choice of the proposal distribution is the random walk transition kernel:\n",
    "$$\n",
    "T(x,x') = \\mathcal{N}(x'|x, \\Sigma).\n",
    "$$\n",
    "However, this is just one possibility.\n",
    "Once we have pick a proposal, we construct the desired Markov chain as follows:\n",
    "\n",
    "+ **Initialization:** Pick an arbitrary starting point $x_0$.\n",
    "+ For each time step $n$:\n",
    "    - **Generation:** Sample a candidate $x$ from $T(x_n, x)$.\n",
    "    - **Calculation:** Calculate the *acceptance ratio*:\n",
    "    $$\n",
    "    \\alpha(x_n, x) = \\min\\left\\{1, \\frac{h(x)}{h(x_n)}\\right\\}.\n",
    "    $$\n",
    "    This is the only place where you may need to evaluate the underlying model.\n",
    "    - **Accept/Reject:**\n",
    "        - Generate a uniform number $u\\sim \\mathcal{U}([0,1])$.\n",
    "        - If $u\\le \\alpha$, *accept* and set $x_{n+1}=x$.\n",
    "        - If $u > \\alpha$, *reject* ad set $x_{n+1} = x_n$.\n",
    "        \n",
    "Here is a generic implementation. Note that we are working with the $\\log h(x)$ for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rwmetropolis(x0, log_h, n, sigma, args=()):\n",
    "    \"\"\"\n",
    "    Random walk metropolis.\n",
    "    \n",
    "    :param x0:     The initial point (numpy array).\n",
    "    :param log_h:  The logartihm of the function that is proportional to the density you want to sample from (function).\n",
    "    :param n:      The maximum number of steps you want to take.\n",
    "    :param sigma:  The standard deviation of the random walk proposal.\n",
    "    :param args:   Any parameters to log_h.\n",
    "    \n",
    "    :returns:  X, acceptance_rate\n",
    "    \"\"\"\n",
    "    x0 = np.array(x0)\n",
    "    assert x0.ndim == 1\n",
    "    # Dimensionality of space\n",
    "    d = x0.shape[0]\n",
    "    # A place to store the samples\n",
    "    X = np.ndarray((n + 1, d))\n",
    "    X[0, :] = x0\n",
    "    # Previous value of log(h(x))\n",
    "    log_h_p = log_h(x0, *args)\n",
    "    # Keep track of how many samples are accepted\n",
    "    count_accepted = 0\n",
    "    # Start the simulation\n",
    "    for t in range(1, n + 1):\n",
    "        # Generation\n",
    "        x = X[t - 1, :] + sigma * np.random.randn(d)\n",
    "        # Calculation\n",
    "        log_h_c = log_h(x, *args) # Current value of log(h(x))\n",
    "        alpha = min(1, np.exp(log_h_c - log_h_p))\n",
    "        # Accept/Reject\n",
    "        u = np.random.rand()\n",
    "        if u <= alpha: # Accept\n",
    "            X[t, :] = x\n",
    "            log_h_p = log_h_c\n",
    "            count_accepted += 1\n",
    "        else:          # Reject\n",
    "            X[t, :] = X[t - 1, :]\n",
    "    # Empirical acceptance rate\n",
    "    acceptance_rate = count_accepted / (1. * n)\n",
    "    return X, acceptance_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Sampling from an Exponential using Random Walk Metropolis\n",
    "\n",
    "Let's take $\\mathcal{X}=(0,\\infty)$ and:\n",
    "$$\n",
    "\\pi(x) \\propto e^{-10x}.\n",
    "$$\n",
    "This is proportional to an exponential density with rate $\\lambda=10$.\n",
    "Of course, we know that the normalization constant is $Z=1/10$, but we are not going to use it.\n",
    "As a proposal distribution, we will use a simple random walk:\n",
    "$$\n",
    "T(x_n,x) = \\mathcal{N}(x|x_n, \\sigma^2),\n",
    "$$\n",
    "and we will just pick $\\sigma>0$ by hand.\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function proportional to the logarithm of the density from which you wish to sample.\n",
    "# (We always work with the log for numerical stability)\n",
    "def log_h(x):\n",
    "    if x[0] < 0:\n",
    "        return -1e99 # Negative values are not allowed - Give back something very negative\n",
    "    return -10. * x[0]  # The log of h(x)\n",
    "\n",
    "# Initialiazation:\n",
    "x0 = np.array([10.])\n",
    "# Parameters of the proposal:\n",
    "sigma = 1.\n",
    "# Number of steps\n",
    "n = 10000\n",
    "\n",
    "X, acceptance_rate = rwmetropolis(x0, log_h, n, sigma)\n",
    "\n",
    "print('Acceptance rate: %1.2f' % acceptance_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The acceptance rate gives the percentage of steps with an accepted move.\n",
    "For a local proposal distribution (like our random walk above) we should struggle to keep the acceptance rate between 0.2 and 0.6. This can be done by adjusting the size of the steps that we propose.\n",
    "In general:\n",
    "+ If the acceptance rate is too small, then our chain makes too ambitious moves.\n",
    "+ If the acceptance rate is too big, then our chain is not ambitious enough.\n",
    "\n",
    "### Questions\n",
    "\n",
    "+ Choose $\\sigma$ so that you make the acceptance rate around $0.25$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(n + 1), X, lw=1)\n",
    "ax.set_xlabel('$n$ (steps)')\n",
    "ax.set_ylabel('$X_n$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By construction, samples from this chain are supposed to be samples from the right propability distribution.\n",
    "There are, however, two issues:\n",
    "\n",
    "+ Ergodicity, guarantees samples only for large $n>n_b$.\n",
    "At the beginning the chain exhibits transient behavior.\n",
    "What we need to do is *burn* these initial samples, i.e., we throw them away.\n",
    "The only reason to see the transient regime is to study the chain path in the figure above.\n",
    "\n",
    "+ Consecutive samples are highly correleated (as the chain state does not even change if a move is rejected).\n",
    "Ideally, we want as independent samples as possible.\n",
    "To achieve this, we need to throw samples in between.\n",
    "This is called *thinning* the process.\n",
    "To figure out how to thin the process, you need to look at the *autocorrelation* of the process.\n",
    "Since, we have a stationary process the autocorrelation is expressed as a function of the time lag $k$ between two steps $n$ and $n+k$:\n",
    "$$\n",
    "R(k) = \\frac{\\mathbb{E}\\left[(X_n - \\mu)(X_{n+k}-\\mu)\\right]}{\\sigma^2}.\n",
    "$$\n",
    "Ideally, you want to think every $k*$ so that:\n",
    "$$\n",
    "R(k^*) \\approx 0.\n",
    "$$\n",
    "That is, you would pick $X_{n_b + k^*}, X_{n_b + 2k*},\\dots$.\n",
    "These samples will look independent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples do you want to burn?\n",
    "burn = 200\n",
    "# How many samples do you want to throw in between?\n",
    "thin = 1 # Keep one every ten samples (k*)\n",
    "# Here are the remaining samples:\n",
    "X_rest = X[burn::thin]\n",
    "fig, ax = plt.subplots()\n",
    "ax.acorr(X_rest[:, 0], detrend=plt.mlab.detrend_mean, maxlags=100)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_ylabel(r'$R(%d \\times k)$ (Autocorrelation)' % thin)\n",
    "ax.set_xlabel(r'$k$ ($%d \\times$ lag)' % thin);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have chosen ``burn`` and ``thin`` the right way (see questions), you should see samples that look almost independent.\n",
    "Let's test this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_rest, lw=1.)\n",
    "ax.set_xlabel('$m$ (steps)')\n",
    "ax.set_ylabel('$X_{n_b+m k^*}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's estimate the mean and the variance by sampling average and compare them to the true values.\n",
    "**Note:** It is also possible to get error bars because the CLT holds (if certain regularity condtions hold), but we do not do it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "idx = np.arange(1, X_rest.shape[0] + 1)\n",
    "X_ave = np.cumsum(X_rest) / idx\n",
    "ax.plot(idx, X_ave, label='Sampling average of $\\mathbb{E}[X_n]$')\n",
    "ax.plot(idx, 0.10 * np.ones(idx.shape[0]), label='True $\\mathbb{E}[X_n]$')\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$m$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "X2_ave = np.cumsum(X_rest ** 2) / idx\n",
    "X_var = X2_ave - X_ave ** 2\n",
    "ax.plot(idx, X_var, label='Sampling average of $\\mathbb{V}[X_n]$')\n",
    "ax.plot(idx, 0.01 * np.ones(idx.shape[0]), label='True $\\mathbb{V}[X_n]$')\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$m$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use these *independent* variables to draw the empirical histrogram and compare it to the true density:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(X_rest, density=True, alpha=0.25, bins=50);\n",
    "xx = np.linspace(0, 1, 100)\n",
    "ax.plot(xx, 10. * np.exp(-10. * xx))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\pi(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions:\n",
    "\n",
    "+ Choose ``burn`` so that you ignore the transient regime of the process. How does this affect your averages and you histograms?\n",
    "\n",
    "+ Choose ``thin`` ($k^*$) so that you make the autocorrelation of almost zero. How does this affect your averages and you histograms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 4: Sampling from a Beta using Random Walk Metropolis\n",
    "\n",
    "Let's take $\\mathcal{X}=(0,1)$ and:\n",
    "$$\n",
    "\\pi(x) \\propto x^{\\alpha-1}(1-x)^{\\beta - 1}.\n",
    "$$\n",
    "As a proposal distribution, we will use a simple random walk:\n",
    "$$\n",
    "T(x_n,x) = \\mathcal{N}(x|x_n, \\sigma^2),\n",
    "$$\n",
    "and we will just pick $\\sigma>0$ by hand.\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_h_beta(x, alpha, beta):\n",
    "    if x[0] <= 0 or x[0] >= 1:\n",
    "        return -1e99\n",
    "    return (alpha - 1.) * np.log(x[0]) + (beta - 1.) * np.log(1. - x[0])\n",
    "\n",
    "# Initialiazation:\n",
    "x0 = np.array([.5])\n",
    "# Parameters of the proposal:\n",
    "sigma = .1\n",
    "# Number of steps\n",
    "n = 1000000\n",
    "# For which alpha and beta do you want to run it?\n",
    "alpha = .1\n",
    "beta = .1\n",
    "\n",
    "# Start sampling\n",
    "X, acceptance_rate = rwmetropolis(x0, log_h_beta, n, sigma, args=(alpha, beta))\n",
    "\n",
    "print('Acceptance rate: %1.2f' % acceptance_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(n + 1), X, lw=1)\n",
    "ax.set_xlabel('$n$ (steps)')\n",
    "ax.set_ylabel('$X_n$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples do you want to burn?\n",
    "burn = 10\n",
    "# How many samples do you want to throw in between?\n",
    "thin = 1000 # Keep one every ten samples (k*)\n",
    "# Here are the remaining samples:\n",
    "X_rest = X[burn::thin]\n",
    "fig, ax = plt.subplots()\n",
    "ax.acorr(X_rest[:, 0], detrend=plt.mlab.detrend_mean, maxlags=10)\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylabel('$R(10k)$ (Autocorrelation)')\n",
    "ax.set_xlabel(r'$k$ ($%d \\times$ lag)' % thin);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_rest, lw=0.5)\n",
    "ax.set_xlabel('$m$ (steps)')\n",
    "ax.set_ylabel('$X_{n_0+m k^*}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "idx = np.arange(1, X_rest.shape[0] + 1)\n",
    "X_ave = np.cumsum(X_rest) / idx\n",
    "ax.plot(idx, X_ave, label='Sampling average of $\\mathbb{E}[X_n]$')\n",
    "ax.plot(idx, alpha / (alpha + beta) * np.ones(idx.shape[0]), label='True $\\mathbb{E}[X_n]$')\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$m$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "X2_ave = np.cumsum(X_rest ** 2) / idx\n",
    "X_var = X2_ave - X_ave ** 2\n",
    "ax.plot(idx, X_var, label='Sampling average of $\\mathbb{V}[X_n]$')\n",
    "ax.plot(idx, alpha * beta / ((alpha + beta) ** 2 * (alpha + beta + 1)) * np.ones(idx.shape[0]), label='True $\\mathbb{V}[X_n]$')\n",
    "plt.legend(loc='best')\n",
    "ax.set_xlabel('$m$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.hist(X_rest, density=True, alpha=0.25, bins=50);\n",
    "xx = np.linspace(0, 1, 100)\n",
    "ax.plot(xx, st.beta(alpha, beta).pdf(xx))\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$\\pi(x)$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ For $\\alpha = 2, \\beta=5$ do the following:\n",
    "    - find the $\\sigma$ that gives you an acceptance rate of about $0.25$.\n",
    "    - find how many samples $n_b$ you need to burn to get over the transient (if any).\n",
    "    - find how many samples $k^*$ you need to drop in between to drive the autocorrelation down to almost zero.\n",
    "    \n",
    "+ Repeat the steps above for $\\alpha = 0.5$ and $\\beta=0.5$. What do you observe now that you have two modes?\n",
    "\n",
    "+ Repeat the steps above for $\\alpha = 0.1$ and $\\beta=0.1$. What do you observe now that your modes are even more pronounced?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 5: Sampling from a Gaussian with Random Walk Metropolis\n",
    "\n",
    "Let's take $\\mathcal{X}=\\mathbb{R}^2$ and:\n",
    "$$\n",
    "\\pi(x) \\propto \\exp\\left\\{-\\frac{1}{2}\\left(x-\\mu\\right)^T\\Lambda(x-\\mu)\\right\\},\n",
    "$$\n",
    "where $\\mu\\in\\mathbb{R}^2$ is the mean and $\\Lambda = \\Sigma^{-1}\\in\\mathbb{R}^{2\\times 2}$ is the precision matrix.\n",
    "As a proposal distribution, we will use a simple random walk:\n",
    "$$\n",
    "T(x_n,x) = \\mathcal{N}(x|x_n, \\sigma^2 I_2),\n",
    "$$\n",
    "and we will just pick $\\sigma>0$ by hand.\n",
    "Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The log of the density (up to a normalizing constant) of the distribution from which we want to sample:\n",
    "def log_h_mvn(x, mu, Lambda):\n",
    "    tmp = x - mu\n",
    "    return -0.5 * np.dot(tmp, np.dot(Lambda, tmp))\n",
    "\n",
    "# The parameters of the disribution from which we wish to sample\n",
    "mu = np.array([5., 2.])\n",
    "Sigma = np.array([[1., .4],\n",
    "                  [.3, 0.2]]) # This has to be positive definite - otherwise you will get garbage!\n",
    "Lambda = np.linalg.inv(Sigma)\n",
    "\n",
    "# Initialiazation:\n",
    "x0 = np.array([-5., 5.])\n",
    "# Parameters of the proposal:\n",
    "sigma = 0.2\n",
    "# Number of steps:\n",
    "n = 10000\n",
    "\n",
    "# Start sampling\n",
    "X, acceptance_rate = rwmetropolis(x0, log_h_mvn, n, sigma, args=(mu, Lambda))\n",
    "\n",
    "print('Acceptance rate: %1.2f' % acceptance_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(range(n + 1), X, lw=1)\n",
    "ax.set_xlabel('$n$ (steps)')\n",
    "ax.set_ylabel('$X_{ni}$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X[:, 0], X[:, 1], lw=1)\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples do you want to burn?\n",
    "burn = 100\n",
    "# How many samples do you want to throw in between?\n",
    "thin = 1 # Keep one every ten samples (k*)\n",
    "# Here are the remaining samples:\n",
    "X_rest = X[burn::thin]\n",
    "for i in range(X_rest.shape[1]):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.acorr(X_rest[:, 0], detrend=plt.mlab.detrend_mean, maxlags=50)\n",
    "    ax.set_xlim(0, 50)\n",
    "    ax.set_ylabel('$R_{%d}(10k)$ (Autocorrelation)' % (i + 1))\n",
    "    ax.set_xlabel(r'$k$ ($%d \\times$ lag)' % thin);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(X_rest, lw=0.5)\n",
    "ax.set_xlabel('$m$ (steps)')\n",
    "ax.set_ylabel('$X_{n_0+m k^*}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Questions\n",
    "\n",
    "+ For the case shown:\n",
    "    - find the $\\sigma$ that gives you an acceptance rate of about $0.25$.\n",
    "    - find how many samples $n_b$ you need to burn to get over the transient (if any).\n",
    "    - find how many samples $k^*$ you need to drop in between to drive the autocorrelation down to almost zero."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
