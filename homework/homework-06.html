

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Homework 6 &#8212; Introduction to Scientific Machine Learning (Lecture Book)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'homework/homework-06';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Homework 7" href="homework-07.html" />
    <link rel="prev" title="Homework 5" href="homework-05.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    <p class="title logo__title">Introduction to Scientific Machine Learning (Lecture Book)</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../index.html">
                    Preface
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../introduction.html">Introduction</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture01/intro.html">Lecture 1 - Introduction to Predictive Modeling</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture01/reading-01.html">The Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture01/hands-on-01.1.html">The Uncertainty Propagation Problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture01/hands-on-01.2.html">The Model Calibration Problem</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../review_probability.html">Review of Probability</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture02/intro.html">Lecture 2 - Basics of Probability Theory</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture02/reading-02.html">Basics of Probability Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture02/hands-on-02.html">Experiment with “Randomness”</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture03/intro.html">Lecture 3 - Discrete Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture03/reading-03.html">Discrete Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture03/hands-on-03.html">Discrete Random Variables in Python</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture04/intro.html">Lecture 4 - Continuous Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture04/reading-04.html">Continuous Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture04/hands-on-04.1.html">The Uniform Distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture04/hands-on-04.2.html">The Gaussian Distribution</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture05/intro.html">Lecture 5 - Collections of Random Variables</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture05/reading-05.html">Collections of Random Variables: Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture05/hands-on-05.html">Practicing with Joint Probability Mass Functions</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture06/intro.html">Lecture 6 - Random Vectors</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/reading-06.html">Random Vectors</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.1.html">The Multivariate Normal - Diagonal Covariance Case</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.2.html">The Multivariate Normal - Full Covariance Case</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.3.html">The Multivariate Normal - Marginalization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture06/hands-on-06.4.html">The Multivariate Normal - Conditioning</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../uncertainty_propagation.html">Uncertainty Propagation</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture07/intro.html">Lecture 7 - Basic Sampling</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.1.html">Pseudo-random number generators</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.2.html">Sampling the uniform distribution</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.3.html">Sampling the categorical</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture07/hands-on-07.4.html">Sampling from continuous distributions - Inverse sampling</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture08/intro.html">Lecture 8 - The Monte Carlo Method for Estimating Expectations</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture08/reading-08.html">The Uncertainty Propagation Problem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture08/hands-on-08.3.html">The Monte Carlo Method for Estimating Expectations</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture08/hands-on-08.4.html">Sampling Estimates of Variance</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture09/intro.html">Lecture 9 - Monte Carlo Estimates of Various Statistics</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.1.html">Sampling Estimates of the Cumulative Distribution Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.2.html">Sampling Estimates of the Probability Density via Histograms</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.3.html">Estimating Predictive Quantiles</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture09/hands-on-09.4.html">Uncertainty propagation through an ordinary differential equation</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture10/intro.html">Lecture 10 - Quantify Uncertainty in Monte Carlo Estimates</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.1.html">Visualizing Monte Carlo Uncertainty</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.2.html">The Central Limit Theorem</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.3.html">Quantifying Epistemic Uncertainty in Monte Carlo Estimates</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture10/hands-on-10.4.html">Uncertainty Propagation Through a Boundary Value Problem</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../principles_of_bi.html">Principles of Bayesian Inference</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture11/intro.html">Lecture 11 - Selecting Prior Information</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/reading-11.html">Selecting Prior Information</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/hands-on-11.1.html">Information Entropy</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/hands-on-11.2.html">The Principle of Maximum Entropy for Discrete Random Variables</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture11/hands-on-11.3.html">The Principle of Maximum Entropy for Continuous Random Variables</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture12/intro.html">Lecture 12 - Analytical Examples of Bayesian Inference</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/reading-12.html">Bayesian inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.1.html">Example: Inferring the probability of a coin toss from data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.2.html">Credible Intervals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.3.html">Decision Making</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture12/hands-on-12.4.html">Posterior Predictive Checking</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../supervised_learning.html">Supervised Learning</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture13/intro.html">Lecture 13 - Linear Regression via Least Squares</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/reading-13.html">Linear Regression via Least Squares</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.1.html">Linear regression with a single variable</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.2.html">Polynomial Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.3.html">The Generalized Linear Model</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture13/hands-on-13.4.html">Measures of Predictive Accuracy</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture14/intro.html">Lecture 14 - Bayesian Linear Regression</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/reading-14.html">Bayesian Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.1.html">Probabilistic Interpretation of Least Squares - Estimating the Measurement Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.2.html">Maximum a Posteriori Estimate - Avoiding Overfitting</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.3.html">Bayesian Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture14/hands-on-14.4.html">The point-predictive Distribution - Separating Epistemic and Aleatory Uncertainty</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture15/intro.html">Lecture 15 - Advanced Topics in Bayesian Linear Regression</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/reading-15.html">Advanced Topics in Bayesian Linear Regression</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/hands-on-15.1.html">Evidence approximation</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/hands-on-15.2.html">Automatic Relevance Determination</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture15/hands-on-15.3.html">Diagnostics for Posterior Predictive</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture16/intro.html">Lecture 16 - Classification</a><input class="toctree-checkbox" id="toctree-checkbox-21" name="toctree-checkbox-21" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-21"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/reading-16.html">Theoretical Background on Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.1.html">Logistic regression with one variable (High melting explosives)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.2.html">Logistic Regression with Many Features</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.3.html">Decision making</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.4.html">Diagnostics for Classifications</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture16/hands-on-16.5.html">Multi-class Logistic Regression</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../unsupervised_learning.html">Unsupervised Learning</a><input class="toctree-checkbox" id="toctree-checkbox-22" name="toctree-checkbox-22" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-22"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture17/intro.html">Lecture 17 - Clustering and Density Estimation</a><input class="toctree-checkbox" id="toctree-checkbox-23" name="toctree-checkbox-23" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-23"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture17/reading-17.html">Unsupervised Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture17/hands-on-17.1.html">Clustering using k-means</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture17/hands-on-17.2.html">Density Estimation via Gaussian mixtures</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture18/intro.html">Lecture 18 - Dimensionality Reduction</a><input class="toctree-checkbox" id="toctree-checkbox-24" name="toctree-checkbox-24" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-24"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/reading-18.html">Dimensionality Reduction</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/hands-on-18.1.html">Dimensionality Reduction Examples</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/hands-on-18.2.html">Clustering High-dimensional Data</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture18/hands-on-18.3.html">Density Estimation with High-dimensional Data</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../state_space_models.html">State Space Models</a><input class="toctree-checkbox" id="toctree-checkbox-25" name="toctree-checkbox-25" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-25"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture19/intro.html">Lecture 19 - State Space Models - Filtering Basics</a><input class="toctree-checkbox" id="toctree-checkbox-26" name="toctree-checkbox-26" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-26"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture19/reading-19.html">State Space Models - Filtering Basics</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture19/hands-on-19.1.html">Object Tracking Example</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture20/intro.html">Lecture 20 - State Space Models - Kalman Filters</a><input class="toctree-checkbox" id="toctree-checkbox-27" name="toctree-checkbox-27" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-27"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture20/reading-20.html">State Space Models - Kalman Filters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture20/hands-on-20.1.html">Kalman Filter for the Object Tracking Example</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../gaussian_process_regression.html">Gaussian Process Regression</a><input class="toctree-checkbox" id="toctree-checkbox-28" name="toctree-checkbox-28" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-28"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture21/intro.html">Lecture 21 - Gaussian Process Regression: Priors on Function Spaces</a><input class="toctree-checkbox" id="toctree-checkbox-29" name="toctree-checkbox-29" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-29"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture21/reading-21.html">Gaussian Process Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture21/hands-on-21.html">Example: Priors on function spaces</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture22/intro.html">Lecture 22 - Gaussian Process Regression: Conditioning on Data</a><input class="toctree-checkbox" id="toctree-checkbox-30" name="toctree-checkbox-30" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-30"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/reading-22.html">Gaussian Process Regression - Theory</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.1.html">Gaussian Process Regression Without Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.2.html">Gaussian Process Regression with Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.3.html">Tuning the Hyperparameters</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture22/hands-on-22.4.html">Multivariate Gaussian Process Regression</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture23/intro.html">Lecture 23 - Bayesian Global Optimization</a><input class="toctree-checkbox" id="toctree-checkbox-31" name="toctree-checkbox-31" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-31"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/reading-23.html">Bayesian Global Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.1.html">Maximum Mean - A Bad Information Acquisition Function</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.2.html">Maximum Upper Interval</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.3.html">Probability of Improvement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.4.html">Expected Improvement</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.5.html">Expected Improvement - With Observation Noise</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture23/hands-on-23.6.html">Quantifying Epistemic Uncertainty about the Solution of the Optimization problem</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../neural_networks.html">Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-32" name="toctree-checkbox-32" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-32"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture24/intro.html">Lecture 24 - Deep Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-33" name="toctree-checkbox-33" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-33"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture24/reading-24.html">Deep Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture24/hands-on-24.html">Regression with Deep Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture25/intro.html">Lecture 25 - Deep Neural Networks Continued</a><input class="toctree-checkbox" id="toctree-checkbox-34" name="toctree-checkbox-34" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-34"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture25/reading-25.html">Deep Neural Networks Continued</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture25/hands-on-25.html">Classification with Deep Neural Networks</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture26/intro.html">Lecture 26 - Physics-informed Deep Neural Networks</a><input class="toctree-checkbox" id="toctree-checkbox-35" name="toctree-checkbox-35" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-35"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture26/reading-26.html">Physics-informed Deep Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture26/hands-on-26.1.html">Physics-informed regularization: Solving ODEs</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture26/hands-on-26.2.html">Physics-informed regularization: Solving PDEs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../advanced_methods.html">Advanced Methods for Characterizing Posteriors</a><input class="toctree-checkbox" id="toctree-checkbox-36" name="toctree-checkbox-36" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-36"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture27/intro.html">Lecture 27 - Sampling Methods</a><input class="toctree-checkbox" id="toctree-checkbox-37" name="toctree-checkbox-37" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-37"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/reading-27.html">Sampling Methods</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.1.html">Probabilistic programming with <code class="docutils literal notranslate"><span class="pre">PyMC3</span></code></a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.2.html">Sampling From the Distributions With Random Walk Metropolis</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.3.html">The Metropolis-Hastings Algorithm</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.4.html">Gibbs Sampling</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture27/hands-on-27.5.html">Sequential Monte Carlo</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../lecture28/intro.html">Lecture 28 - Variational Inference</a><input class="toctree-checkbox" id="toctree-checkbox-38" name="toctree-checkbox-38" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-38"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="../lecture28/reading-28.html">Variational Inference</a></li>
<li class="toctree-l3"><a class="reference internal" href="../lecture28/hands-on-28.html">Variational Inference Examples</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Homework</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-39" name="toctree-checkbox-39" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-39"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="homework-01.html">Homework 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="homework-02.html">Homework 2</a></li>
<li class="toctree-l2"><a class="reference internal" href="homework-03.html">Homework 3</a></li>
<li class="toctree-l2"><a class="reference internal" href="homework-04.html">Homework 4</a></li>



<li class="toctree-l2"><a class="reference internal" href="homework-05.html">Homework 5</a></li>



<li class="toctree-l2 current active"><a class="current reference internal" href="#">Homework 6</a></li>
<li class="toctree-l2"><a class="reference internal" href="homework-07.html">Homework 7</a></li>
<li class="toctree-l2"><a class="reference internal" href="homework-08.html">Homework 8</a></li>












</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">Bibliography</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/PredictiveScienceLab/data-analytics-se/blob/master/lecturebook/homework/homework-06.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="../_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/homework/homework-06.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Homework 6</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions">Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#student-details">Student details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1-defining-priors-on-function-spaces">Problem 1 - Defining priors on function spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-super-smooth-function-with-known-length-scale">Part A - Super smooth function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-super-smooth-function-with-known-ultra-small-length-scale">Part B - Super smooth function with known ultra small length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-continuous-function-with-known-length-scale">Part C - Continuous function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-smooth-periodic-function-with-known-length-scale">Part D - Smooth periodic function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-e-smooth-periodic-function-with-known-length-scale">Part E - Smooth periodic function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-f-the-sum-of-two-functions">Part F - The sum of two functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-g-the-product-of-two-functions">Part G - The product of two functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2">Problem 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-naive-approach">Part A - Naive approach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-improving-the-prior-covariance">Part B - Improving the prior covariance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-predicting-the-future">Part C - Predicting the future</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-bayesian-information-criterion">Part D - Bayesian information criterion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-e-plot-samples-from-the-posterior-gaussian-process">Part E - Plot samples from the posterior Gaussian process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3-using-bayesian-global-optimization-to-calibrate-an-expensive-physical-model">Problem 3 - Using Bayesian Global optimization to calibrate an expensive physical model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-theory-of-inverse-problems">Classical theory of inverse problems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formulation-of-inverse-problems-as-optimization-problems">Formulation of Inverse Problems as Optimization Problems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-square-loss">The Square Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-methodologies">Solution Methodologies</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-the-catalysis-model">Back to the catalysis model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-making-our-life-easier-by-simplifying-the-notation">Step 1: Making our life easier by simplifying the notation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-scale-the-unknown-parameters-to-your-best-of-your-abilities">Step 2: Scale the unknown parameters to your best of your abilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-making-the-connection-between-our-model-and-the-experimental-measurements">Step 3: Making the connection between our model and the experimental measurements</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-programming-our-ode-solver-and-the-loss-function">Step 4: Programming our ODE solver and the loss function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-minimize-the-loss-function">Step 5: Minimize the loss function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-perform-multivariate-gaussian-process-regression-on-an-initial-set-of-data">Part A - Perform multivariate Gaussian process regression on an initial set of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-inspecting-your-model">Part B - Inspecting your model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-diagnostics">Part C - Diagnostics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-calibrate-the-model-with-bayesian-global-optimization">Part D - Calibrate the model with Bayesian global optimization</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="homework-6">
<h1>Homework 6<a class="headerlink" href="#homework-6" title="Permalink to this heading">#</a></h1>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Lectures 21-23 (inclusive).</p></li>
</ul>
</section>
<section id="instructions">
<h2>Instructions<a class="headerlink" href="#instructions" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p>Type your name and email in the “Student details” section below.</p></li>
<li><p>Develop the code and generate the figures you need to solve the problems using this notebook.</p></li>
<li><p>For the answers that require a mathematical proof or derivation you should type them using latex. If you have never written latex before and you find it exceedingly difficult, we will likely accept handwritten solutions.</p></li>
<li><p>The total homework points are 100. Please note that the problems are not weighed equally.</p></li>
</ul>
<div class="cell tag_hide-input docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell source</span>
<span class="expanded">Hide code cell source</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">precision</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">rc</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;figure.dpi&quot;</span><span class="p">:</span><span class="mi">100</span><span class="p">,</span> <span class="s2">&quot;savefig.dpi&quot;</span><span class="p">:</span><span class="mi">300</span><span class="p">})</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;notebook&quot;</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;ticks&quot;</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>
<span class="kn">import</span> <span class="nn">urllib.request</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="k">def</span> <span class="nf">download</span><span class="p">(</span>
    <span class="n">url</span> <span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">local_filename</span> <span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Download a file from a url.</span>
<span class="sd">    </span>
<span class="sd">    Arguments</span>
<span class="sd">    url            -- The url we want to download.</span>
<span class="sd">    local_filename -- The filemame to write on. If not</span>
<span class="sd">                      specified </span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">local_filename</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">local_filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlretrieve</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">local_filename</span><span class="p">)</span>

<span class="k">try</span><span class="p">:</span>
  <span class="kn">import</span> <span class="nn">GPy</span>
<span class="k">except</span><span class="p">:</span>
  <span class="n">_</span><span class="o">=!</span>pip<span class="w"> </span>install<span class="w"> </span>GPy
  <span class="kn">import</span> <span class="nn">GPy</span>
<span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="nn">st</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="student-details">
<h2>Student details<a class="headerlink" href="#student-details" title="Permalink to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>First Name:</strong></p></li>
<li><p><strong>Last Name:</strong></p></li>
<li><p><strong>Email:</strong></p></li>
</ul>
</section>
<section id="problem-1-defining-priors-on-function-spaces">
<h2>Problem 1 - Defining priors on function spaces<a class="headerlink" href="#problem-1-defining-priors-on-function-spaces" title="Permalink to this heading">#</a></h2>
<p>In this problem we are going to explore further how Gaussian processes can be used to define probability measures over function spaces.
To this end, assume that there is a 1D function, call if <span class="math notranslate nohighlight">\(f(x)\)</span>, which we do not know.
For simplicity, assume that <span class="math notranslate nohighlight">\(x\)</span> takes values in <span class="math notranslate nohighlight">\([0,1]\)</span>.
We will employ Gaussian process regression to encode our state of knowledge about <span class="math notranslate nohighlight">\(f(x)\)</span> and sample some possibilities for it.
For each of the cases below:</p>
<ul class="simple">
<li><p>assume that <span class="math notranslate nohighlight">\(f\sim \operatorname{GP}(m, k)\)</span> and pick a mean (<span class="math notranslate nohighlight">\(m(x)\)</span>) and a covariance function <span class="math notranslate nohighlight">\(f(x)\)</span> that match the provided information.</p></li>
<li><p>write code that samples a few times (up to five) the values of <span class="math notranslate nohighlight">\(f(x)\)</span> at a 100 equidistant points between 0 and 1.</p></li>
</ul>
<section id="part-a-super-smooth-function-with-known-length-scale">
<h3>Part A - Super smooth function with known length scale<a class="headerlink" href="#part-a-super-smooth-function-with-known-length-scale" title="Permalink to this heading">#</a></h3>
<p>Assume that you hold the following beliefs</p>
<ul class="simple">
<li><p>You know that <span class="math notranslate nohighlight">\(f(x)\)</span> has as many derivatives as you want and they are all continuous</p></li>
<li><p>You don’t know if <span class="math notranslate nohighlight">\(f(x)\)</span> has a specific trend.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> has “wiggles” that are approximatly of size <span class="math notranslate nohighlight">\(\Delta x=0.1\)</span>.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> is between -4 and 4.</p></li>
</ul>
<p><strong>Answer:</strong></p>
<p><strong>I am doing this for you so that you have a concrete example of what is requested.</strong></p>
<p>The mean function should be:</p>
<div class="math notranslate nohighlight">
\[
m(x) = 0.
\]</div>
<p>The covariance function should be a squared exponential:</p>
<div class="math notranslate nohighlight">
\[
k(x,x') = s^2\exp\left\{-\frac{(x-x')^2}{2\ell^2}\right\},
\]</div>
<p>with variance:</p>
<div class="math notranslate nohighlight">
\[
s^2 = k(x,x) = \mathbb{V}[f(x)] = 4,
\]</div>
<p>and lengthscale <span class="math notranslate nohighlight">\(\ell = 0.1\)</span>.
We chose the variance to be 4 so that with (about) 95% probability the values of <span class="math notranslate nohighlight">\(f(x)\)</span> are between -4 and 4.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the covariance function</span>
<span class="n">k</span> <span class="o">=</span> <span class="n">GPy</span><span class="o">.</span><span class="n">kern</span><span class="o">.</span><span class="n">RBF</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">k</span><span class="o">.</span><span class="n">lengthscale</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">k</span><span class="o">.</span><span class="n">variance</span> <span class="o">=</span> <span class="mf">4.0</span>
<span class="c1"># Sample</span>
<span class="n">xs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="c1"># The mean function at xs</span>
<span class="n">ms</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="c1"># Find the covariance matrix. You need to add a small number</span>
<span class="c1"># to the diagonal to ensure numerical stability</span>
<span class="n">nugget</span> <span class="o">=</span> <span class="mf">1e-6</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">K</span><span class="p">(</span><span class="n">xs</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">])</span> <span class="o">+</span> <span class="n">nugget</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">xs</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="c1"># A multivariate normal that can be used to sample the function values</span>
<span class="n">F</span> <span class="o">=</span> <span class="n">st</span><span class="o">.</span><span class="n">multivariate_normal</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">ms</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">cov</span><span class="o">=</span><span class="n">K</span><span class="p">)</span>
<span class="c1"># Take the function samples</span>
<span class="n">f_samples</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="c1"># Plot the samples</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">f_samples</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/adf81a6e3be88960dc509fa17017472779905d235ab20b24c5aa2184ff5e0b76.png" src="../_images/adf81a6e3be88960dc509fa17017472779905d235ab20b24c5aa2184ff5e0b76.png" />
</div>
</div>
</section>
<section id="part-b-super-smooth-function-with-known-ultra-small-length-scale">
<h3>Part B - Super smooth function with known ultra small length scale<a class="headerlink" href="#part-b-super-smooth-function-with-known-ultra-small-length-scale" title="Permalink to this heading">#</a></h3>
<p>Assume that you hold the following beliefs</p>
<ul class="simple">
<li><p>You know that <span class="math notranslate nohighlight">\(f(x)\)</span> has as many derivatives as you want and they are all continuous</p></li>
<li><p>You don’t know if <span class="math notranslate nohighlight">\(f(x)\)</span> has a specific trend.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> has “wiggles” that are approximatly of size <span class="math notranslate nohighlight">\(\Delta x=0.05\)</span>.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> is between -3 and 3.</p></li>
</ul>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-c-continuous-function-with-known-length-scale">
<h3>Part C - Continuous function with known length scale<a class="headerlink" href="#part-c-continuous-function-with-known-length-scale" title="Permalink to this heading">#</a></h3>
<p>Assume that you hold the following beliefs</p>
<ul class="simple">
<li><p>You know that <span class="math notranslate nohighlight">\(f(x)\)</span> is continuous, nowhere differentiable.</p></li>
<li><p>You don’t know if <span class="math notranslate nohighlight">\(f(x)\)</span> has a specific trend.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> has “wiggles” that are approximatly of size <span class="math notranslate nohighlight">\(\Delta x=0.1\)</span>.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> is between -5 and 5.</p></li>
</ul>
<p>Hint: Use <code class="docutils literal notranslate"><span class="pre">GPy.kern.Exponential</span></code>.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-d-smooth-periodic-function-with-known-length-scale">
<h3>Part D - Smooth periodic function with known length scale<a class="headerlink" href="#part-d-smooth-periodic-function-with-known-length-scale" title="Permalink to this heading">#</a></h3>
<p>Assume that you hold the following beliefs</p>
<ul class="simple">
<li><p>You know that <span class="math notranslate nohighlight">\(f(x)\)</span> is smooth.</p></li>
<li><p>You know that <span class="math notranslate nohighlight">\(f(x)\)</span> is periodic with period 0.1.</p></li>
<li><p>You don’t know if <span class="math notranslate nohighlight">\(f(x)\)</span> has a specific trend.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> has “wiggles” that are approximatly of size <span class="math notranslate nohighlight">\(\Delta x=0.5\)</span> of the period.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> is between -5 and 5.</p></li>
</ul>
<p>Hint: Use <code class="docutils literal notranslate"><span class="pre">GPy.kern.StdPeriodic</span></code>.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-e-smooth-periodic-function-with-known-length-scale">
<h3>Part E - Smooth periodic function with known length scale<a class="headerlink" href="#part-e-smooth-periodic-function-with-known-length-scale" title="Permalink to this heading">#</a></h3>
<p>Assume that you hold the following beliefs</p>
<ul class="simple">
<li><p>You know that <span class="math notranslate nohighlight">\(f(x)\)</span> is smooth.</p></li>
<li><p>You know that <span class="math notranslate nohighlight">\(f(x)\)</span> is periodic with period 0.1.</p></li>
<li><p>You don’t know if <span class="math notranslate nohighlight">\(f(x)\)</span> has a specific trend.</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> has “wiggles” that are approximatly of size <span class="math notranslate nohighlight">\(\Delta x=0.1\)</span> of the period (<strong>the only thing that is different compared to D</strong>).</p></li>
<li><p>You think that <span class="math notranslate nohighlight">\(f(x)\)</span> is between -5 and 5.</p></li>
</ul>
<p>Hint: Use <code class="docutils literal notranslate"><span class="pre">GPy.kern.StdPeriodic</span></code>.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-f-the-sum-of-two-functions">
<h3>Part F - The sum of two functions<a class="headerlink" href="#part-f-the-sum-of-two-functions" title="Permalink to this heading">#</a></h3>
<p>Assume that you hold the following beliefs</p>
<ul class="simple">
<li><p>You know that <span class="math notranslate nohighlight">\(f(x) = f_1(x) + f_2(x)\)</span>, where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f_1(x)\)</span> is smooth with variance 2 and lengthscale 0.5</p></li>
<li><p><span class="math notranslate nohighlight">\(f_2(x)\)</span> is continuous, nowhere differentiable with variance 0.1 and lengthscale 0.1</p></li>
</ul>
</li>
</ul>
<p>Hint: Use must create a new covariance function that is the sum of two other covariances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-g-the-product-of-two-functions">
<h3>Part G - The product of two functions<a class="headerlink" href="#part-g-the-product-of-two-functions" title="Permalink to this heading">#</a></h3>
<p>Assume that you hold the following beliefs</p>
<ul class="simple">
<li><p>You know that <span class="math notranslate nohighlight">\(f(x) = f_1(x)f_2(x)\)</span>, where:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(f_1(x)\)</span> is smooth, periodic (period = 0.1), lengthscale 0.1 (relative to the period), and variance 2.</p></li>
<li><p><span class="math notranslate nohighlight">\(f_2(x)\)</span> is smooth with lengthscale 0.5 and variance 1.</p></li>
</ul>
</li>
</ul>
<p>Hint: Use must create a new covariance function that is the product of two other covariances.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="problem-2">
<h2>Problem 2<a class="headerlink" href="#problem-2" title="Permalink to this heading">#</a></h2>
<p>The National Oceanic and Atmospheric Administration (NOAA) has been measuring the levels of atmospheric CO2 at the Mauna Loa, Hawaii. The measurements start on March 1958 and go all the way to Janurary 2016.
The data can be found <a class="reference external" href="http://www.esrl.noaa.gov/gmd/ccgg/trends/data.html">here</a>.
The Python script below, downloads and plots the data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/lecturebook/data/mauna_loa_co2.txt&quot;</span>
<span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">data</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">loadtxt</span><span class="p">(</span><span class="s1">&#39;mauna_loa_co2.txt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#load data </span>
<span class="n">t</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1">#time (in decimal dates)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">][:,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1">#CO2 level (mole fraction in dry air, micromol/mol, abbreviated as ppm)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$t$ (year)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y$ (CO2 level in ppm)&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/437def5ccdf3a1783ea80e8394e3e89a1f168201532da148c14505bfb0a9c705.png" src="../_images/437def5ccdf3a1783ea80e8394e3e89a1f168201532da148c14505bfb0a9c705.png" />
</div>
</div>
<p>Overall, we observe a steady growth of CO2 levels. The wiggles correspond to seasonal changes. Since the vast majority of the population inhabits the Northen hemisphere, fuel consumption goes up during the Northen winters and CO2 emissions follow. Our goal is to study this dataset with Gaussian process regression. Specifically we would like to predict the evolution of the CO2 levels from Feb 2018 to Feb 2028 and quantify our uncertainty about this prediction.</p>
<p>It’s always a good idea to work with at scaled version of the inptus and the outputs. We are going to scale the times as follows:</p>
<div class="math notranslate nohighlight">
\[
t_s = t - t_{\min}.
\]</div>
<p>So, time is still in fractional years, but we start counting at zero instead of 1950.
We scale the <span class="math notranslate nohighlight">\(y\)</span>’s as:</p>
<div class="math notranslate nohighlight">
\[
y_s = \frac{y - y_{\min}}{y_{\max}-y_{\min}}.
\]</div>
<p>This takes all the <span class="math notranslate nohighlight">\(y\)</span> between <span class="math notranslate nohighlight">\(0\)</span> and <span class="math notranslate nohighlight">\(1\)</span>.
Here is how the scaled data look like:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t_s</span> <span class="o">=</span> <span class="n">t</span> <span class="o">-</span> <span class="n">t</span><span class="o">.</span><span class="n">min</span><span class="p">()</span>
<span class="n">y_s</span> <span class="o">=</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">())</span> <span class="o">/</span> <span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">-</span> <span class="n">y</span><span class="o">.</span><span class="n">min</span><span class="p">())</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_s</span><span class="p">,</span> <span class="n">y_s</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$t_s$ (Scaled year)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y_s (Scaled CO2 level)$&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/61f1171963353d19bd7d9ebcc938cf9c5c099833be53ef74a0e58927a9777a78.png" src="../_images/61f1171963353d19bd7d9ebcc938cf9c5c099833be53ef74a0e58927a9777a78.png" />
</div>
</div>
<p>In what follows, just work with the scaled data as you develop your model.
Scale back to the original units for your final predictions.</p>
</section>
<section id="part-a-naive-approach">
<h2>Part A - Naive approach<a class="headerlink" href="#part-a-naive-approach" title="Permalink to this heading">#</a></h2>
<p>Use a zero mean Gaussian process with a squared exponential covariance function to fit the data and make the required prediction (ten years after the last observation).</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
<span class="n">naive_model</span> <span class="o">=</span> <span class="o">?</span>
</pre></div>
</div>
</div>
</div>
<p>Predict everything:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">tss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t_s</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">200</span><span class="p">)[:,</span> <span class="kc">None</span><span class="p">]</span>
<span class="n">ys</span><span class="p">,</span> <span class="n">vs</span> <span class="o">=</span> <span class="n">naive_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">tss</span><span class="p">)</span>
<span class="n">ls</span> <span class="o">=</span> <span class="n">ys</span> <span class="o">-</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vs</span><span class="p">)</span>
<span class="n">us</span> <span class="o">=</span> <span class="n">ys</span> <span class="o">+</span> <span class="mf">1.96</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">vs</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span> <span class="n">dpi</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">tss</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Posterior mean&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">tss</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">ls</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">us</span><span class="o">.</span><span class="n">flatten</span><span class="p">(),</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;blue&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_s</span><span class="p">,</span> <span class="n">y_s</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Scaled observed data&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;$t_s$ (Scaled year)&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;$y_s$ (Scaled CO2 level)&#39;</span><span class="p">);</span>
</pre></div>
</div>
</div>
</div>
<p>Notice that the squared exponential covariance caputes the long terms, but it fails to capture the seasonal fluctuations. As a matter of fact the seasonabl fluctions as treated as noise. This is clearly false. How can we fix it?</p>
</section>
<section id="part-b-improving-the-prior-covariance">
<h2>Part B - Improving the prior covariance<a class="headerlink" href="#part-b-improving-the-prior-covariance" title="Permalink to this heading">#</a></h2>
<p>Now use the ideas of Problem 1, to come up with a covariance function that is exhibits the following characteristics clearly visible in the data (call <span class="math notranslate nohighlight">\(f(x)\)</span> the scaled CO2 level.</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(f(x)\)</span> is smooth</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x)\)</span> has a clear trend with a multi-year lengthscale (it is also an increasing trend, but we are not going to impose this)</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x)\)</span> has seasonal fluctations with a period of one year</p></li>
<li><p><span class="math notranslate nohighlight">\(f(x)\)</span> exhibits small fluctiations within its period.</p></li>
</ul>
<p>Use summation and multiplication of simple covariance functions to create a covariance function that exhibits these trends.
Sample a few times from it.</p>
<p>Hint: Do not attempt to fit the data in any way. Just try to find a covariance function that has the right features. We also do not care about getting the parameters 100% right at this point. The parameters will be optimized later.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-c-predicting-the-future">
<h2>Part C - Predicting the future<a class="headerlink" href="#part-c-predicting-the-future" title="Permalink to this heading">#</a></h2>
<p>Use a zero mean Gaussian process with the covariance function you picked above to do Gaussian process regression
and make the required prediction (ten years after the last observation).</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-d-bayesian-information-criterion">
<h2>Part D - Bayesian information criterion<a class="headerlink" href="#part-d-bayesian-information-criterion" title="Permalink to this heading">#</a></h2>
<p>As we have seen in earlier lectures, the Bayesian informationc criterion (BIC), see <a class="reference external" href="https://en.wikipedia.org/wiki/Bayesian_information_criterion">this</a>, can bse used to compare two models.
The criterion says that one should:</p>
<ul class="simple">
<li><p>fit the models with maximum likelihood,</p></li>
<li><p>and compute the quantity:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\text{BIC} = d\ln(n) - 2\ln(\hat{L}),
\]</div>
<p>where <span class="math notranslate nohighlight">\(d\)</span> is the number of model parameters, and <span class="math notranslate nohighlight">\(\hat{L}\)</span> the maximum likelihood.</p>
<ul class="simple">
<li><p>pick the model with the smallest BIC.</p></li>
</ul>
<p>Use BIC to show that the model you constructed in Part C is indeed better than the naïve model of Part A.</p>
<p>Hint: Do a <code class="docutils literal notranslate"><span class="pre">help(GPy.models.GPRegression)</span></code> and you will find a way to get both the number of parameters and the log likelihood. Ask on piazza if you can’t find it - or Google it.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-e-plot-samples-from-the-posterior-gaussian-process">
<h2>Part E - Plot samples from the posterior Gaussian process<a class="headerlink" href="#part-e-plot-samples-from-the-posterior-gaussian-process" title="Permalink to this heading">#</a></h2>
<p>Using the model of Part C, plot 5 samples from the posterior Gaussian process between 2018 and 2028.</p>
<p>Hint: You need to use <code class="docutils literal notranslate"><span class="pre">GPy.models.GPRegression.posterior_samples_f</span></code>.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="problem-3-using-bayesian-global-optimization-to-calibrate-an-expensive-physical-model">
<h2>Problem 3 - Using Bayesian Global optimization to calibrate an expensive physical model<a class="headerlink" href="#problem-3-using-bayesian-global-optimization-to-calibrate-an-expensive-physical-model" title="Permalink to this heading">#</a></h2>
<p>This is Example 3.1 of <a class="reference external" href="http://arxiv.org/abs/1410.5522">(Tsilifis, 2014)</a>.</p>
<p>Consider the catalytic
conversion of nitrate (<span class="math notranslate nohighlight">\(\mbox{NO}_3^-\)</span>) to nitrogen (<span class="math notranslate nohighlight">\(\mbox{N}_2\)</span>) and other
by-products by electrochemical means.
The mechanism that is followed is complex and not well understood.
The experiment of <a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0013468612005208">(Katsounaros, 2012)</a> confirmed the
production of nitrogen (<span class="math notranslate nohighlight">\(\mbox{N}_2\)</span>), ammonia
(<span class="math notranslate nohighlight">\(\mbox{NH}_3\)</span>), and nitrous oxide (<span class="math notranslate nohighlight">\(\mbox{N}_2\mbox{O}\)</span>) as final products
of the reaction, as well as the intermediate production of nitrite (<span class="math notranslate nohighlight">\(\mbox{NO}_2^-\)</span>).
The data are reproduced in <a class="reference external" href="https://en.wikipedia.org/wiki/Comma-separated_values">Comma-separated values</a> (CSV) and stored in
<a class="reference external" href="https://raw.githubusercontent.com/PredictiveScienceLab/data-analytics-se/master/homework/catalysis.csv">catalysis.csv</a>.
The time is measured in minutes and the conentrations are measured in <span class="math notranslate nohighlight">\(\mbox{mmol}\cdot\mbox{L}^{-1}\)</span>.
Let’s load the data into this notebook using the <a class="reference external" href="http://pandas.pydata.org">Pandas</a> Python module:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">url</span> <span class="o">=</span> <span class="s2">&quot;https://github.com/PredictiveScienceLab/data-analytics-se/raw/master/lecturebook/data/catalysis.csv&quot;</span>
<span class="n">download</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Load the data</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="n">catalysis_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s1">&#39;catalysis.csv&#39;</span><span class="p">)</span>
<span class="n">catalysis_data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_html"><div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time</th>
      <th>NO3</th>
      <th>NO2</th>
      <th>N2</th>
      <th>NH3</th>
      <th>N2O</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>500.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
      <td>0.00</td>
    </tr>
    <tr>
      <th>1</th>
      <td>30</td>
      <td>250.95</td>
      <td>107.32</td>
      <td>18.51</td>
      <td>3.33</td>
      <td>4.98</td>
    </tr>
    <tr>
      <th>2</th>
      <td>60</td>
      <td>123.66</td>
      <td>132.33</td>
      <td>74.85</td>
      <td>7.34</td>
      <td>20.14</td>
    </tr>
    <tr>
      <th>3</th>
      <td>90</td>
      <td>84.47</td>
      <td>98.81</td>
      <td>166.19</td>
      <td>13.14</td>
      <td>42.10</td>
    </tr>
    <tr>
      <th>4</th>
      <td>120</td>
      <td>30.24</td>
      <td>38.74</td>
      <td>249.78</td>
      <td>19.54</td>
      <td>55.98</td>
    </tr>
    <tr>
      <th>5</th>
      <td>150</td>
      <td>27.94</td>
      <td>10.42</td>
      <td>292.32</td>
      <td>24.07</td>
      <td>60.65</td>
    </tr>
    <tr>
      <th>6</th>
      <td>180</td>
      <td>13.54</td>
      <td>6.11</td>
      <td>309.50</td>
      <td>27.26</td>
      <td>62.54</td>
    </tr>
  </tbody>
</table>
</div></div></div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">catalysis_data</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7990c6fe9a3a48ebcc2b771f694178b6262c899b02a1c816e66848ba95a56b75.png" src="../_images/7990c6fe9a3a48ebcc2b771f694178b6262c899b02a1c816e66848ba95a56b75.png" />
</div>
</div>
<p>The theory of catalytic reactions guarantees that the total mass must be conserved.
However, this is not the case in our dataset:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">catalysis_data</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>0    500.00
1    415.09
2    418.32
3    494.71
4    514.28
5    565.40
6    598.95
dtype: float64
</pre></div>
</div>
</div>
</div>
<p>This inconsistency suggests the existence of an intermediate unobserved reaction product X.
<a class="reference external" href="http://www.sciencedirect.com/science/article/pii/S0013468612005208">(Katsounaros, 2012)</a> suggested that the following reaction path shown in the following figure.</p>
<p>The dynamical system associated with the reaction is:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{cc}
\frac{d \left[\mbox{NO}_3^-\right]}{dt} &amp;= -k_1\left[\mbox{NO}_3^-\right], \\
\frac{d\left[\mbox{NO}_2^-\right]}{dt} &amp;= k_1\left[\mbox{NO}_3^-\right] - (k_2 + k_4 +
k_5)[\mbox{NO}_2^-], \\
\frac{d \left[\mbox{X}\right]}{dt} &amp;= k_2 \left[\mbox{NO}_2^-\right] - k_3 [X],\\
\frac{d \left[\mbox{N}_2\right]}{dt} &amp;= k_3 \left[\mbox{X}\right], \\
\frac{d \left[\mbox{NH}_3\right]}{dt} &amp;= k_4 \left[\mbox{NO}_2^-\right],\\
\frac{d \left[\mbox{N}_2O\right]}{dt} &amp;= k_5 \left[\mbox{NO}_2^-\right],
\end{array}
\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\([\cdot]\)</span> denotes the concentration of a quantity, and
<span class="math notranslate nohighlight">\(k_i &gt; 0\)</span>, <span class="math notranslate nohighlight">\(i=1,...5\)</span> are the <em>kinetic rate constants</em>.</p>
<p>In this problem, I am going to guide you through the calibration of the parameters of this model so that we match the observations.
These problems are also known as <em>inverse problems</em>.
The problem can, and should, be formulated in a Bayesian way.
However, in this homework problem we are going to do it using a classical loss-minimization approach.
We will discuss the Bayesian approach for calibrating the same model in a later lecture.</p>
<p>Before you proceed, please read a little bit about the “classical theory of inverse problems:”</p>
<section id="classical-theory-of-inverse-problems">
<h3>Classical theory of inverse problems<a class="headerlink" href="#classical-theory-of-inverse-problems" title="Permalink to this heading">#</a></h3>
<p>Suppose that you have a model (any model really) that predicts a quantity of interest.
Let’s assume that this model has parameters that you do not know.
These parameters could be simple scalars (mass, spring constant, dumping coefficients, etc.) or it could be also be functions (initial conditions, boundary values, spatially distributed constitutive relations, etc.)
Let’s denote all these parameters with the vector <span class="math notranslate nohighlight">\(x\)</span>.
Assume that:</p>
<div class="math notranslate nohighlight">
\[
x\in\mathcal{X} \subset\mathbb{R}^d.
\]</div>
<p>Now, let’s say we perform an experiment that measures a <em>noisy</em> vector:</p>
<div class="math notranslate nohighlight">
\[
y\in\mathcal{Y}\subset \mathbb{R}^m.
\]</div>
<p>Assume that, you can use your model <em>model</em> to predict <span class="math notranslate nohighlight">\(y\)</span>.
It does not matter how complicated your model is.
It could be a system of ordinary differential or partial differential equations, or something more complicated.
If it predicts <span class="math notranslate nohighlight">\(y\)</span>, you can always think of it as a function from the unknown parameter space <span class="math notranslate nohighlight">\(\mathcal{X}\)</span> to the space of <span class="math notranslate nohighlight">\(y\)</span>’s, <span class="math notranslate nohighlight">\(\mathcal{Y}\subset\mathbb{R}^m\)</span>.
That is, you can think of it as giving rise to a function:</p>
<div class="math notranslate nohighlight">
\[
f :\mathcal{X} \rightarrow \mathcal{Y}.
\]</div>
<p>The <strong>inverse problem</strong>, otherwise known as the <strong>model calibration</strong> problem is to find the best <span class="math notranslate nohighlight">\(x\in\mathcal{X}\)</span> so that:</p>
<div class="math notranslate nohighlight">
\[
f(x) \approx y.
\]</div>
</section>
<section id="formulation-of-inverse-problems-as-optimization-problems">
<h3>Formulation of Inverse Problems as Optimization Problems<a class="headerlink" href="#formulation-of-inverse-problems-as-optimization-problems" title="Permalink to this heading">#</a></h3>
<p>Saying that <span class="math notranslate nohighlight">\(f(x)\approx y\)</span> is not an exact mathematical statement.
What does it really mean for <span class="math notranslate nohighlight">\(f(x)\)</span> to be close to <span class="math notranslate nohighlight">\(y\)</span>?
To quantify this, let us introduce a <em>loss metric</em>:</p>
<div class="math notranslate nohighlight">
\[
\ell: \mathcal{Y}\times\mathcal{Y}\rightarrow \mathbb{R},
\]</div>
<p>such that <span class="math notranslate nohighlight">\(\ell(f(x),y)\)</span> is how much our prediction is off if we chose the input <span class="math notranslate nohighlight">\(x\in\mathcal{X}\)</span>.
Equiped with this loss metric, we can formulate the mathematical problem as:</p>
<div class="math notranslate nohighlight">
\[
\min_{x\in\mathcal{X}} \ell(f(x),y).
\]</div>
<section id="the-square-loss">
<h4>The Square Loss<a class="headerlink" href="#the-square-loss" title="Permalink to this heading">#</a></h4>
<p>The choice of the metric is somewhat subjective (it depends on what it means to be wrong in your problem).
However, a very common assumption is that to take the <em>square loss</em>:</p>
<div class="math notranslate nohighlight">
\[
\ell(f(x), y) = \frac{1}{2}\parallel f(x) - y\parallel_2^2 = \frac{1}{2}\sum_{i=1}^m\left(f_i(x)-y_i\right)^2.
\]</div>
<p>For this case, the inverse problem can be formulated as:</p>
<div class="math notranslate nohighlight">
\[
\min_{x\in\mathcal{X}}\frac{1}{2}\parallel f(x) - y\parallel_2^2.
\]</div>
</section>
<section id="solution-methodologies">
<h4>Solution Methodologies<a class="headerlink" href="#solution-methodologies" title="Permalink to this heading">#</a></h4>
<p>We basically have to solve an optimization problem.
For the square loss function, if <span class="math notranslate nohighlight">\(f(x)\)</span> is linear, then you get the classic least squares problem which has a known solution.
Otherwise, you get what is known as <em>generalized least squares</em>.
There are many algorithms that you could use this problem.
Several are implemented in <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/optimize.html">scipy.optimize</a>.
If you are able to implement your model as a simple python function, then you can use them.
Alternatively, and this is what we are going to do here, we could use Bayesian global optimization instead.
The absolutely, essential thing that you need to provide to these methods is the function they are optimizing, i.e.,</p>
<div class="math notranslate nohighlight">
\[
L(x,y) = \ell(f(x),y).
\]</div>
</section>
</section>
<section id="back-to-the-catalysis-model">
<h3>Back to the catalysis model<a class="headerlink" href="#back-to-the-catalysis-model" title="Permalink to this heading">#</a></h3>
<p>Let’s now formulate the calibration problem for the catalysis model.
We proceed in several steps.</p>
<section id="step-1-making-our-life-easier-by-simplifying-the-notation">
<h4>Step 1: Making our life easier by simplifying the notation<a class="headerlink" href="#step-1-making-our-life-easier-by-simplifying-the-notation" title="Permalink to this heading">#</a></h4>
<p>Note that this is actually a linear system.
To simplify our notation, let’s define:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ccc}
z_1 &amp;:=&amp; \left[\mbox{NO}_3^-\right],\\
z_2 &amp;:=&amp; \left[\mbox{NO}_2^-\right],\\
z_3 &amp;:=&amp; \left[\mbox{X}\right],\\
z_4 &amp;:=&amp; \left[\mbox{N}_2\right],\\
z_5 &amp;:=&amp; \left[\mbox{NH}_3\right],\\
z_6 &amp;:=&amp; \left[\mbox{N}_2O\right],
\end{array}
\end{split}\]</div>
<p>the vector:</p>
<div class="math notranslate nohighlight">
\[
z = (z_1,z_2,z_3,z_4,z_5,z_6),
\]</div>
<p>and the matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A(k_1,\dots,k_5) = \left(\begin{array}{cccccc}
-k_1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
k_1 &amp; -(k_2+k_4+k_5) &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; k_2 &amp; -k_3 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; k_3 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; k_4 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; k_5 &amp; 0 &amp; 0 &amp; 0 &amp; 0
\end{array}\right)\in\mathbb{R}^{6\times 6}.
\end{split}\]</div>
<p>With these definitions, the dynamical system becomes:</p>
<div class="math notranslate nohighlight">
\[
\dot{z} = A(k_1,\dots,k_5)z,
\]</div>
<p>with initial conditions</p>
<div class="math notranslate nohighlight">
\[
z(0) = z_0 = (500, 0, 0, 0, 0, 0)\in\mathbb{R}^6,
\]</div>
<p>read directly from the experimental data.
What we are definitely going to need is a solver for this system.
That’s easy.
Let’s denote the solution of the system at time <span class="math notranslate nohighlight">\(t\)</span> by:</p>
<div class="math notranslate nohighlight">
\[
z(t;k_1,\dots,k_5).
\]</div>
</section>
<section id="step-2-scale-the-unknown-parameters-to-your-best-of-your-abilities">
<h4>Step 2: Scale the unknown parameters to your best of your abilities<a class="headerlink" href="#step-2-scale-the-unknown-parameters-to-your-best-of-your-abilities" title="Permalink to this heading">#</a></h4>
<p>The constraints you have on your parameters, the better.
If you do have constraints, you would have to use constrained optimization algorithms.
The way you scale things depend on the problem.
Here we would think as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k_i\)</span> has units of inverse time. It is proparly appropriate to scale it with the total time which is 180 minutes.
So, let’s just multiply <span class="math notranslate nohighlight">\(k_i\)</span> with 180. This makes the resulting variable dimensionless:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
\hat{x}_i = 180k_i.
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(k_i\)</span> is positive, therefore <span class="math notranslate nohighlight">\(\hat{x_i}\)</span> must be positive.
So, let’s just work with the logarithm of <span class="math notranslate nohighlight">\(\hat{x_i}\)</span>:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
x_i = \log \hat{x_i} = \log 180k_i.
\]</div>
<ul class="simple">
<li><p>define the parameter vector:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
x = (x_1,\dots,x_5)\in\mathcal{X} = \mathbb{R}^5.
\]</div>
<p>From now on, we will write</p>
<div class="math notranslate nohighlight">
\[
A = A(x),
\]</div>
<p>for the matrix of the dynamical system, and</p>
<div class="math notranslate nohighlight">
\[
z = z(t;x),
\]</div>
<p>for the solution at <span class="math notranslate nohighlight">\(t\)</span> given that the parameters are <span class="math notranslate nohighlight">\(x\)</span>.</p>
</section>
<section id="step-3-making-the-connection-between-our-model-and-the-experimental-measurements">
<h4>Step 3: Making the connection between our model and the experimental measurements<a class="headerlink" href="#step-3-making-the-connection-between-our-model-and-the-experimental-measurements" title="Permalink to this heading">#</a></h4>
<p>Our experimental data include measurements of everything except <span class="math notranslate nohighlight">\(z_3\)</span> at times six (6) time instants:</p>
<div class="math notranslate nohighlight">
\[
t_j = 30j\;\mbox{minutes},
\]</div>
<p><span class="math notranslate nohighlight">\(j=1,\dots,6\)</span>.</p>
<p>Now, let <span class="math notranslate nohighlight">\(Y\in\mathbb{R}^{5\times 6}\)</span> be the experimental measurements:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">catalysis_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<p>You can think of the measurements as vector by flattening the matrix:</p>
<div class="math notranslate nohighlight">
\[
y = \operatorname{vec}(Y)\in\mathbb{R}^{30}.
\]</div>
<p>Note that <code class="docutils literal notranslate"><span class="pre">vec</span></code> is the vectorization operator.</p>
<p>What is the connection between the solution of the dynamical system <span class="math notranslate nohighlight">\(z(t,x)\)</span> and the experimental data?
It is as follows:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ccc}
z_1(30j;x) &amp;\longrightarrow&amp; Y_{j1},\\
z_2(30j;x) &amp;\longrightarrow&amp; Y_{j2},\\
z_4(30j;x) &amp;\longrightarrow&amp; Y_{j3},\\
z_5(30j;x) &amp;\longrightarrow&amp; Y_{j4},\\
z_6(30j;x) &amp;\longrightarrow&amp; Y_{j5},
\end{array}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(j=1,\dots,6\)</span>.</p>
<p>We are now ready to define a function:</p>
<div class="math notranslate nohighlight">
\[
f:\mathcal{X} \rightarrow \mathcal{Y}=\mathbb{R}^{30}_+,
\]</div>
<p>as follows:</p>
<ul class="simple">
<li><p>Define the matrix function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
F:\mathcal{X} \rightarrow \mathbb{R}^{5\times 6},
\]</div>
<p>by:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}{ccccc}
F_{j1}(x) &amp;=&amp; z_1(30j;x)&amp;\longrightarrow&amp; Y_{j1},\\
F_{j2}(x) &amp;=&amp; z_2(30j;x) &amp;\longrightarrow&amp; Y_{j2},\\
F_{j3}(x) &amp;=&amp; z_4(30j;x) &amp;\longrightarrow&amp; Y_{j3},\\
F_{j4}(x) &amp;=&amp; z_5(30j;x) &amp;\longrightarrow&amp; Y_{j4},\\
F_{j5}(x) &amp;=&amp; z_6(30j;x) &amp;\longrightarrow&amp; Y_{j5},
\end{array}
\end{split}\]</div>
<ul class="simple">
<li><p>And flatten that function:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[
f(x) = \operatorname{vec}(F(x))\in\mathbb{R}^{30}.
\]</div>
<p>Now, we have made the connection with our theoretical formulation of inverse problems crystal clear.</p>
</section>
<section id="step-4-programming-our-ode-solver-and-the-loss-function">
<h4>Step 4: Programming our ODE solver and the loss function<a class="headerlink" href="#step-4-programming-our-ode-solver-and-the-loss-function" title="Permalink to this heading">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.integrate</span>

<span class="k">def</span> <span class="nf">A</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Return the matrix of the dynamical system.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Scale back to the k&#39;s</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">/</span> <span class="mf">180.</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">6</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
    <span class="n">res</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">k</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">res</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">k</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+</span> <span class="n">k</span><span class="p">[</span><span class="mi">4</span><span class="p">])</span>
    <span class="n">res</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">res</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="o">-</span><span class="n">k</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">res</span><span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">res</span><span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> 
    <span class="n">res</span><span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="mi">4</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">res</span>
    

<span class="k">def</span> <span class="nf">g</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    The right hand side of the dynamical system.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">A</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">z</span><span class="p">)</span>


<span class="c1"># The initial conditions</span>
<span class="n">z0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">500.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">])</span>


<span class="c1"># The times at which we need the solution (experimental times)</span>
<span class="n">t_exp</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">30.</span> <span class="o">*</span> <span class="n">j</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">7</span><span class="p">)])</span>

<span class="c1"># The experimental data as a matrix</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">catalysis_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span><span class="o">.</span><span class="n">values</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>

<span class="c1"># The experimental as a vector</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># The full solution of the dynamical system</span>
<span class="k">def</span> <span class="nf">Z</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the solution for parameters x at times t.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">scipy</span><span class="o">.</span><span class="n">integrate</span><span class="o">.</span><span class="n">odeint</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">z0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">x</span><span class="p">,))</span>


<span class="c1"># The matrix function F (matches to Y)</span>
<span class="k">def</span> <span class="nf">F</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="n">res</span> <span class="o">=</span> <span class="n">Z</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">res</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">],</span> <span class="n">res</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">:]])</span>
    

<span class="c1"># The function f (matches to y)</span>
<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">F</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>

<span class="c1"># Finally, the loss function that we need to minimize over x:</span>
<span class="k">def</span> <span class="nf">L</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">f</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="mf">500.</span> <span class="o">-</span> <span class="n">y</span> <span class="o">/</span> <span class="mf">500.</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="c1"># We scale for numerical stability</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="step-5-minimize-the-loss-function">
<h4>Step 5: Minimize the loss function<a class="headerlink" href="#step-5-minimize-the-loss-function" title="Permalink to this heading">#</a></h4>
<p>Let’s optimize with scipy.optimize:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">scipy.optimize</span>

<span class="c1"># Initial guess for x</span>
<span class="n">x0</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">+</span> <span class="mf">2.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># Optimize</span>
<span class="n">res</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">minimize</span><span class="p">(</span><span class="n">L</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">t_exp</span><span class="p">,</span> <span class="n">y</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> /opt/homebrew/lib/python3.9/site-packages/scipy/integrate/_odepack_py.py:247: ODEintWarning:Excess work done on this call (perhaps wrong Dfun type). Run with full_output = 1 to get quantitative information.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>      fun: 0.281663032651243
 hess_inv: array([[ 113.712,    1.636,  121.512,  -78.324, -121.997],
       [   1.636,    0.954,    1.953,   -1.742,   -1.875],
       [ 121.512,    1.953,  130.795,  -83.975, -131.128],
       [ -78.324,   -1.742,  -83.975,   54.762,   84.125],
       [-121.997,   -1.875, -131.128,   84.125,  131.58 ]])
      jac: array([ 0.012,  0.004, -0.007, -0.002, -0.003])
  message: &#39;Desired error not necessarily achieved due to precision loss.&#39;
     nfev: 282
      nit: 13
     njev: 45
   status: 2
  success: False
        x: array([4.205, 6.916, 0.257, 0.611, 2.083])
</pre></div>
</div>
</div>
</div>
<p>And here is how you can visualize the model with the “best” parameters:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">res</span><span class="o">.</span><span class="n">x</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.359</span><span class="p">,</span> <span class="mf">1.657</span><span class="p">,</span> <span class="mf">1.347</span><span class="p">,</span> <span class="o">-</span><span class="mf">.16</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.01</span><span class="p">])</span>
<span class="n">Yp</span> <span class="o">=</span> <span class="n">Z</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">catalysis_data</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model NO3-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model NO2-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">5</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model X&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model N2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">3</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model NH3&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model N2O&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/87bac22481f1674b502b7ccd6c75979f63a368a5b5b9ba121931a9d59ba47e80.png" src="../_images/87bac22481f1674b502b7ccd6c75979f63a368a5b5b9ba121931a9d59ba47e80.png" />
</div>
</div>
<p>Note that the code above will not work every time… Some times it will work and sometimes it won’t work.
Run it 3-4 times if it accidentally works.
There are several problems.
Here are the three most relevant in our context:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> needs the gradient of the loss function. Since we do not provide it, it tries to get it using numerical differentiation. Numerical differentiation introduces errors…</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scipy.optimize</span></code> find a local minimum of the loss function. This may actually be a bad local minimum.</p></li>
<li><p>okay, this particular model is not very computationally expensive. But imagine trying to calibrate a model that takes a while for a single evaluation (e.g., a finite element model). Then, using scipy.optimize (especially without supplying the derivatives), is doomed to fail.</p></li>
</ul>
<p>To overcome these difficulties, you have to use Bayesian global optimization to solve the problem.
Note that in the hands-on activities, we introduced this code:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ei</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">ymax</span><span class="p">,</span> <span class="n">psi</span><span class="o">=</span><span class="mf">0.</span><span class="p">):</span>
    <span class="n">u</span> <span class="o">=</span> <span class="p">(</span><span class="n">m</span> <span class="o">-</span> <span class="n">ymax</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span>
    <span class="n">ei</span> <span class="o">=</span> <span class="n">sigma</span> <span class="o">*</span> <span class="p">(</span><span class="n">u</span> <span class="o">*</span> <span class="n">st</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">cdf</span><span class="p">(</span><span class="n">u</span><span class="p">)</span> <span class="o">+</span> <span class="n">st</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">u</span><span class="p">))</span>
    <span class="n">ei</span><span class="p">[</span><span class="n">sigma</span> <span class="o">&lt;=</span> <span class="mf">0.</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">return</span> <span class="n">ei</span>

<span class="k">def</span> <span class="nf">maximize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="n">gpr</span><span class="p">,</span> <span class="n">domain</span><span class="p">,</span> <span class="n">num_candidates</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span>
             <span class="n">alpha</span><span class="o">=</span><span class="n">ei</span><span class="p">,</span> <span class="n">psi</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span> <span class="n">max_it</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Optimize f using a limited number of evaluations.</span>
<span class="sd">    </span>
<span class="sd">    :param f:        The function to optimize.</span>
<span class="sd">    :param gpr:      A Gaussian process model to use for representing our state of knowldege.</span>
<span class="sd">    :param X_design: The set of candidate points for identifying the maximum.</span>
<span class="sd">    :param alpha:    The acquisition function.</span>
<span class="sd">    :param psi:      The parameter value for the acquisition function (not used for EI).</span>
<span class="sd">    :param max_it:   The maximum number of iterations.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">af_all</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Iteration</span><span class="se">\t</span><span class="s1">Current best objective </span><span class="se">\t</span><span class="s1">Current acquisition func. value&#39;</span><span class="p">)</span>
    <span class="n">dim</span> <span class="o">=</span> <span class="n">gpr</span><span class="o">.</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">count</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_it</span><span class="p">):</span>
        <span class="n">X_design</span> <span class="o">=</span> <span class="n">domain</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> \
                   <span class="p">(</span><span class="n">domain</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">domain</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">])</span> <span class="o">*</span> \
                        <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">num_candidates</span><span class="p">,</span> <span class="n">dim</span><span class="p">)</span>
        <span class="n">m</span><span class="p">,</span> <span class="n">sigma2</span> <span class="o">=</span> <span class="n">gpr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_design</span><span class="p">)</span>
        <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">sigma2</span><span class="p">)</span>
        <span class="n">af_values</span> <span class="o">=</span> <span class="n">alpha</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">sigma</span><span class="p">,</span> <span class="n">gpr</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">max</span><span class="p">(),</span> <span class="n">psi</span><span class="o">=</span><span class="n">psi</span><span class="p">)</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">af_values</span><span class="p">)</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">gpr</span><span class="o">.</span><span class="n">X</span><span class="p">,</span> <span class="n">X_design</span><span class="p">[</span><span class="n">i</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="p">:]])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">gpr</span><span class="o">.</span><span class="n">Y</span><span class="p">,</span> <span class="p">[</span><span class="n">f</span><span class="p">(</span><span class="n">X_design</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:])]])</span>
        <span class="n">gpr</span><span class="o">.</span><span class="n">set_XY</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="c1"># Uncomment the following to optimize the hyper-parameters</span>
        <span class="c1">#gpr.optimize()</span>
        <span class="n">idx_opt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">gpr</span><span class="o">.</span><span class="n">Y</span><span class="o">.</span><span class="n">flatten</span><span class="p">())</span>
        <span class="n">f_opt</span> <span class="o">=</span> <span class="n">gpr</span><span class="o">.</span><span class="n">Y</span><span class="p">[</span><span class="n">idx_opt</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;</span><span class="si">{0:d}</span><span class="se">\t\t</span><span class="si">{1:1.2f}</span><span class="se">\t\t\t</span><span class="si">{2:1.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">count</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">f_opt</span><span class="p">,</span> <span class="n">af_values</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">]))</span>
    <span class="n">x_opt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">gpr</span><span class="o">.</span><span class="n">X</span><span class="p">[</span><span class="n">idx_opt</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x_opt</span><span class="p">,</span> <span class="n">f_opt</span><span class="p">,</span> <span class="n">gpr</span>
</pre></div>
</div>
</div>
</div>
<p>The code <em>maximizes</em> a function, but you want to <em>minimize</em> the loss.
To recast the problem as a maximization problem, you need to work with <em>minus the loss</em>.
Also, the code does not allow for a function with extra parameters (like the <code class="docutils literal notranslate"><span class="pre">t_exp</span></code> and the <code class="docutils literal notranslate"><span class="pre">y</span></code> we have for <code class="docutils literal notranslate"><span class="pre">L</span></code>).
Here is the function that you should be optimizing:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="o">-</span><span class="n">L</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t_exp</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>
<section id="part-a-perform-multivariate-gaussian-process-regression-on-an-initial-set-of-data">
<h2>Part A - Perform multivariate Gaussian process regression on an initial set of data<a class="headerlink" href="#part-a-perform-multivariate-gaussian-process-regression-on-an-initial-set-of-data" title="Permalink to this heading">#</a></h2>
<p>We are going to search for the best parameters <span class="math notranslate nohighlight">\(x\)</span> within the set <span class="math notranslate nohighlight">\([-2,2]^5\)</span>.
Consider the following two datasets consisting of parameter and minus loss pairs:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initial training points</span>
<span class="n">n_init_train</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_init_train</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">+</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_init_train</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">Y_init_train</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_init_train</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Use a squared exponential covariance function with automatic relevance determination to do Gaussian process regression with <code class="docutils literal notranslate"><span class="pre">X_init_train</span></code> and <code class="docutils literal notranslate"><span class="pre">Y_init_train</span></code>.</p>
<p>Hint: You may want to experiment by constraining the likelihood noise of your model to be very small, say <span class="math notranslate nohighlight">\(10^{-6}\)</span>. This is because the observations of the loss do not really have any noise.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-b-inspecting-your-model">
<h2>Part B - Inspecting your model<a class="headerlink" href="#part-b-inspecting-your-model" title="Permalink to this heading">#</a></h2>
<p>Use the lengthscale information to rank the model parameters according their effect on the calibration loss.</p>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Your</span> <span class="n">code</span> <span class="n">here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-c-diagnostics">
<h2>Part C - Diagnostics<a class="headerlink" href="#part-c-diagnostics" title="Permalink to this heading">#</a></h2>
<p>Here are some test data:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Test points</span>
<span class="n">n_test</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="o">-</span><span class="mf">2.0</span> <span class="o">+</span> <span class="mf">4.0</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_test</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">Y_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">h</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">])[:,</span> <span class="kc">None</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<p>Do the following:</p>
<ul class="simple">
<li><p>Predictions vs observations plot</p></li>
<li><p>Standarized errors plot</p></li>
</ul>
<p><strong>Answer:</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Your code here</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="part-d-calibrate-the-model-with-bayesian-global-optimization">
<h2>Part D - Calibrate the model with Bayesian global optimization<a class="headerlink" href="#part-d-calibrate-the-model-with-bayesian-global-optimization" title="Permalink to this heading">#</a></h2>
<p>Now use Bayesian global optimization with expected improvement to calibrate your model using the GP that you built above as the starting point.
Do not expect this to give you a perfect model.
But it will be better than nothing.
We will get the best possible model in the next homework assignment.</p>
<p><strong>Hint:</strong> Here you basically need to read the docstring of <code class="docutils literal notranslate"><span class="pre">maximize</span></code> and use it correctly.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># For your convenience, the `domain` argument of minimize should be:
domain = np.array([[-2, 2], [-2, 2], [-2, 2], [-2, 2], [-2, 2]])
# Run maximize here:
# Your code here
x_opt = ? # The best parameters
</pre></div>
</div>
</div>
</div>
<p>Use this code to plot your calibrated model:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">x</span> <span class="o">=</span> <span class="n">x_opt</span>
<span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">180</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">x1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.359</span><span class="p">,</span> <span class="mf">1.657</span><span class="p">,</span> <span class="mf">1.347</span><span class="p">,</span> <span class="o">-</span><span class="mf">.16</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.01</span><span class="p">])</span>
<span class="n">Yp</span> <span class="o">=</span> <span class="n">Z</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">catalysis_data</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">style</span><span class="o">=</span><span class="s1">&#39;s&#39;</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model NO3-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model NO2-&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">5</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model X&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model N2&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">4</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">3</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model NH3&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">Yp</span><span class="p">[:,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">4</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Model N2O&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">();</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./homework"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="homework-05.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Homework 5</p>
      </div>
    </a>
    <a class="right-next"
       href="homework-07.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Homework 7</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#instructions">Instructions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#student-details">Student details</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-1-defining-priors-on-function-spaces">Problem 1 - Defining priors on function spaces</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-super-smooth-function-with-known-length-scale">Part A - Super smooth function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-super-smooth-function-with-known-ultra-small-length-scale">Part B - Super smooth function with known ultra small length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-continuous-function-with-known-length-scale">Part C - Continuous function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-smooth-periodic-function-with-known-length-scale">Part D - Smooth periodic function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-e-smooth-periodic-function-with-known-length-scale">Part E - Smooth periodic function with known length scale</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-f-the-sum-of-two-functions">Part F - The sum of two functions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#part-g-the-product-of-two-functions">Part G - The product of two functions</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-2">Problem 2</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-naive-approach">Part A - Naive approach</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-improving-the-prior-covariance">Part B - Improving the prior covariance</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-predicting-the-future">Part C - Predicting the future</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-bayesian-information-criterion">Part D - Bayesian information criterion</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-e-plot-samples-from-the-posterior-gaussian-process">Part E - Plot samples from the posterior Gaussian process</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#problem-3-using-bayesian-global-optimization-to-calibrate-an-expensive-physical-model">Problem 3 - Using Bayesian Global optimization to calibrate an expensive physical model</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#classical-theory-of-inverse-problems">Classical theory of inverse problems</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#formulation-of-inverse-problems-as-optimization-problems">Formulation of Inverse Problems as Optimization Problems</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#the-square-loss">The Square Loss</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#solution-methodologies">Solution Methodologies</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#back-to-the-catalysis-model">Back to the catalysis model</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-making-our-life-easier-by-simplifying-the-notation">Step 1: Making our life easier by simplifying the notation</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-scale-the-unknown-parameters-to-your-best-of-your-abilities">Step 2: Scale the unknown parameters to your best of your abilities</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-making-the-connection-between-our-model-and-the-experimental-measurements">Step 3: Making the connection between our model and the experimental measurements</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-programming-our-ode-solver-and-the-loss-function">Step 4: Programming our ODE solver and the loss function</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-minimize-the-loss-function">Step 5: Minimize the loss function</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-a-perform-multivariate-gaussian-process-regression-on-an-initial-set-of-data">Part A - Perform multivariate Gaussian process regression on an initial set of data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-b-inspecting-your-model">Part B - Inspecting your model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-c-diagnostics">Part C - Diagnostics</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#part-d-calibrate-the-model-with-bayesian-global-optimization">Part D - Calibrate the model with Bayesian global optimization</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Ilias Bilionis (ibilion[at]purdue.edu)
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>